<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Interview Questions</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary-color: #0ea5e9;
            --secondary-color: #3b82f6;
            --accent-color: #06b6d4;
            --dark-bg: #0f172a;
            --card-bg: #1e293b;
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --border-color: #334155;
            --success-color: #10b981;
            --warning-color: #f59e0b;
            --error-color: #ef4444;
            --code-bg: #1e1e1e;
            --sidebar-width: 320px;
            --header-height: 70px;
        }

        body {
            font-family: 'Inter', sans-serif;
            background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
            color: var(--text-primary);
            line-height: 1.6;
            overflow-x: hidden;
        }

        /* Header */
        .header {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            height: var(--header-height);
            background: rgba(15, 23, 42, 0.95);
            backdrop-filter: blur(10px);
            border-bottom: 1px solid var(--border-color);
            z-index: 1000;
            display: flex;
            align-items: center;
            padding: 0 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
        }

        .header-content {
            display: flex;
            align-items: center;
            justify-content: space-between;
            width: 100%;
            max-width: 1800px;
            margin: 0 auto;
        }

        .logo {
            display: flex;
            align-items: center;
            gap: 1rem;
            font-size: 1.5rem;
            font-weight: 700;
            background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .logo i {
            font-size: 2rem;
            background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .header-actions {
            display: flex;
            gap: 1rem;
            align-items: center;
        }

        .search-box {
            position: relative;
        }

        .search-box input {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 25px;
            padding: 0.5rem 1rem 0.5rem 2.5rem;
            color: var(--text-primary);
            width: 300px;
            transition: all 0.3s ease;
        }

        .search-box input:focus {
            outline: none;
            border-color: var(--primary-color);
            box-shadow: 0 0 0 3px rgba(14, 165, 233, 0.1);
        }

        .search-box i {
            position: absolute;
            left: 1rem;
            top: 50%;
            transform: translateY(-50%);
            color: var(--text-secondary);
        }

        .progress-indicator {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            background: var(--card-bg);
            padding: 0.5rem 1rem;
            border-radius: 25px;
            border: 1px solid var(--border-color);
        }

        .progress-bar-mini {
            width: 100px;
            height: 6px;
            background: var(--border-color);
            border-radius: 3px;
            overflow: hidden;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
            width: 0%;
            transition: width 0.3s ease;
        }

        /* Sidebar Navigation */
        .sidebar {
            position: fixed;
            left: 0;
            top: var(--header-height);
            bottom: 0;
            width: var(--sidebar-width);
            background: rgba(30, 41, 59, 0.95);
            backdrop-filter: blur(10px);
            border-right: 1px solid var(--border-color);
            overflow-y: auto;
            z-index: 999;
            transition: transform 0.3s ease;
        }

        .sidebar::-webkit-scrollbar {
            width: 6px;
        }

        .sidebar::-webkit-scrollbar-track {
            background: transparent;
        }

        .sidebar::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 3px;
        }

        .sidebar::-webkit-scrollbar-thumb:hover {
            background: var(--primary-color);
        }

        .difficulty-section {
            margin: 1rem 0;
        }

        .difficulty-header {
            padding: 1rem 1.5rem;
            background: var(--card-bg);
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-left: 4px solid transparent;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .difficulty-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 4px;
            height: 100%;
            background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
            transition: width 0.3s ease;
        }

        .difficulty-header:hover::before {
            width: 100%;
            opacity: 0.1;
        }

        .difficulty-header:hover {
            background: rgba(14, 165, 233, 0.05);
        }

        .difficulty-header.active {
            background: rgba(14, 165, 233, 0.1);
            border-left-color: var(--primary-color);
        }

        .difficulty-title {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            font-weight: 600;
            font-size: 0.95rem;
            position: relative;
            z-index: 1;
        }

        .difficulty-badge {
            padding: 0.25rem 0.75rem;
            border-radius: 12px;
            font-size: 0.7rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .badge-intermediate {
            background: rgba(245, 158, 11, 0.2);
            color: var(--warning-color);
        }

        .badge-advanced {
            background: rgba(239, 68, 68, 0.2);
            color: var(--error-color);
        }

        .badge-expert {
            background: rgba(168, 85, 247, 0.2);
            color: #a855f7;
        }

        .difficulty-header i {
            transition: transform 0.3s ease;
            position: relative;
            z-index: 1;
        }

        .difficulty-header.active i {
            transform: rotate(180deg);
        }

        .question-list {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }

        .question-list.active {
            max-height: 3000px;
        }

        .question-item {
            padding: 0.75rem 1.5rem 0.75rem 2.5rem;
            cursor: pointer;
            transition: all 0.3s ease;
            border-left: 3px solid transparent;
            display: flex;
            align-items: center;
            gap: 0.75rem;
            font-size: 0.9rem;
        }

        .question-item:hover {
            background: rgba(14, 165, 233, 0.05);
            border-left-color: var(--primary-color);
            padding-left: 2.75rem;
        }

        .question-item.active {
            background: rgba(14, 165, 233, 0.15);
            border-left-color: var(--primary-color);
            color: var(--primary-color);
            font-weight: 500;
        }

        .question-number {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 24px;
            height: 24px;
            border-radius: 50%;
            background: var(--border-color);
            font-size: 0.75rem;
            font-weight: 600;
            flex-shrink: 0;
        }

        .question-item.active .question-number {
            background: var(--primary-color);
            color: var(--dark-bg);
        }

        .question-item.completed .question-number {
            background: var(--success-color);
            color: white;
        }

        /* Main Content */
        .main-content {
            margin-left: var(--sidebar-width);
            margin-top: var(--header-height);
            padding: 2rem;
            min-height: calc(100vh - var(--header-height));
        }

        .content-wrapper {
            max-width: 1200px;
            margin: 0 auto;
        }

        .question-content {
            display: none;
            animation: fadeInUp 0.5s ease;
        }

        .question-content.active {
            display: block;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .question-header {
            background: linear-gradient(135deg, rgba(14, 165, 233, 0.1) 0%, rgba(6, 182, 212, 0.1) 100%);
            border: 1px solid var(--border-color);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            position: relative;
            overflow: hidden;
        }

        .question-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 4px;
            background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
        }

        .question-title {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .question-meta {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .meta-tag {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: var(--card-bg);
            border-radius: 8px;
            font-size: 0.85rem;
            border: 1px solid var(--border-color);
        }

        .content-card {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            transition: all 0.3s ease;
        }

        .content-card:hover {
            border-color: var(--primary-color);
            box-shadow: 0 8px 16px rgba(14, 165, 233, 0.1);
        }

        .section-title {
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.75rem;
            color: var(--primary-color);
        }

        .section-title i {
            font-size: 1.75rem;
        }

        .definition-box {
            background: linear-gradient(135deg, rgba(14, 165, 233, 0.05) 0%, rgba(6, 182, 212, 0.05) 100%);
            border-left: 4px solid var(--primary-color);
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 1.5rem;
        }

        .definition-box p {
            font-size: 1.05rem;
            line-height: 1.8;
            color: var(--text-secondary);
        }

        .definition-box strong {
            color: var(--primary-color);
            font-weight: 600;
        }

        .example-box {
            background: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            position: relative;
        }

        .example-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 0.75rem;
            border-bottom: 1px solid var(--border-color);
        }

        .example-title {
            font-weight: 600;
            color: var(--primary-color);
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .copy-btn {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
            padding: 0.4rem 0.8rem;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.85rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            transition: all 0.3s ease;
        }

        .copy-btn:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        pre {
            margin: 0;
            overflow-x: auto;
        }

        code {
            font-family: 'Fira Code', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            color: #e0e0e0;
        }

        .keyword-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.75rem;
            margin: 1.5rem 0;
        }

        .keyword-tag {
            background: linear-gradient(135deg, rgba(14, 165, 233, 0.2) 0%, rgba(6, 182, 212, 0.2) 100%);
            color: var(--primary-color);
            padding: 0.5rem 1rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            border: 1px solid rgba(14, 165, 233, 0.3);
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            transition: all 0.3s ease;
        }

        .keyword-tag:hover {
            background: linear-gradient(135deg, rgba(14, 165, 233, 0.3) 0%, rgba(6, 182, 212, 0.3) 100%);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(14, 165, 233, 0.2);
        }

        .cross-questions {
            background: linear-gradient(135deg, rgba(245, 158, 11, 0.05) 0%, rgba(251, 191, 36, 0.05) 100%);
            border: 1px solid rgba(245, 158, 11, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .cross-question-item {
            padding: 1rem;
            margin: 0.75rem 0;
            background: rgba(30, 41, 59, 0.5);
            border-radius: 8px;
            border-left: 3px solid var(--warning-color);
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .cross-question-item:hover {
            background: rgba(30, 41, 59, 0.8);
            transform: translateX(5px);
        }

        .cross-question-text {
            display: flex;
            align-items: flex-start;
            gap: 0.75rem;
            color: var(--text-secondary);
        }

        .cross-question-text i {
            color: var(--warning-color);
            margin-top: 0.2rem;
        }

        .cross-answer {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
            margin-top: 0.75rem;
            padding-left: 1.5rem;
            color: var(--text-secondary);
            font-size: 0.95rem;
        }

        .cross-question-item.active .cross-answer {
            max-height: 800px;
            margin-top: 1rem;
        }

        .scenario-box {
            background: linear-gradient(135deg, rgba(168, 85, 247, 0.05) 0%, rgba(217, 70, 239, 0.05) 100%);
            border: 1px solid rgba(168, 85, 247, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .scenario-title {
            color: #a855f7;
            font-weight: 600;
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 1.1rem;
        }

        .image-container {
            text-align: center;
            margin: 2rem 0;
            padding: 1.5rem;
            background: var(--card-bg);
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.3);
        }

        .image-caption {
            margin-top: 1rem;
            color: var(--text-secondary);
            font-size: 0.9rem;
            font-style: italic;
        }

        .navigation-buttons {
            display: flex;
            justify-content: space-between;
            margin-top: 3rem;
            gap: 1rem;
        }

        .nav-btn {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            color: var(--text-primary);
            padding: 1rem 2rem;
            border-radius: 8px;
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 0.75rem;
            font-size: 1rem;
            font-weight: 500;
            transition: all 0.3s ease;
            flex: 1;
            max-width: 250px;
        }

        .nav-btn:hover:not(:disabled) {
            background: var(--primary-color);
            border-color: var(--primary-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(14, 165, 233, 0.3);
        }

        .nav-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .nav-btn.next {
            margin-left: auto;
        }

        .highlight-box {
            background: linear-gradient(135deg, rgba(16, 185, 129, 0.05) 0%, rgba(5, 150, 105, 0.05) 100%);
            border-left: 4px solid var(--success-color);
            padding: 1rem 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .highlight-box strong {
            color: var(--success-color);
        }

        .note-box {
            background: linear-gradient(135deg, rgba(59, 130, 246, 0.05) 0%, rgba(37, 99, 235, 0.05) 100%);
            border-left: 4px solid var(--secondary-color);
            padding: 1rem 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .warning-box {
            background: linear-gradient(135deg, rgba(239, 68, 68, 0.05) 0%, rgba(220, 38, 38, 0.05) 100%);
            border-left: 4px solid var(--error-color);
            padding: 1rem 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .table-container {
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            background: var(--card-bg);
            border-radius: 8px;
            overflow: hidden;
        }

        th {
            background: rgba(14, 165, 233, 0.1);
            color: var(--primary-color);
            padding: 1rem;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 1rem;
            border-top: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        tr:hover td {
            background: rgba(14, 165, 233, 0.05);
        }

        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1.5rem;
            margin: 1.5rem 0;
        }

        .comparison-card {
            padding: 1.5rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .comparison-card.narrow {
            background: linear-gradient(135deg, rgba(16, 185, 129, 0.1), rgba(5, 150, 105, 0.05));
            border-color: rgba(16, 185, 129, 0.3);
        }

        .comparison-card.wide {
            background: linear-gradient(135deg, rgba(239, 68, 68, 0.1), rgba(220, 38, 38, 0.05));
            border-color: rgba(239, 68, 68, 0.3);
        }

        .comparison-card h4 {
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .comparison-card.narrow h4 {
            color: var(--success-color);
        }

        .comparison-card.wide h4 {
            color: var(--error-color);
        }

        /* Mobile Toggle */
        .mobile-toggle {
            display: none;
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 60px;
            height: 60px;
            background: var(--primary-color);
            border-radius: 50%;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            z-index: 1001;
            box-shadow: 0 4px 12px rgba(14, 165, 233, 0.4);
        }

        .mobile-toggle i {
            font-size: 1.5rem;
            color: white;
        }

        /* Scroll to Top Button */
        .scroll-top {
            position: fixed;
            bottom: 2rem;
            right: 5rem;
            width: 50px;
            height: 50px;
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 50%;
            display: none;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: all 0.3s ease;
            z-index: 999;
        }

        .scroll-top:hover {
            background: var(--primary-color);
            border-color: var(--primary-color);
            transform: translateY(-5px);
        }

        .scroll-top.visible {
            display: flex;
        }

        .scroll-top i {
            color: var(--text-primary);
        }

        /* Responsive Design */
        @media (max-width: 1024px) {
            .sidebar {
                transform: translateX(-100%);
            }

            .sidebar.active {
                transform: translateX(0);
            }

            .main-content {
                margin-left: 0;
            }

            .mobile-toggle {
                display: flex;
            }

            .search-box input {
                width: 200px;
            }

            .comparison-grid {
                grid-template-columns: 1fr;
            }
        }

        @media (max-width: 768px) {
            .header {
                padding: 0 1rem;
            }

            .logo {
                font-size: 1.2rem;
            }

            .search-box {
                display: none;
            }

            .main-content {
                padding: 1rem;
            }

            .question-title {
                font-size: 1.5rem;
            }

            .navigation-buttons {
                flex-direction: column;
            }

            .nav-btn {
                max-width: 100%;
            }

            .nav-btn.next {
                margin-left: 0;
            }
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="header-content">
            <div class="logo">
                <i class="fas fa-rocket"></i>
                <span>PySpark Interview Questions</span>
            </div>
            <div class="header-actions">
                <div class="search-box">
                    <i class="fas fa-search"></i>
                    <input type="text" id="searchInput" placeholder="Search questions...">
                </div>
                <div class="progress-indicator">
                    <span id="progressText">0%</span>
                    <div class="progress-bar-mini">
                        <div class="progress-fill" id="progressFill"></div>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Sidebar Navigation -->
    <nav class="sidebar" id="sidebar">
          <!-- Very Basic / Foundational -->
        <div class="difficulty-section">
            <div class="difficulty-header" data-section="beginner">
                <div class="difficulty-title">
                    <span class="difficulty-badge badge-beginner">Beginner</span>
                    <span>Foundational Concepts</span>
                </div>
                <i class="fas fa-chevron-down"></i>
            </div>
            <div class="question-list">
                <div class="question-item" data-question="q1">
                    <span class="question-number">1</span>
                    <span>What is PySpark?</span>
                </div>
                <div class="question-item" data-question="q2">
                    <span class="question-number">2</span>
                    <span>Apache Spark & PySpark Relation</span>
                </div>
                <div class="question-item" data-question="q3">
                    <span class="question-number">3</span>
                    <span>Main Components of Spark</span>
                </div>
            </div>
        </div>

        <!-- Basic -->
        <div class="difficulty-section">
            <div class="difficulty-header" data-section="basic">
                <div class="difficulty-title">
                    <span class="difficulty-badge badge-basic">Basic</span>
                    <span>Core Concepts</span>
                </div>
                <i class="fas fa-chevron-down"></i>
            </div>
            <div class="question-list">
                <div class="question-item" data-question="q4">
                    <span class="question-number">4</span>
                    <span>Spark vs Hadoop MapReduce</span>
                </div>
                <div class="question-item" data-question="q5">
                    <span class="question-number">5</span>
                    <span>Problems PySpark Solves</span>
                </div>
                <div class="question-item" data-question="q6">
                    <span class="question-number">6</span>
                    <span>PySpark vs Pandas</span>
                </div>
                <div class="question-item" data-question="q7">
                    <span class="question-number">7</span>
                    <span>Installation & Setup</span>
                </div>
            </div>
        </div>

        <!-- Intermediate -->
        <div class="difficulty-section">
            <div class="difficulty-header" data-section="intermediate">
                <div class="difficulty-title">
                    <span class="difficulty-badge badge-intermediate">Intermediate</span>
                    <span>Working with Spark</span>
                </div>
                <i class="fas fa-chevron-down"></i>
            </div>
            <div class="question-list">
                <div class="question-item" data-question="q8">
                    <span class="question-number">8</span>
                    <span>SparkContext Explained</span>
                </div>
                <div class="question-item" data-question="q9">
                    <span class="question-number">9</span>
                    <span>SparkConf Overview</span>
                </div>
                <div class="question-item" data-question="q10">
                    <span class="question-number">10</span>
                    <span>SparkSession Deep Dive</span>
                </div>
                <div class="question-item" data-question="q11">
                    <span class="question-number">11</span>
                    <span>Understanding RDDs</span>
                </div>
                <div class="question-item" data-question="q12">
                    <span class="question-number">12</span>
                    <span>Creating RDDs</span>
                </div>
            </div>
        </div>

        <!-- Advanced -->
        <div class="difficulty-section">
            <div class="difficulty-header" data-section="advanced">
                <div class="difficulty-title">
                    <span class="difficulty-badge badge-advanced">Advanced</span>
                    <span>Performance & Optimization</span>
                </div>
                <i class="fas fa-chevron-down"></i>
            </div>
            <div class="question-list">
                <div class="question-item" data-question="q13">
                    <span class="question-number">13</span>
                    <span>Lazy Evaluation Concept</span>
                </div>
                <div class="question-item" data-question="q14">
                    <span class="question-number">14</span>
                    <span>Transformations & Actions</span>
                </div>
                <div class="question-item" data-question="q15">
                    <span class="question-number">15</span>
                    <span>Actions Triggering Computation</span>
                </div>
            </div>
        </div>
        <!-- Intermediate/Advanced -->
        <div class="difficulty-section">
            <div class="difficulty-header" data-section="intermediate">
                <div class="difficulty-title">
                    <span class="difficulty-badge badge-intermediate">Intermediate</span>
                    <span>Transformations & Dependencies</span>
                </div>
                <i class="fas fa-chevron-down"></i>
            </div>
            <div class="question-list">
                <div class="question-item" data-question="q16">
                    <span class="question-number">16</span>
                    <span>Narrow vs Wide Transformations</span>
                </div>
                <div class="question-item" data-question="q17">
                    <span class="question-number">17</span>
                    <span>Narrow vs Wide Dependencies</span>
                </div>
            </div>
        </div>

        <!-- Advanced - DataFrames -->
        <div class="difficulty-section">
            <div class="difficulty-header" data-section="advanced">
                <div class="difficulty-title">
                    <span class="difficulty-badge badge-advanced">Advanced</span>
                    <span>DataFrames Fundamentals</span>
                </div>
                <i class="fas fa-chevron-down"></i>
            </div>
            <div class="question-list">
                <div class="question-item" data-question="q18">
                    <span class="question-number">18</span>
                    <span>What is PySpark DataFrame?</span>
                </div>
                <div class="question-item" data-question="q19">
                    <span class="question-number">19</span>
                    <span>DataFrame vs RDD</span>
                </div>
                <div class="question-item" data-question="q20">
                    <span class="question-number">20</span>
                    <span>DataFrame vs Dataset APIs</span>
                </div>
            </div>
        </div>

        <!-- Expert - Schema & Operations -->
        <div class="difficulty-section">
            <div class="difficulty-header" data-section="expert">
                <div class="difficulty-title">
                    <span class="difficulty-badge badge-expert">Expert</span>
                    <span>Schema & Data Operations</span>
                </div>
                <i class="fas fa-chevron-down"></i>
            </div>
            <div class="question-list">
                <div class="question-item" data-question="q21">
                    <span class="question-number">21</span>
                    <span>Role of Schema in DataFrame</span>
                </div>
                <div class="question-item" data-question="q22">
                    <span class="question-number">22</span>
                    <span>Schema Inference</span>
                </div>
                <div class="question-item" data-question="q23">
                    <span class="question-number">23</span>
                    <span>View DataFrame Schema</span>
                </div>
                <div class="question-item" data-question="q24">
                    <span class="question-number">24</span>
                    <span>Get Column Names & Data Types</span>
                </div>
                <div class="question-item" data-question="q25">
                    <span class="question-number">25</span>
                    <span>Using show()</span>
                </div>
                <div class="question-item" data-question="q26">
                    <span class="question-number">26</span>
                    <span>View First Few Rows</span>
                </div>
                <div class="question-item" data-question="q27">
                    <span class="question-number">27</span>
                    <span>View Data from RDD/DataFrame</span>
                </div>
                <div class="question-item" data-question="q28">
                    <span class="question-number">28</span>
                    <span>collect() vs take() vs show()</span>
                </div>
                <div class="question-item" data-question="q29">
                    <span class="question-number">29</span>
                    <span>take() vs collect()</span>
                </div>
                <div class="question-item" data-question="q30">
                    <span class="question-number">30</span>
                    <span>Using collect()</span>
                </div>
            </div>
        </div>

        <!-- Advanced - Data I/O & Collection -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="io-operations">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Data I/O & Collection Operations</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q31">
            <span class="question-number">31</span>
            <span>collectAsMap() Function</span>
        </div>
        <div class="question-item" data-question="q32">
            <span class="question-number">32</span>
            <span>Reading CSV Files</span>
        </div>
        <div class="question-item" data-question="q33">
            <span class="question-number">33</span>
            <span>Reading JSON Data</span>
        </div>
        <div class="question-item" data-question="q34">
            <span class="question-number">34</span>
            <span>Reading Nested JSON</span>
        </div>
        <div class="question-item" data-question="q35">
            <span class="question-number">35</span>
            <span>Data Sources in Spark</span>
        </div>
    </div>
</div>

<!-- Expert - File Formats & Reading -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="file-formats">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-expert">Expert</span>
            <span>File Formats & Data Reading</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q36">
            <span class="question-number">36</span>
            <span>Common File Formats</span>
        </div>
        <div class="question-item" data-question="q37">
            <span class="question-number">37</span>
            <span>Reading Multiple CSVs</span>
        </div>
        <div class="question-item" data-question="q38">
            <span class="question-number">38</span>
            <span>select() vs selectExpr()</span>
        </div>
        <div class="question-item" data-question="q39">
            <span class="question-number">39</span>
            <span>Selecting Columns</span>
        </div>
        <div class="question-item" data-question="q40">
            <span class="question-number">40</span>
            <span>Filtering in PySpark</span>
        </div>
        <div class="question-item" data-question="q41">
            <span class="question-number">41</span>
            <span>Filtering DataFrame Rows</span>
        </div>
        <div class="question-item" data-question="q42">
            <span class="question-number">42</span>
            <span>Filtering RDD</span>
        </div>
    </div>
</div>

<!-- Expert - Column Operations -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="column-ops">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-expert">Expert</span>
            <span>Column Operations & Transformations</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q43">
            <span class="question-number">43</span>
            <span>Adding New Columns</span>
        </div>
        <div class="question-item" data-question="q44">
            <span class="question-number">44</span>
            <span>Creating Columns in PySpark</span>
        </div>
        <div class="question-item" data-question="q45">
            <span class="question-number">45</span>
            <span>Conditional Column Operations</span>
        </div>
        <div class="question-item" data-question="q46">
            <span class="question-number">46</span>
            <span>Renaming Columns</span>
        </div>
        <div class="question-item" data-question="q47">
            <span class="question-number">47</span>
            <span>Column Rename Methods</span>
        </div>
        <div class="question-item" data-question="q48">
            <span class="question-number">48</span>
            <span>Renaming DataFrame Columns</span>
        </div>
        <div class="question-item" data-question="q49">
            <span class="question-number">49</span>
            <span>Casting Column Data Types</span>
        </div>
        <div class="question-item" data-question="q50">
            <span class="question-number">50</span>
            <span>Dropping Columns</span>
        </div>
    </div>
</div>

<!-- Expert - Data Quality & Null Handling -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="data-quality">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-expert">Expert</span>
            <span>Data Quality & Null Handling</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q51">
            <span class="question-number">51</span>
            <span>Dropping Duplicates in DataFrame</span>
        </div>
        <div class="question-item" data-question="q52">
            <span class="question-number">52</span>
            <span>Removing Duplicates in PySpark</span>
        </div>
        <div class="question-item" data-question="q53">
            <span class="question-number">53</span>
            <span>Remove Duplicates by Columns</span>
        </div>
        <div class="question-item" data-question="q54">
            <span class="question-number">54</span>
            <span>Finding Null Values</span>
        </div>
        <div class="question-item" data-question="q55">
            <span class="question-number">55</span>
            <span>Handling Missing/Null Values</span>
        </div>
        <div class="question-item" data-question="q56">
            <span class="question-number">56</span>
            <span>Missing Value Strategies</span>
        </div>
    </div>
</div>

<!-- Advanced - Aggregation & Sorting -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="aggregation-sorting">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Aggregation & Sorting Operations</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q57">
            <span class="question-number">57</span>
            <span>count() vs approx_count_distinct()</span>
        </div>
        <div class="question-item" data-question="q58">
            <span class="question-number">58</span>
            <span>orderBy() vs sort()</span>
        </div>
        <div class="question-item" data-question="q59">
            <span class="question-number">59</span>
            <span>Difference: orderBy() and sort()</span>
        </div>
    </div>
</div>

<!-- Advanced - Cluster Management -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="cluster-management">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Cluster Management</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q60">
            <span class="question-number">60</span>
            <span>Spark Cluster Managers</span>
        </div>
    </div>

</div>
    <!-- Advanced - Cluster & Deployment -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="cluster-deployment">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Cluster & Deployment</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q61">
            <span class="question-number">61</span>
            <span>Default Deployment Mode</span>
        </div>
        <div class="question-item" data-question="q62">
            <span class="question-number">62</span>
            <span>Submitting Jobs with spark-submit</span>
        </div>
        <div class="question-item" data-question="q63">
            <span class="question-number">63</span>
            <span>Role of Driver and Executors</span>
        </div>
        <div class="question-item" data-question="q64">
            <span class="question-number">64</span>
            <span>Driver and Executors Explained</span>
        </div>
    </div>
</div>

<!-- Expert - Execution Model & DAG -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="execution-model">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-expert">Expert</span>
            <span>Execution Model & DAG</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q65">
            <span class="question-number">65</span>
            <span>DAG in Spark</span>
        </div>
        <div class="question-item" data-question="q66">
            <span class="question-number">66</span>
            <span>Spark Execution Flow</span>
        </div>
        <div class="question-item" data-question="q67">
            <span class="question-number">67</span>
            <span>Caching and Persistence</span>
        </div>
        <div class="question-item" data-question="q68">
            <span class="question-number">68</span>
            <span>Default Storage Level of cache()</span>
        </div>
        <div class="question-item" data-question="q69">
            <span class="question-number">69</span>
            <span>MEMORY_ONLY vs DISK_ONLY</span>
        </div>
    </div>
</div>

<!-- Intermediate - Transformations & Shuffling -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="transformations-shuffle">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-intermediate">Intermediate</span>
            <span>Transformations & Shuffling</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q70">
            <span class="question-number">70</span>
            <span>Wide and Narrow Transformations</span>
        </div>
        <div class="question-item" data-question="q71">
            <span class="question-number">71</span>
            <span>Narrow vs Wide Transformations</span>
        </div>
        <div class="question-item" data-question="q72">
            <span class="question-number">72</span>
            <span>Shuffling in Spark</span>
        </div>
        <div class="question-item" data-question="q73">
            <span class="question-number">73</span>
            <span>Excessive Shuffling Causes</span>
        </div>
        <div class="question-item" data-question="q74">
            <span class="question-number">74</span>
            <span>Avoiding Full Data Shuffles</span>
        </div>
    </div>
</div>

<!-- Advanced - Partitioning -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="partitioning">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Data Partitioning</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q75">
            <span class="question-number">75</span>
            <span>Data Partitioning Handling</span>
        </div>
        <div class="question-item" data-question="q76">
            <span class="question-number">76</span>
            <span>Partitions in PySpark</span>
        </div>
        <div class="question-item" data-question="q77">
            <span class="question-number">77</span>
            <span>Partitioning Importance</span>
        </div>
        <div class="question-item" data-question="q78">
            <span class="question-number">78</span>
            <span>What is Partitioning?</span>
        </div>
        <div class="question-item" data-question="q79">
            <span class="question-number">79</span>
            <span>Checking Partition Count</span>
        </div>
        <div class="question-item" data-question="q81">
            <span class="question-number">81</span>
            <span>repartition() vs coalesce()</span>
        </div>
        <div class="question-item" data-question="q83">
            <span class="question-number">83</span>
            <span>Using repartition(1)</span>
        </div>
        <div class="question-item" data-question="q84">
            <span class="question-number">84</span>
            <span>spark.sql.shuffle.partitions</span>
        </div>
    </div>
</div>

<!-- Advanced - Broadcast & Accumulators -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="broadcast-accumulators">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Broadcast Variables & Accumulators</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q85">
            <span class="question-number">85</span>
            <span>Broadcast Variables</span>
        </div>
        <div class="question-item" data-question="q88">
            <span class="question-number">88</span>
            <span>Broadcast Variables Under the Hood</span>
        </div>
        <div class="question-item" data-question="q89">
            <span class="question-number">89</span>
            <span>What is Accumulator?</span>
        </div>
        <div class="question-item" data-question="q90">
            <span class="question-number">90</span>
            <span>Accumulators Explained</span>
        </div>
        <div class="question-item" data-question="q91">
            <span class="question-number">91</span>
            <span>Accumulator Usage</span>
        </div>
        <div class="question-item" data-question="q92">
            <span class="question-number">92</span>
            <span>Role of Accumulator Variables</span>
        </div>
    </div>
</div>

<!-- Expert - Query Optimization -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="query-optimization">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-expert">Expert</span>
            <span>Query Optimization</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q93">
            <span class="question-number">93</span>
            <span>Catalyst Optimizer</span>
        </div>
        <div class="question-item" data-question="q95">
            <span class="question-number">95</span>
            <span>Tungsten Execution Engine</span>
        </div>
        <div class="question-item" data-question="q96">
            <span class="question-number">96</span>
            <span>SQL Optimizations Internally</span>
        </div>
        <div class="question-item" data-question="q97">
            <span class="question-number">97</span>
            <span>Predicate Pushdown</span>
        </div>
        <div class="question-item" data-question="q98">
            <span class="question-number">98</span>
            <span>Dynamic Partition Pruning</span>
        </div>
    </div>
</div>

<!-- Advanced - Joins -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="joins">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Join Operations</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q99">
            <span class="question-number">99</span>
            <span>Joins in PySpark</span>
        </div>
        <div class="question-item" data-question="q100">
            <span class="question-number">100</span>
            <span>Joining Two DataFrames</span>
        </div>
        <div class="question-item" data-question="q101">
            <span class="question-number">101</span>
            <span>union() vs join()</span>
        </div>
        <div class="question-item" data-question="q102">
            <span class="question-number">102</span>
            <span>Joins Using RDD</span>
        </div>
        <div class="question-item" data-question="q103">
            <span class="question-number">103</span>
            <span>Join Internals</span>
        </div>
        <div class="question-item" data-question="q104">
            <span class="question-number">104</span>
            <span>Skew Joins Handling</span>
        </div>
    </div>
</div>

<!-- Expert - Execution Plans -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="execution-plans">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-expert">Expert</span>
            <span>Execution Plans & Debugging</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q105">
            <span class="question-number">105</span>
            <span>Using explain() Method</span>
        </div>
        <div class="question-item" data-question="q106">
            <span class="question-number">106</span>
            <span>Viewing Execution Plans</span>
        </div>
        <div class="question-item" data-question="q107">
            <span class="question-number">107</span>
            <span>explain() vs queryExecution()</span>
        </div>
        <div class="question-item" data-question="q108">
            <span class="question-number">108</span>
            <span>Checking Execution Plans</span>
        </div>
    </div>
</div>

<!-- Advanced - Aggregations -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="aggregations">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-advanced">Advanced</span>
            <span>Aggregations & GroupBy</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q109">
            <span class="question-number">109</span>
            <span>Performing Aggregations</span>
        </div>
        <div class="question-item" data-question="q110">
            <span class="question-number">110</span>
            <span>Aggregation Operations</span>
        </div>
        <div class="question-item" data-question="q111">
            <span class="question-number">111</span>
            <span>groupBy and agg Functions</span>
        </div>
        <div class="question-item" data-question="q112">
            <span class="question-number">112</span>
            <span>groupBy and agg Operations</span>
        </div>
        <div class="question-item" data-question="q113">
            <span class="question-number">113</span>
            <span>reduceByKey() vs groupByKey()</span>
        </div>
        <div class="question-item" data-question="q115">
            <span class="question-number">115</span>
            <span>Sorting Within Groups</span>
        </div>
    </div>
</div>

<!-- Expert - Window Functions & Pivoting -->
<div class="difficulty-section">
    <div class="difficulty-header" data-section="window-functions">
        <div class="difficulty-title">
            <span class="difficulty-badge badge-expert">Expert</span>
            <span>Window Functions & Pivoting</span>
        </div>
        <i class="fas fa-chevron-down"></i>
    </div>
    <div class="question-list">
        <div class="question-item" data-question="q116">
            <span class="question-number">116</span>
            <span>Window Functions</span>
        </div>
        <div class="question-item" data-question="q117">
            <span class="question-number">117</span>
            <span>Window Operations</span>
        </div>
        <div class="question-item" data-question="q118">
            <span class="question-number">118</span>
            <span>Using Window Functions</span>
        </div>
        <div class="question-item" data-question="q119">
            <span class="question-number">119</span>
            <span>Running Total & Moving Average</span>
        </div>
        <div class="question-item" data-question="q120">
            <span class="question-number">120</span>
            <span>Pivot and Unpivot Data</span>
        </div>
    </div>
</div>
    </nav>

    <!-- Main Content -->
    <main class="main-content">
        <div class="content-wrapper">
             <!-- Question 1: What is PySpark? -->
            <div class="question-content active" id="q1">
                <div class="question-header">
                    <h1 class="question-title">What is PySpark?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Foundational</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Beginner</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>PySpark</strong> is the Python API for Apache Spark, an open-source, distributed computing framework designed for large-scale data processing. It allows Python developers to harness the power of Spark's distributed computing capabilities using familiar Python syntax.</p>
                        <p style="margin-top: 1rem;">PySpark enables you to write Spark applications using Python programming language, making big data processing accessible to Python developers without needing to learn Scala or Java.</p>
                    </div>

                    <div class="highlight-box">
                        <strong>Key Point:</strong> PySpark = Python + Apache Spark = Powerful Big Data Processing with Python!
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Distributed Computing</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Python API</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Big Data</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Apache Spark</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Data Processing</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Example: Basic PySpark Code
                    </div>
                    
                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-terminal"></i> Simple PySpark Program</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark.sql import SparkSession

# Create a Spark Session
spark = SparkSession.builder \
    .appName("MyFirstPySparkApp") \
    .getOrCreate()

# Create a simple dataset
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
columns = ["Name", "Age"]

# Create DataFrame
df = spark.createDataFrame(data, columns)

# Show the data
df.show()

# Output:
# +-------+---+
# |   Name|Age|
# +-------+---+
# |  Alice| 25|
# |    Bob| 30|
# |Charlie| 35|
# +-------+---+

# Stop the Spark session
spark.stop()</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-lightbulb"></i>
                        Real-World Scenario
                    </div>
                    <div class="scenario-box">
                        <div class="scenario-title">
                            <i class="fas fa-briefcase"></i> E-commerce Analytics
                        </div>
                        <p><strong>Problem:</strong> An e-commerce company has 100 million customer transactions stored across multiple servers. They need to analyze customer purchasing patterns to make recommendations.</p>
                        <p style="margin-top: 1rem;"><strong>Solution with PySpark:</strong></p>
                        <ul style="margin-left: 1.5rem; margin-top: 0.5rem; color: var(--text-secondary);">
                            <li>PySpark reads data from distributed storage (HDFS, S3)</li>
                            <li>Processes data in parallel across multiple machines</li>
                            <li>Aggregates results to identify top products and customer segments</li>
                            <li>Completes analysis in minutes instead of hours</li>
                        </ul>
                    </div>

                    <div class="example-box" style="margin-top: 1.5rem;">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-chart-line"></i> E-commerce Analysis Code</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Read customer transaction data
transactions = spark.read.parquet("s3://company-data/transactions/")

# Group by product and count purchases
popular_products = transactions.groupBy("product_id") \
    .count() \
    .orderBy("count", ascending=False) \
    .limit(10)

# Show top 10 products
popular_products.show()

# Calculate revenue by customer segment
revenue_by_segment = transactions.groupBy("customer_segment") \
    .agg({"amount": "sum"})

revenue_by_segment.show()</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How is PySpark different from Apache Spark?</strong></span>
                            </div>
                            <div class="cross-answer">
                                PySpark is not different from Apache Spark - it's an API (interface) for Spark written in Python. Apache Spark is the core engine written in Scala, while PySpark provides Python bindings to use Spark. Think of it like this: Spark is the car engine, and PySpark is the steering wheel for Python drivers.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How is it different from regular Python?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Regular Python runs on a single machine and processes data sequentially. PySpark distributes data and computation across multiple machines (cluster), processing data in parallel. Regular Python can handle MBs to few GBs, while PySpark can process TBs to PBs of data.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Why do we use Python with Spark?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Python is easy to learn, has rich libraries for data science (NumPy, Pandas, Scikit-learn), and has a large community. PySpark allows data scientists familiar with Python to leverage Spark's distributed computing without learning Scala or Java.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Is PySpark an independent tool or an API?</strong></span>
                            </div>
                            <div class="cross-answer">
                                PySpark is an API (Application Programming Interface), not an independent tool. It's a Python wrapper around Apache Spark's core functionality. You need Apache Spark installed to use PySpark.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can you use PySpark without Spark?</strong></span>
                            </div>
                            <div class="cross-answer">
                                No, you cannot use PySpark without Apache Spark. PySpark requires Spark to be installed because it's just a Python interface to Spark's underlying engine. Without Spark, PySpark has no engine to execute distributed computations.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can Spark run without Python?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Yes! Spark can run independently with Scala, Java, or R. Spark was originally written in Scala. Python (via PySpark) is just one of several language APIs available for Spark.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-image"></i>
                        Visual Understanding
                    </div>
                    <div class="image-container">
                        <svg width="800" height="400" viewBox="0 0 800 400">
                            <!-- Background -->
                            <rect width="800" height="400" fill="#1e293b"/>
                            
                            <!-- Title -->
                            <text x="400" y="40" font-size="24" fill="#0ea5e9" text-anchor="middle" font-weight="bold">PySpark Architecture</text>
                            
                            <!-- Regular Python Box -->
                            <rect x="50" y="100" width="200" height="250" fill="#334155" stroke="#0ea5e9" stroke-width="2" rx="10"/>
                            <text x="150" y="130" font-size="18" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Regular Python</text>
                            <text x="150" y="160" font-size="14" fill="#cbd5e1" text-anchor="middle">Single Machine</text>
                            
                            <!-- Python Icon -->
                            <circle cx="150" cy="200" r="30" fill="#0ea5e9" opacity="0.2"/>
                            <text x="150" y="210" font-size="40" text-anchor="middle"></text>
                            
                            <text x="150" y="260" font-size="13" fill="#cbd5e1" text-anchor="middle"> Sequential Processing</text>
                            <text x="150" y="285" font-size="13" fill="#cbd5e1" text-anchor="middle"> Limited to RAM</text>
                            <text x="150" y="310" font-size="13" fill="#cbd5e1" text-anchor="middle"> MBs to GBs</text>
                            <text x="150" y="335" font-size="13" fill="#cbd5e1" text-anchor="middle"> Single Core</text>
                            
                            <!-- Arrow -->
                            <path d="M 270 225 L 330 225" stroke="#06b6d4" stroke-width="3" fill="none" marker-end="url(#arrowhead)"/>
                            <text x="300" y="215" font-size="14" fill="#06b6d4" text-anchor="middle" font-weight="bold">VS</text>
                            
                            <!-- PySpark Box -->
                            <rect x="350" y="100" width="400" height="250" fill="#334155" stroke="#10b981" stroke-width="2" rx="10"/>
                            <text x="550" y="130" font-size="18" fill="#f1f5f9" text-anchor="middle" font-weight="bold">PySpark (Python + Spark)</text>
                            <text x="550" y="160" font-size="14" fill="#cbd5e1" text-anchor="middle">Distributed Cluster</text>
                            
                            <!-- Cluster Nodes -->
                            <circle cx="450" cy="210" r="25" fill="#10b981" opacity="0.3"/>
                            <text x="450" y="218" font-size="30" text-anchor="middle"></text>
                            <text x="450" y="245" font-size="11" fill="#cbd5e1" text-anchor="middle">Node 1</text>
                            
                            <circle cx="550" cy="210" r="25" fill="#10b981" opacity="0.3"/>
                            <text x="550" y="218" font-size="30" text-anchor="middle"></text>
                            <text x="550" y="245" font-size="11" fill="#cbd5e1" text-anchor="middle">Node 2</text>
                            
                            <circle cx="650" cy="210" r="25" fill="#10b981" opacity="0.3"/>
                            <text x="650" y="218" font-size="30" text-anchor="middle"></text>
                            <text x="650" y="245" font-size="11" fill="#cbd5e1" text-anchor="middle">Node N</text>
                            
                            <text x="550" y="280" font-size="13" fill="#cbd5e1" text-anchor="middle"> Parallel Processing</text>
                            <text x="550" y="305" font-size="13" fill="#cbd5e1" text-anchor="middle"> Distributed Memory</text>
                            <text x="550" y="330" font-size="13" fill="#cbd5e1" text-anchor="middle"> GBs to PBs (Petabytes)</text>
                            
                            <!-- Arrow marker definition -->
                            <defs>
                                <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                    <polygon points="0 0, 10 3, 0 6" fill="#06b6d4"/>
                                </marker>
                            </defs>
                        </svg>
                        <p class="image-caption">PySpark vs Regular Python: Distributed vs Single Machine Processing</p>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')" disabled>
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 2: Apache Spark & PySpark Relation -->
            <div class="question-content" id="q2">
                <div class="question-header">
                    <h1 class="question-title">What is Apache Spark and how is PySpark related to it?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Foundational</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Beginner</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Apache Spark</strong> is an open-source, unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general execution graphs for data analysis.</p>
                        <p style="margin-top: 1rem;"><strong>PySpark</strong> is the Python API for Apache Spark. It allows Python programmers to interface with the Spark framework and leverage its distributed computing capabilities using Python syntax and libraries.</p>
                    </div>

                    <div class="highlight-box">
                        <strong>Relationship:</strong> Apache Spark is the engine, PySpark is the Python steering wheel to control that engine!
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Apache Spark</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Unified Analytics Engine</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> PySpark API</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Multi-Language Support</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Distributed Processing</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-diagram-project"></i>
                        Architecture Overview
                    </div>
                    
                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Apache Spark</th>
                                    <th>PySpark</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Nature</strong></td>
                                    <td>Core computing engine</td>
                                    <td>Python API/Interface</td>
                                </tr>
                                <tr>
                                    <td><strong>Written In</strong></td>
                                    <td>Scala</td>
                                    <td>Python (wrapper)</td>
                                </tr>
                                <tr>
                                    <td><strong>Languages Supported</strong></td>
                                    <td>Scala, Java, Python, R, SQL</td>
                                    <td>Python only</td>
                                </tr>
                                <tr>
                                    <td><strong>Performance</strong></td>
                                    <td>Native (fastest)</td>
                                    <td>Slightly slower due to serialization</td>
                                </tr>
                                <tr>
                                    <td><strong>Use Case</strong></td>
                                    <td>Universal big data processing</td>
                                    <td>Python developers working with big data</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Example: Comparing Spark APIs
                    </div>
                    
                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-brands fa-python"></i> PySpark (Python)</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># PySpark Example
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PythonExample").getOrCreate()

# Read CSV file
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Filter and aggregate
result = df.filter(df.age > 25).groupBy("department").count()

result.show()</code></pre>
                    </div>

                    <div class="example-box" style="margin-top: 1.5rem;">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-code"></i> Spark with Scala</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>// Scala Spark Example
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder.appName("ScalaExample").getOrCreate()

// Read CSV file
val df = spark.read.option("header", "true").csv("data.csv")

// Filter and aggregate
val result = df.filter($"age" > 25).groupBy("department").count()

result.show()</code></pre>
                    </div>

                    <div class="note-box">
                        <strong>Note:</strong> Both examples do the same thing - demonstrating that Spark supports multiple languages while maintaining similar functionality across all APIs.
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What languages can Spark be used with other than Python?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Spark supports multiple languages: <strong>Scala</strong> (native language, best performance), <strong>Java</strong> (enterprise applications), <strong>Python</strong> (via PySpark), <strong>R</strong> (via SparkR for statistical computing), and <strong>SQL</strong> (via Spark SQL).
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can Spark work on top of HDFS?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Yes! Spark can read from and write to HDFS (Hadoop Distributed File System). In fact, Spark is often used with HDFS for storage in big data ecosystems. However, Spark can also work with other storage systems like Amazon S3, Azure Blob Storage, Apache Cassandra, and local file systems.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-image"></i>
                        Visual Understanding
                    </div>
                    <div class="image-container">
                        <svg width="800" height="500" viewBox="0 0 800 500">
                            <!-- Background -->
                            <rect width="800" height="500" fill="#1e293b"/>
                            
                            <!-- Title -->
                            <text x="400" y="35" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Apache Spark Ecosystem & Language APIs</text>
                            
                            <!-- Spark Core (Center) -->
                            <ellipse cx="400" cy="280" rx="120" ry="80" fill="#334155" stroke="#10b981" stroke-width="3"/>
                            <text x="400" y="270" font-size="20" fill="#10b981" text-anchor="middle" font-weight="bold">Apache Spark</text>
                            <text x="400" y="295" font-size="14" fill="#cbd5e1" text-anchor="middle">Core Engine</text>
                            <text x="400" y="315" font-size="12" fill="#94a3b8" text-anchor="middle">(Written in Scala)</text>
                            
                            <!-- Language APIs around Spark -->
                            
                            <!-- PySpark (Top Left) -->
                            <circle cx="200" cy="140" r="60" fill="#3b82f6" opacity="0.2" stroke="#3b82f6" stroke-width="2"/>
                            <text x="200" y="135" font-size="32" text-anchor="middle"></text>
                            <text x="200" y="160" font-size="15" fill="#f1f5f9" text-anchor="middle" font-weight="bold">PySpark</text>
                            <path d="M 240 170 L 340 240" stroke="#3b82f6" stroke-width="2" stroke-dasharray="5,5"/>
                            
                            <!-- Scala API (Top Right) -->
                            <circle cx="600" cy="140" r="60" fill="#ef4444" opacity="0.2" stroke="#ef4444" stroke-width="2"/>
                            <text x="600" y="140" font-size="35" text-anchor="middle"></text>
                            <text x="600" y="160" font-size="15" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Scala API</text>
                            <path d="M 560 170 L 460 240" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5"/>
                            
                            <!-- Java API (Right) -->
                            <circle cx="680" cy="280" r="60" fill="#f59e0b" opacity="0.2" stroke="#f59e0b" stroke-width="2"/>
                            <text x="680" y="280" font-size="35" text-anchor="middle"></text>
                            <text x="680" y="300" font-size="15" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Java API</text>
                            <path d="M 660 280 L 520 280" stroke="#f59e0b" stroke-width="2" stroke-dasharray="5,5"/>
                            
                            <!-- R API (Bottom Right) -->
                            <circle cx="600" cy="420" r="60" fill="#8b5cf6" opacity="0.2" stroke="#8b5cf6" stroke-width="2"/>
                            <text x="600" y="420" font-size="35" text-anchor="middle"></text>
                            <text x="600" y="440" font-size="15" fill="#f1f5f9" text-anchor="middle" font-weight="bold">SparkR</text>
                            <path d="M 560 390 L 460 320" stroke="#8b5cf6" stroke-width="2" stroke-dasharray="5,5"/>
                            
                            <!-- SQL (Bottom Left) -->
                            <circle cx="200" cy="420" r="60" fill="#06b6d4" opacity="0.2" stroke="#06b6d4" stroke-width="2"/>
                            <text x="200" y="420" font-size="35" text-anchor="middle"></text>
                            <text x="200" y="440" font-size="15" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Spark SQL</text>
                            <path d="M 240 390 L 340 320" stroke="#06b6d4" stroke-width="2" stroke-dasharray="5,5"/>
                            
                            <!-- Storage Systems (Bottom) -->
                            <text x="400" y="485" font-size="13" fill="#94a3b8" text-anchor="middle">Data Sources: HDFS, S3, Cassandra, HBase, PostgreSQL, etc.</text>
                            
                        </svg>
                        <p class="image-caption">Apache Spark supports multiple language APIs - all connecting to the same core engine</p>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Continue with more questions... Due to length, I'll create a few more key questions -->
            
            <!-- Question 3: Main Components of Spark -->
            <div class="question-content" id="q3">
                <div class="question-header">
                    <h1 class="question-title">What are the main components of Apache Spark?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Foundational</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Beginner</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p>Apache Spark consists of <strong>five main components</strong> that work together to provide a unified platform for big data processing:</p>
                        <ul style="margin-left: 2rem; margin-top: 1rem; color: var(--text-secondary); line-height: 2;">
                            <li><strong style="color: var(--primary-color);">Spark Core:</strong> The foundation that provides basic I/O functionalities, task scheduling, and memory management</li>
                            <li><strong style="color: var(--primary-color);">Spark SQL:</strong> Module for working with structured data using SQL queries and DataFrames</li>
                            <li><strong style="color: var(--primary-color);">Spark Streaming:</strong> Enables processing of live data streams in real-time</li>
                            <li><strong style="color: var(--primary-color);">MLlib:</strong> Machine learning library with common algorithms and utilities</li>
                            <li><strong style="color: var(--primary-color);">GraphX:</strong> API for graphs and graph-parallel computation</li>
                        </ul>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Spark Core</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Spark SQL</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Spark Streaming</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> MLlib</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> GraphX</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-layer-group"></i>
                        Detailed Component Breakdown
                    </div>

                    <div class="collapsible-section">
                        <div class="collapsible-header">
                            <h3><i class="fas fa-cube" style="color: #10b981;"></i> 1. Spark Core</h3>
                            <i class="fas fa-chevron-down"></i>
                        </div>
                        <div class="collapsible-content">
                            <p style="color: var(--text-secondary); margin-bottom: 1rem;">The foundational component that provides:</p>
                            <ul style="margin-left: 1.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li>Basic I/O functionalities</li>
                                <li>Task scheduling and distribution</li>
                                <li>Memory management</li>
                                <li>Fault recovery</li>
                                <li>RDD (Resilient Distributed Dataset) API</li>
                            </ul>
                            
                            <div class="example-box" style="margin-top: 1rem;">
                                <div class="example-header">
                                    <span class="example-title"><i class="fas fa-code"></i> Spark Core Example</span>
                                    <button class="copy-btn" onclick="copyCode(this)">
                                        <i class="fas fa-copy"></i> Copy
                                    </button>
                                </div>
                                <pre><code># Using RDD from Spark Core
from pyspark import SparkContext

sc = SparkContext("local", "Core Example")

# Create RDD
numbers = sc.parallelize([1, 2, 3, 4, 5])

# Transform and action
squared = numbers.map(lambda x: x ** 2)
result = squared.collect()

print(result)  # Output: [1, 4, 9, 16, 25]</code></pre>
                            </div>
                        </div>
                    </div>

                    <div class="collapsible-section">
                        <div class="collapsible-header">
                            <h3><i class="fas fa-database" style="color: #3b82f6;"></i> 2. Spark SQL</h3>
                            <i class="fas fa-chevron-down"></i>
                        </div>
                        <div class="collapsible-content">
                            <p style="color: var(--text-secondary); margin-bottom: 1rem;">Works with structured and semi-structured data:</p>
                            <ul style="margin-left: 1.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li>SQL query support</li>
                                <li>DataFrame and Dataset APIs</li>
                                <li>Integration with Hive, JSON, Parquet, CSV</li>
                                <li>Optimized query execution with Catalyst optimizer</li>
                            </ul>
                            
                            <div class="example-box" style="margin-top: 1rem;">
                                <div class="example-header">
                                    <span class="example-title"><i class="fas fa-code"></i> Spark SQL Example</span>
                                    <button class="copy-btn" onclick="copyCode(this)">
                                        <i class="fas fa-copy"></i> Copy
                                    </button>
                                </div>
                                <pre><code># Using Spark SQL
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SQL Example").getOrCreate()

# Create DataFrame
data = [("Alice", 25, "Engineer"),
        ("Bob", 30, "Manager"),
        ("Charlie", 35, "Engineer")]

df = spark.createDataFrame(data, ["name", "age", "role"])

# Register as temporary view
df.createOrReplaceTempView("employees")

# Run SQL query
result = spark.sql("""
    SELECT role, AVG(age) as avg_age 
    FROM employees 
    GROUP BY role
""")

result.show()
# +--------+-------+
# |    role|avg_age|
# +--------+-------+
# |Engineer|   30.0|
# | Manager|   30.0|
# +--------+-------+</code></pre>
                            </div>
                        </div>
                    </div>

                    <div class="collapsible-section">
                        <div class="collapsible-header">
                            <h3><i class="fas fa-stream" style="color: #06b6d4;"></i> 3. Spark Streaming</h3>
                            <i class="fas fa-chevron-down"></i>
                        </div>
                        <div class="collapsible-content">
                            <p style="color: var(--text-secondary); margin-bottom: 1rem;">Real-time data processing:</p>
                            <ul style="margin-left: 1.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li>Process live data streams</li>
                                <li>Mini-batch processing (DStreams)</li>
                                <li>Structured Streaming with DataFrames</li>
                                <li>Integration with Kafka, Flume, Kinesis</li>
                            </ul>
                            
                            <div class="example-box" style="margin-top: 1rem;">
                                <div class="example-header">
                                    <span class="example-title"><i class="fas fa-code"></i> Streaming Example</span>
                                    <button class="copy-btn" onclick="copyCode(this)">
                                        <i class="fas fa-copy"></i> Copy
                                    </button>
                                </div>
                                <pre><code># Structured Streaming Example
from pyspark.sql import SparkSession
from pyspark.sql.functions import window, col

spark = SparkSession.builder.appName("Streaming").getOrCreate()

# Read streaming data from socket
lines = spark.readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# Count words in 10-second windows
words = lines.select(
    explode(split(col("value"), " ")).alias("word")
)

word_counts = words.groupBy("word").count()

# Write stream to console
query = word_counts.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()</code></pre>
                            </div>
                        </div>
                    </div>

                    <div class="collapsible-section">
                        <div class="collapsible-header">
                            <h3><i class="fas fa-brain" style="color: #f59e0b;"></i> 4. MLlib (Machine Learning)</h3>
                            <i class="fas fa-chevron-down"></i>
                        </div>
                        <div class="collapsible-content">
                            <p style="color: var(--text-secondary); margin-bottom: 1rem;">Scalable machine learning library:</p>
                            <ul style="margin-left: 1.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li>Classification and Regression algorithms</li>
                                <li>Clustering algorithms</li>
                                <li>Collaborative filtering</li>
                                <li>Dimensionality reduction</li>
                                <li>Feature extraction and transformation</li>
                            </ul>
                            
                            <div class="example-box" style="margin-top: 1rem;">
                                <div class="example-header">
                                    <span class="example-title"><i class="fas fa-code"></i> MLlib Example</span>
                                    <button class="copy-btn" onclick="copyCode(this)">
                                        <i class="fas fa-copy"></i> Copy
                                    </button>
                                </div>
                                <pre><code># Machine Learning with MLlib
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler

# Prepare training data
training_data = spark.createDataFrame([
    (1.0, [2.0, 1.0]),
    (0.0, [1.0, 2.0]),
    (1.0, [3.0, 1.5]),
    (0.0, [1.5, 2.5])
], ["label", "features"])

# Create and train model
lr = LogisticRegression(maxIter=10)
model = lr.fit(training_data)

# Make predictions
test_data = spark.createDataFrame([
    ([2.5, 1.2],),
    ([1.2, 2.3],)
], ["features"])

predictions = model.transform(test_data)
predictions.select("features", "prediction").show()</code></pre>
                            </div>
                        </div>
                    </div>

                    <div class="collapsible-section">
                        <div class="collapsible-header">
                            <h3><i class="fas fa-project-diagram" style="color: #8b5cf6;"></i> 5. GraphX</h3>
                            <i class="fas fa-chevron-down"></i>
                        </div>
                        <div class="collapsible-content">
                            <p style="color: var(--text-secondary); margin-bottom: 1rem;">Graph processing library:</p>
                            <ul style="margin-left: 1.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li>Graph construction and transformation</li>
                                <li>Graph algorithms (PageRank, Triangle counting)</li>
                                <li>Social network analysis</li>
                                <li>Recommendation systems</li>
                            </ul>
                            
                            <div class="note-box" style="margin-top: 1rem;">
                                <strong>Note:</strong> GraphX is primarily available in Scala. For Python users, consider using GraphFrames library which provides similar functionality.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which components are commonly used in data analytics pipelines?</strong></span>
                            </div>
                            <div class="cross-answer">
                                The most commonly used components are <strong>Spark SQL</strong> (for data processing and analysis with DataFrames) and <strong>MLlib</strong> (for machine learning). Spark SQL is used in almost every data pipeline because it provides an easy and optimized way to work with structured data. Spark Streaming is used when real-time processing is required.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-image"></i>
                        Visual Understanding
                    </div>
                    <div class="image-container">
                        <svg width="800" height="600" viewBox="0 0 800 600">
                            <!-- Background -->
                            <rect width="800" height="600" fill="#1e293b"/>
                            
                            <!-- Title -->
                            <text x="400" y="40" font-size="24" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Apache Spark Components Stack</text>
                            
                            <!-- Layer 5 - Applications -->
                            <rect x="100" y="80" width="600" height="60" fill="#8b5cf6" opacity="0.3" stroke="#8b5cf6" stroke-width="2" rx="8"/>
                            <text x="400" y="115" font-size="18" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Applications & Use Cases</text>
                            
                            <!-- Layer 4 - Libraries -->
                            <rect x="100" y="160" width="140" height="100" fill="#3b82f6" opacity="0.3" stroke="#3b82f6" stroke-width="2" rx="8"/>
                            <text x="170" y="195" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Spark SQL</text>
                            <text x="170" y="215" font-size="11" fill="#cbd5e1" text-anchor="middle">Structured</text>
                            <text x="170" y="230" font-size="11" fill="#cbd5e1" text-anchor="middle">Data</text>
                            <text x="170" y="245" font-size="11" fill="#cbd5e1" text-anchor="middle">Processing</text>
                            
                            <rect x="260" y="160" width="130" height="100" fill="#06b6d4" opacity="0.3" stroke="#06b6d4" stroke-width="2" rx="8"/>
                            <text x="325" y="195" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Streaming</text>
                            <text x="325" y="215" font-size="11" fill="#cbd5e1" text-anchor="middle">Real-time</text>
                            <text x="325" y="230" font-size="11" fill="#cbd5e1" text-anchor="middle">Data</text>
                            <text x="325" y="245" font-size="11" fill="#cbd5e1" text-anchor="middle">Processing</text>
                            
                            <rect x="410" y="160" width="130" height="100" fill="#f59e0b" opacity="0.3" stroke="#f59e0b" stroke-width="2" rx="8"/>
                            <text x="475" y="195" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">MLlib</text>
                            <text x="475" y="215" font-size="11" fill="#cbd5e1" text-anchor="middle">Machine</text>
                            <text x="475" y="230" font-size="11" fill="#cbd5e1" text-anchor="middle">Learning</text>
                            <text x="475" y="245" font-size="11" fill="#cbd5e1" text-anchor="middle">Library</text>
                            
                            <rect x="560" y="160" width="140" height="100" fill="#8b5cf6" opacity="0.3" stroke="#8b5cf6" stroke-width="2" rx="8"/>
                            <text x="630" y="195" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">GraphX</text>
                            <text x="630" y="215" font-size="11" fill="#cbd5e1" text-anchor="middle">Graph</text>
                            <text x="630" y="230" font-size="11" fill="#cbd5e1" text-anchor="middle">Processing</text>
                            
                            <!-- Layer 3 - Spark Core -->
                            <rect x="100" y="280" width="600" height="80" fill="#10b981" opacity="0.3" stroke="#10b981" stroke-width="3" rx="8"/>
                            <text x="400" y="310" font-size="20" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Spark Core (RDD API)</text>
                            <text x="400" y="335" font-size="13" fill="#cbd5e1" text-anchor="middle">Task Scheduling  Memory Management  Fault Recovery  I/O</text>
                            
                            <!-- Layer 2 - Cluster Managers -->
                            <rect x="100" y="380" width="600" height="80" fill="#ef4444" opacity="0.3" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="400" y="410" font-size="18" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Cluster Managers</text>
                            <text x="400" y="435" font-size="13" fill="#cbd5e1" text-anchor="middle">Standalone  YARN  Mesos  Kubernetes</text>
                            
                            <!-- Layer 1 - Storage -->
                            <rect x="100" y="480" width="600" height="80" fill="#0ea5e9" opacity="0.3" stroke="#0ea5e9" stroke-width="2" rx="8"/>
                            <text x="400" y="510" font-size="18" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Storage Layer</text>
                            <text x="400" y="535" font-size="13" fill="#cbd5e1" text-anchor="middle">HDFS  S3  Cassandra  HBase  Local FS  Azure Blob</text>
                            
                        </svg>
                        <p class="image-caption">Layered architecture showing how Spark components work together</p>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 4: Spark vs Hadoop MapReduce -->
            <div class="question-content" id="q4">
                <div class="question-header">
                    <h1 class="question-title">How does Spark differ from Hadoop MapReduce?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Basic</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Basic</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Apache Spark</strong> and <strong>Hadoop MapReduce</strong> are both distributed computing frameworks for processing big data, but they differ significantly in architecture, speed, and ease of use.</p>
                        <p style="margin-top: 1rem;">Spark is 10-100x faster than MapReduce because it processes data in-memory, while MapReduce writes intermediate results to disk after each step, making it slower for iterative algorithms.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> In-Memory Processing</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Disk-Based Processing</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> MapReduce</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Iterative Processing</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-balance-scale"></i>
                        Detailed Comparison
                    </div>
                    
                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Feature</th>
                                    <th>Apache Spark</th>
                                    <th>Hadoop MapReduce</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Processing Speed</strong></td>
                                    <td>10-100x faster (in-memory)</td>
                                    <td>Slower (disk-based)</td>
                                </tr>
                                <tr>
                                    <td><strong>Data Processing</strong></td>
                                    <td>In-memory (RAM)</td>
                                    <td>Disk-based (HDFS)</td>
                                </tr>
                                <tr>
                                    <td><strong>Ease of Use</strong></td>
                                    <td>High-level APIs, simpler code</td>
                                    <td>Complex, verbose code</td>
                                </tr>
                                <tr>
                                    <td><strong>Real-time Processing</strong></td>
                                    <td>Yes (Spark Streaming)</td>
                                    <td>No (batch only)</td>
                                </tr>
                                <tr>
                                    <td><strong>Machine Learning</strong></td>
                                    <td>Built-in MLlib library</td>
                                    <td>Requires external tools (Mahout)</td>
                                </tr>
                                <tr>
                                    <td><strong>Iterative Processing</strong></td>
                                    <td>Excellent (caches data)</td>
                                    <td>Poor (disk I/O overhead)</td>
                                </tr>
                                <tr>
                                    <td><strong>Fault Tolerance</strong></td>
                                    <td>RDD lineage (recomputation)</td>
                                    <td>Data replication</td>
                                </tr>
                                <tr>
                                    <td><strong>Languages</strong></td>
                                    <td>Scala, Python, Java, R, SQL</td>
                                    <td>Java, Python (limited)</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Code Comparison
                    </div>
                    
                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-bolt"></i> Spark - Word Count (5 lines)</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkContext

sc = SparkContext("local", "WordCount")
text = sc.textFile("input.txt")
counts = text.flatMap(lambda line: line.split()) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile("output")</code></pre>
                    </div>

                    <div class="example-box" style="margin-top: 1.5rem;">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-hard-drive"></i> MapReduce - Word Count (50+ lines)</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>// MapReduce requires separate Mapper and Reducer classes
// Mapper Class
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
    
    public void map(LongWritable key, Text value, Context context) 
        throws IOException, InterruptedException {
        String line = value.toString();
        StringTokenizer tokenizer = new StringTokenizer(line);
        while (tokenizer.hasMoreTokens()) {
            word.set(tokenizer.nextToken());
            context.write(word, one);
        }
    }
}

// Reducer Class
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    public void reduce(Text key, Iterable<IntWritable> values, Context context) 
        throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }
        context.write(key, new IntWritable(sum));
    }
}

// Driver Code (additional 20+ lines needed)
// Plus configuration, job setup, input/output paths, etc.</code></pre>
                    </div>

                    <div class="highlight-box">
                        <strong>Key Insight:</strong> Spark accomplishes in 5-10 lines what MapReduce requires 50+ lines to do!
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-lightbulb"></i>
                        Real-World Scenario
                    </div>
                    <div class="scenario-box">
                        <div class="scenario-title">
                            <i class="fas fa-chart-line"></i> Machine Learning on User Behavior Data
                        </div>
                        <p><strong>Problem:</strong> A social media company needs to run machine learning algorithms on 500GB of user behavior data to recommend content. The algorithm requires 10 iterations over the data.</p>
                        
                        <p style="margin-top: 1rem;"><strong>With MapReduce:</strong></p>
                        <ul style="margin-left: 1.5rem; color: var(--text-secondary);">
                            <li>Each iteration reads from disk  processes  writes to disk</li>
                            <li>10 iterations = 10 disk reads + 10 disk writes</li>
                            <li>Total time: ~10 hours</li>
                        </ul>

                        <p style="margin-top: 1rem;"><strong>With Spark:</strong></p>
                        <ul style="margin-left: 1.5rem; color: var(--text-secondary);">
                            <li>Data loaded into memory once</li>
                            <li>All 10 iterations happen in memory</li>
                            <li>Total time: ~30 minutes</li>
                        </ul>

                        <div class="highlight-box" style="margin-top: 1rem;">
                            <strong>Result:</strong> Spark is 20x faster! This is why Spark dominates for machine learning and iterative workloads.
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What are the main differences between Hadoop MapReduce and Spark?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Main differences: <strong>1) Speed:</strong> Spark is 10-100x faster due to in-memory processing. <strong>2) Ease of use:</strong> Spark has high-level APIs while MapReduce requires verbose Java code. <strong>3) Real-time:</strong> Spark supports streaming, MapReduce doesn't. <strong>4) Iterative:</strong> Spark excels at iterative algorithms (ML), MapReduce struggles.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which one is faster and why?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Spark is significantly faster because it keeps data in RAM (memory) between operations, while MapReduce writes intermediate results to disk after every operation. Disk I/O is 100x slower than memory access, which is why Spark achieves 10-100x speed improvements, especially for iterative workloads.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Why do you think Spark is faster than Hadoop?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Spark's speed advantage comes from: <strong>1) In-memory computation:</strong> Data stays in RAM instead of being written to disk. <strong>2) DAG optimization:</strong> Spark optimizes the execution plan before running. <strong>3) Lazy evaluation:</strong> Multiple transformations are combined and executed together. <strong>4) Better pipelining:</strong> Operations are pipelined without intermediate disk writes.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-image"></i>
                        Visual Understanding
                    </div>
                    <div class="image-container">
                        <svg width="800" height="450" viewBox="0 0 800 450">
                            <rect width="800" height="450" fill="#1e293b"/>
                            
                            <text x="400" y="35" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Processing Flow: Spark vs MapReduce</text>
                            
                            <!-- MapReduce Flow (Top) -->
                            <text x="400" y="75" font-size="18" fill="#ef4444" text-anchor="middle" font-weight="bold">Hadoop MapReduce (Disk-Based)</text>
                            
                            <rect x="50" y="90" width="100" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="100" y="115" font-size="12" fill="#f1f5f9" text-anchor="middle">Read from</text>
                            <text x="100" y="132" font-size="12" fill="#f1f5f9" text-anchor="middle">HDFS</text>
                            
                            <path d="M 160 120 L 200 120" stroke="#ef4444" stroke-width="2" marker-end="url(#arrowred)"/>
                            
                            <rect x="210" y="90" width="80" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="250" y="125" font-size="12" fill="#f1f5f9" text-anchor="middle">Map</text>
                            
                            <path d="M 300 120 L 330 120" stroke="#ef4444" stroke-width="2" marker-end="url(#arrowred)"/>
                            
                            <rect x="340" y="90" width="100" height="60" fill="#1e1e1e" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="390" y="115" font-size="11" fill="#ef4444" text-anchor="middle" font-weight="bold">Write to</text>
                            <text x="390" y="132" font-size="11" fill="#ef4444" text-anchor="middle" font-weight="bold">Disk </text>
                            
                            <path d="M 450 120 L 480 120" stroke="#ef4444" stroke-width="2" marker-end="url(#arrowred)"/>
                            
                            <rect x="490" y="90" width="80" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="530" y="125" font-size="12" fill="#f1f5f9" text-anchor="middle">Reduce</text>
                            
                            <path d="M 580 120 L 610 120" stroke="#ef4444" stroke-width="2" marker-end="url(#arrowred)"/>
                            
                            <rect x="620" y="90" width="100" height="60" fill="#1e1e1e" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="670" y="115" font-size="11" fill="#ef4444" text-anchor="middle" font-weight="bold">Write to</text>
                            <text x="670" y="132" font-size="11" fill="#ef4444" text-anchor="middle" font-weight="bold">Disk </text>
                            
                            <text x="400" y="180" font-size="13" fill="#94a3b8" text-anchor="middle" font-style="italic">Multiple disk I/O operations = SLOW </text>
                            
                            <!-- Spark Flow (Bottom) -->
                            <text x="400" y="230" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">Apache Spark (In-Memory)</text>
                            
                            <rect x="50" y="245" width="100" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="100" y="270" font-size="12" fill="#f1f5f9" text-anchor="middle">Read from</text>
                            <text x="100" y="287" font-size="12" fill="#f1f5f9" text-anchor="middle">Storage</text>
                            
                            <path d="M 160 275 L 200 275" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                            
                            <!-- Memory Cloud -->
                            <ellipse cx="450" cy="275" rx="230" ry="70" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="3" stroke-dasharray="5,5"/>
                            <text x="450" y="250" font-size="14" fill="#10b981" text-anchor="middle" font-weight="bold"> IN-MEMORY PROCESSING (RAM)</text>
                            
                            <rect x="220" y="260" width="80" height="40" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="260" y="285" font-size="11" fill="#f1f5f9" text-anchor="middle">Transform</text>
                            
                            <path d="M 310 280 L 350 280" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                            
                            <rect x="360" y="260" width="80" height="40" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="400" y="285" font-size="11" fill="#f1f5f9" text-anchor="middle">Transform</text>
                            
                            <path d="M 450 280 L 490 280" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                            
                            <rect x="500" y="260" width="80" height="40" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="540" y="285" font-size="11" fill="#f1f5f9" text-anchor="middle">Action</text>
                            
                            <path d="M 590 275 L 630 275" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                            
                            <rect x="640" y="245" width="100" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="690" y="270" font-size="12" fill="#f1f5f9" text-anchor="middle">Write Only</text>
                            <text x="690" y="287" font-size="12" fill="#f1f5f9" text-anchor="middle">Final Result</text>
                            
                            <text x="400" y="340" font-size="13" fill="#94a3b8" text-anchor="middle" font-style="italic">All processing in memory = FAST </text>
                            
                            <!-- Performance Comparison -->
                            <rect x="50" y="370" width="700" height="60" fill="rgba(14, 165, 233, 0.1)" stroke="#0ea5e9" stroke-width="2" rx="8"/>
                            <text x="400" y="395" font-size="16" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Speed Comparison</text>
                            <text x="400" y="417" font-size="14" fill="#cbd5e1" text-anchor="middle">Spark: 10-100x faster  MapReduce: Multiple disk I/O bottlenecks</text>
                            
                            <defs>
                                <marker id="arrowred" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                    <polygon points="0 0, 10 3, 0 6" fill="#ef4444"/>
                                </marker>
                                <marker id="arrowgreen" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                    <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                                </marker>
                            </defs>
                        </svg>
                        <p class="image-caption">Spark processes data in-memory while MapReduce relies on disk I/O</p>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 5: Problems PySpark Solves -->
            <div class="question-content" id="q5">
                <div class="question-header">
                    <h1 class="question-title">What kind of problems does PySpark solve?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Basic</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Basic</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p>PySpark solves <strong>big data processing problems</strong> that cannot be handled by traditional tools due to data volume, velocity, or complexity. It enables distributed processing of massive datasets across clusters of computers.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Big Data Processing</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Distributed Computing</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> ETL Pipelines</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Real-time Analytics</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Machine Learning at Scale</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-tasks"></i>
                        Problems PySpark Solves
                    </div>

                    <div class="highlight-box">
                        <strong>1. Processing Data That Doesn't Fit in Memory</strong>
                        <p style="margin-top: 0.5rem;">When you have TBs or PBs of data, a single machine's RAM isn't enough. PySpark distributes data across multiple machines.</p>
                    </div>

                    <div class="highlight-box">
                        <strong>2. Slow Processing with Single-Machine Tools</strong>
                        <p style="margin-top: 0.5rem;">Operations that take hours with pandas can complete in minutes with PySpark through parallel processing.</p>
                    </div>

                    <div class="highlight-box">
                        <strong>3. ETL Pipelines for Big Data</strong>
                        <p style="margin-top: 0.5rem;">Extract, Transform, Load operations on large datasets from multiple sources (databases, data lakes, APIs).</p>
                    </div>

                    <div class="highlight-box">
                        <strong>4. Real-time Stream Processing</strong>
                        <p style="margin-top: 0.5rem;">Process live data streams from IoT devices, social media, financial transactions, etc.</p>
                    </div>

                    <div class="highlight-box">
                        <strong>5. Machine Learning at Scale</strong>
                        <p style="margin-top: 0.5rem;">Train ML models on massive datasets that scikit-learn or pandas cannot handle.</p>
                    </div>

                    <div class="highlight-box">
                        <strong>6. Complex Data Transformations</strong>
                        <p style="margin-top: 0.5rem;">Perform complex joins, aggregations, and transformations on distributed data.</p>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-lightbulb"></i>
                        Real-World Use Cases
                    </div>

                    <div class="scenario-box">
                        <div class="scenario-title">
                            <i class="fas fa-shopping-cart"></i> E-commerce Recommendation Engine
                        </div>
                        <p><strong>Problem:</strong> Amazon needs to analyze 100 million customer transactions daily to generate personalized product recommendations.</p>
                        <p style="margin-top: 0.5rem;"><strong>Solution:</strong> PySpark processes user behavior data in parallel, trains collaborative filtering models using MLlib, and generates recommendations in near real-time.</p>
                    </div>

                    <div class="scenario-box">
                        <div class="scenario-title">
                            <i class="fas fa-video"></i> Netflix Content Analysis
                        </div>
                        <p><strong>Problem:</strong> Analyze viewing patterns of 200+ million subscribers to optimize content delivery and recommendations.</p>
                        <p style="margin-top: 0.5rem;"><strong>Solution:</strong> PySpark processes billions of viewing events, computes user preferences, and predicts content popularity.</p>
                    </div>

                    <div class="scenario-box">
                        <div class="scenario-title">
                            <i class="fas fa-heartbeat"></i> Healthcare Data Analytics
                        </div>
                        <p><strong>Problem:</strong> Hospital network needs to analyze 10 years of patient records (50TB) to identify disease patterns.</p>
                        <p style="margin-top: 0.5rem;"><strong>Solution:</strong> PySpark aggregates patient data across facilities, performs statistical analysis, and identifies risk factors.</p>
                    </div>

                    <div class="scenario-box">
                        <div class="scenario-title">
                            <i class="fas fa-chart-line"></i> Financial Fraud Detection
                        </div>
                        <p><strong>Problem:</strong> Process millions of credit card transactions per hour to detect fraudulent patterns in real-time.</p>
                        <p style="margin-top: 0.5rem;"><strong>Solution:</strong> Spark Streaming analyzes transactions as they occur, applies ML models, and flags suspicious activity instantly.</p>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Example: Processing Large Dataset
                    </div>
                    
                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-database"></i> Analyzing 1TB of Log Data</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg

spark = SparkSession.builder.appName("LogAnalysis").getOrCreate()

# Read 1TB of log files from S3
logs = spark.read.json("s3://company-logs/2024/*")

# Problem 1: Count errors by type (distributed across cluster)
error_counts = logs.filter(col("level") == "ERROR") \
    .groupBy("error_type") \
    .agg(count("*").alias("count")) \
    .orderBy(col("count").desc())

error_counts.show()

# Problem 2: Calculate average response time by endpoint
response_times = logs.groupBy("endpoint") \
    .agg(avg("response_time").alias("avg_response")) \
    .orderBy(col("avg_response").desc())

response_times.show()

# Problem 3: Identify problematic servers
problem_servers = logs.filter(col("response_time") > 5000) \
    .groupBy("server_id") \
    .agg(count("*").alias("slow_requests")) \
    .filter(col("slow_requests") > 1000)

problem_servers.show()

# This would take hours with pandas but completes in minutes with PySpark!</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can you name a real-life use case where PySpark is useful?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Uber's Trip Data Analysis:</strong> Uber uses PySpark to process billions of trip records daily. They analyze driver behavior, passenger patterns, surge pricing needs, and route optimization. The data is too large for traditional tools - PySpark enables them to process this in real-time across their global operations, making decisions on pricing and driver allocation instantly.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How does it handle big data that pandas cannot?</strong></span>
                            </div>
                            <div class="cross-answer">
                                PySpark handles big data through: <strong>1) Distribution:</strong> Splits data across multiple machines instead of loading everything into one machine's memory. <strong>2) Parallel Processing:</strong> Processes different chunks simultaneously across the cluster. <strong>3) Lazy Evaluation:</strong> Optimizes operations before execution. <strong>4) Fault Tolerance:</strong> Automatically recovers from failures. Pandas is limited to single-machine memory (~16-64GB), while PySpark can handle TBs-PBs of data.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 6: PySpark vs Pandas -->
            <div class="question-content" id="q6">
                <div class="question-header">
                    <h1 class="question-title">What is the difference between PySpark and pandas?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Basic</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Basic</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Pandas</strong> is a Python library for data manipulation on a single machine, limited by available RAM. <strong>PySpark</strong> is a distributed computing framework that processes data across multiple machines in a cluster.</p>
                        <p style="margin-top: 1rem;">Choose pandas for datasets < 10GB that fit in memory. Choose PySpark for datasets > 100GB that require distributed processing.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Single Machine vs Distributed</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> In-Memory vs Fault Tolerant</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Eager vs Lazy Evaluation</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> DataFrame API</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-table"></i>
                        Detailed Comparison
                    </div>
                    
                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Feature</th>
                                    <th>Pandas</th>
                                    <th>PySpark</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Architecture</strong></td>
                                    <td>Single machine</td>
                                    <td>Distributed cluster</td>
                                </tr>
                                <tr>
                                    <td><strong>Data Size Limit</strong></td>
                                    <td>RAM size (~10-50GB max)</td>
                                    <td>TBs to PBs</td>
                                </tr>
                                <tr>
                                    <td><strong>Processing</strong></td>
                                    <td>Sequential (single core)</td>
                                    <td>Parallel (distributed)</td>
                                </tr>
                                <tr>
                                    <td><strong>Execution</strong></td>
                                    <td>Eager (immediate)</td>
                                    <td>Lazy (optimized)</td>
                                </tr>
                                <tr>
                                    <td><strong>Fault Tolerance</strong></td>
                                    <td>None (crashes on failure)</td>
                                    <td>Built-in (auto recovery)</td>
                                </tr>
                                <tr>
                                    <td><strong>Mutability</strong></td>
                                    <td>Mutable DataFrames</td>
                                    <td>Immutable DataFrames</td>
                                </tr>
                                <tr>
                                    <td><strong>API Complexity</strong></td>
                                    <td>Simple, intuitive</td>
                                    <td>More complex</td>
                                </tr>
                                <tr>
                                    <td><strong>Speed (small data)</strong></td>
                                    <td>Faster</td>
                                    <td>Slower (overhead)</td>
                                </tr>
                                <tr>
                                    <td><strong>Speed (big data)</strong></td>
                                    <td>Can't handle/crashes</td>
                                    <td>Much faster</td>
                                </tr>
                                <tr>
                                    <td><strong>Use Case</strong></td>
                                    <td>Data analysis, exploration</td>
                                    <td>Big data ETL, ML at scale</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Code Comparison
                    </div>
                    
                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-table"></i> Pandas Example</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>import pandas as pd

# Read CSV (loads entire file into memory)
df = pd.read_csv("sales_data.csv")

# Filter and group (executes immediately)
result = df[df['amount'] > 100] \
    .groupby('product')['amount'] \
    .sum() \
    .sort_values(ascending=False)

print(result)

# Limitation: If file > RAM, this will crash!</code></pre>
                    </div>

                    <div class="example-box" style="margin-top: 1.5rem;">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-fire"></i> PySpark Example</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, col

spark = SparkSession.builder.appName("Sales").getOrCreate()

# Read CSV (distributed, lazy - no data loaded yet)
df = spark.read.csv("sales_data.csv", header=True, inferSchema=True)

# Filter and group (still lazy - just building execution plan)
result = df.filter(col("amount") > 100) \
    .groupBy("product") \
    .agg(sum("amount").alias("total")) \
    .orderBy(col("total").desc())

# Only now is data actually processed (when action is called)
result.show()

# Can handle files > 1TB across cluster!</code></pre>
                    </div>

                    <div class="note-box">
                        <strong>Key Difference:</strong> Pandas executes each operation immediately, while PySpark builds an optimized execution plan and runs it only when needed.
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>When would you prefer PySpark DataFrame over pandas DataFrame?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Prefer PySpark when: <strong>1) Data size > 10-50GB</strong> (doesn't fit in single machine RAM). <strong>2) Need distributed processing</strong> across multiple machines. <strong>3) Working with big data pipelines</strong> in production. <strong>4) Need fault tolerance</strong> (auto-recovery from failures). <strong>5) Processing streaming data</strong> in real-time. <strong>6) Training ML models on massive datasets.</strong> Use pandas for quick analysis, prototyping, or small datasets.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can PySpark handle data that doesn't fit in memory?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Yes!</strong> This is PySpark's main advantage. PySpark distributes data across multiple machines and processes it in partitions. Data doesn't need to fit in a single machine's memory - it can be spread across 10, 100, or 1000 machines. PySpark also spills to disk when needed. It can handle datasets from GBs to PBs. Pandas requires the entire dataset to fit in RAM of one machine.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-chart-bar"></i>
                        When to Use Each
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem;">
                        <div style="background: linear-gradient(135deg, rgba(59, 130, 246, 0.1), rgba(37, 99, 235, 0.05)); padding: 1.5rem; border-radius: 12px; border: 1px solid rgba(59, 130, 246, 0.3);">
                            <h4 style="color: #3b82f6; margin-bottom: 1rem; display: flex; align-items: center; gap: 0.5rem;">
                                <i class="fas fa-table"></i> Use Pandas When:
                            </h4>
                            <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                                <li>Data < 10GB</li>
                                <li>Quick data exploration</li>
                                <li>Prototyping and testing</li>
                                <li>Local development</li>
                                <li>Rich visualization needs</li>
                                <li>Simple transformations</li>
                            </ul>
                        </div>

                        <div style="background: linear-gradient(135deg, rgba(16, 185, 129, 0.1), rgba(5, 150, 105, 0.05)); padding: 1.5rem; border-radius: 12px; border: 1px solid rgba(16, 185, 129, 0.3);">
                            <h4 style="color: #10b981; margin-bottom: 1rem; display: flex; align-items: center; gap: 0.5rem;">
                                <i class="fas fa-fire"></i> Use PySpark When:
                            </h4>
                            <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                                <li>Data > 100GB</li>
                                <li>Production ETL pipelines</li>
                                <li>Distributed processing needed</li>
                                <li>Real-time streaming</li>
                                <li>ML at scale</li>
                                <li>Need fault tolerance</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 7: Installation & Setup -->
            <div class="question-content" id="q7">
                <div class="question-header">
                    <h1 class="question-title">How do you install and start PySpark?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Basic</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Basic</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-download"></i>
                        Installation Methods
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-terminal"></i> Method 1: Using pip</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Install PySpark using pip
pip install pyspark

# Or with specific version
pip install pyspark==3.5.0

# Verify installation
pip show pyspark</code></pre>
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-terminal"></i> Method 2: Launching PySpark Shell</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Start interactive PySpark shell
pyspark

# Start with specific configuration
pyspark --master local[4] --driver-memory 4g

# You'll see output like:
# Welcome to
#       ____              __
#      / __/__  ___ _____/ /__
#     _\ \/ _ \/ _ `/ __/  '_/
#    /__ / .__/\_,_/_/ /_/\_\
#       /_/
# 
# SparkSession available as 'spark'.</code></pre>
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-code"></i> Method 3: In Python Script</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# Use Spark
df = spark.read.csv("data.csv")
df.show()

# Stop Spark when done
spark.stop()</code></pre>
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fab fa-python"></i> Method 4: In Jupyter Notebook</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Install findspark to help Jupyter find Spark
pip install findspark

# In Jupyter notebook:
import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("JupyterApp") \
    .master("local[*]") \
    .getOrCreate()

# Now you can use spark
spark.version</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What command is used to launch the PySpark shell?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Simply type <code>pyspark</code> in your terminal/command prompt. This launches an interactive Python shell with Spark pre-configured and ready to use. The SparkSession is automatically available as the variable 'spark'.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can PySpark be used inside Jupyter Notebook?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Yes! Install PySpark and findspark using pip. Use <code>findspark.init()</code> to configure Jupyter to find Spark, then create a SparkSession. Many data scientists prefer Jupyter for interactive PySpark development because it provides visualization and iterative exploration capabilities.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How do you initialize Spark in PySpark?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Initialize Spark by creating a SparkSession using the builder pattern: <code>spark = SparkSession.builder.appName("MyApp").master("local[*]").getOrCreate()</code>. The SparkSession is the entry point for all Spark functionality and automatically creates SparkContext internally.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 8: SparkContext -->
            <div class="question-content" id="q8">
                <div class="question-header">
                    <h1 class="question-title">What is SparkContext and how is it created?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Intermediate</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Intermediate</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>SparkContext</strong> is the entry point to Spark functionality and represents the connection to a Spark cluster. It coordinates the execution of jobs and communicates with the cluster manager to allocate resources.</p>
                        <p style="margin-top: 1rem;">SparkContext tells Spark how and where to access a cluster. It's the heart of any Spark application.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Entry Point</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Cluster Connection</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Job Coordination</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Resource Allocation</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> RDD Operations</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Creating SparkContext
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-play"></i> Method 1: Direct Creation</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkContext, SparkConf

# Create configuration
conf = SparkConf().setAppName("MyApp").setMaster("local[*]")

# Create SparkContext
sc = SparkContext(conf=conf)

# Use SparkContext
rdd = sc.parallelize([1, 2, 3, 4, 5])
result = rdd.map(lambda x: x * 2).collect()
print(result)  # [2, 4, 6, 8, 10]

# Stop SparkContext
sc.stop()</code></pre>
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-magic"></i> Method 2: Via SparkSession (Recommended)</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark.sql import SparkSession

# Create SparkSession (automatically creates SparkContext)
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .getOrCreate()

# Access SparkContext through SparkSession
sc = spark.sparkContext

# Use it
rdd = sc.textFile("data.txt")
word_count = rdd.flatMap(lambda line: line.split()) \
                .map(lambda word: (word, 1)) \
                .reduceByKey(lambda a, b: a + b)

word_count.take(10)</code></pre>
                    </div>

                    <div class="highlight-box">
                        <strong>Best Practice:</strong> Use SparkSession instead of creating SparkContext directly. SparkSession is the unified entry point that creates SparkContext internally.
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-tasks"></i>
                        What SparkContext Does
                    </div>

                    <div style="display: grid; gap: 1rem;">
                        <div class="highlight-box">
                            <strong>1. Cluster Connection</strong>
                            <p style="margin-top: 0.5rem;">Establishes connection to Spark cluster (Standalone, YARN, Mesos, Kubernetes)</p>
                        </div>

                        <div class="highlight-box">
                            <strong>2. Resource Allocation</strong>
                            <p style="margin-top: 0.5rem;">Requests executors from cluster manager for running tasks</p>
                        </div>

                        <div class="highlight-box">
                            <strong>3. RDD Creation</strong>
                            <p style="margin-top: 0.5rem;">Provides methods to create RDDs: parallelize(), textFile(), wholeTextFiles()</p>
                        </div>

                        <div class="highlight-box">
                            <strong>4. Job Scheduling</strong>
                            <p style="margin-top: 0.5rem;">Converts your code into jobs and schedules them on the cluster</p>
                        </div>

                        <div class="highlight-box">
                            <strong>5. Configuration Access</strong>
                            <p style="margin-top: 0.5rem;">Holds configuration parameters for the Spark application</p>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What is SparkContext and why is it needed?</strong></span>
                            </div>
                            <div class="cross-answer">
                                SparkContext is the main entry point for Spark functionality. It's needed because it establishes the connection between your application and the Spark cluster, coordinates job execution, allocates resources, and provides the ability to create RDDs. Without SparkContext, you cannot use Spark's distributed computing capabilities.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What's the relationship between SparkContext and SparkSession?</strong></span>
                            </div>
                            <div class="cross-answer">
                                SparkSession is a unified entry point introduced in Spark 2.0 that creates SparkContext internally. SparkSession = SparkContext + SQLContext + HiveContext combined. You access SparkContext through <code>spark.sparkContext</code>. SparkSession is higher-level and recommended for new applications, while SparkContext is lower-level for RDD operations.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What happens if you try to create multiple SparkContexts?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Spark will throw an error: "Only one SparkContext should be running in this JVM". You can only have ONE active SparkContext per JVM. If you need another one, you must stop the existing SparkContext using <code>sc.stop()</code> before creating a new one. This is a Spark design limitation for resource management.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 9: SparkConf -->
            <div class="question-content" id="q9">
                <div class="question-header">
                    <h1 class="question-title">What is SparkConf?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Intermediate</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Intermediate</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>SparkConf</strong> is a class that holds configuration parameters for your Spark application. It's used to set various Spark properties like application name, master URL, memory settings, and other runtime parameters.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Configuration</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Properties</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Runtime Settings</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Application Tuning</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Using SparkConf
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-cog"></i> Creating and Configuring SparkConf</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkConf, SparkContext

# Create SparkConf and set properties
conf = SparkConf()
conf.setAppName("MyDataProcessingApp")
conf.setMaster("local[4]")  # Use 4 cores locally
conf.set("spark.executor.memory", "4g")
conf.set("spark.driver.memory", "2g")
conf.set("spark.sql.shuffle.partitions", "200")
conf.set("spark.default.parallelism", "100")

# Create SparkContext with this configuration
sc = SparkContext(conf=conf)

# Or chain methods:
conf = SparkConf() \
    .setAppName("MyApp") \
    .setMaster("local[*]") \
    .set("spark.executor.memory", "4g") \
    .set("spark.driver.memory", "2g")

sc = SparkContext(conf=conf)</code></pre>
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-rocket"></i> Common Configuration Options</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ProductionApp") \
    .master("yarn") \
    .config("spark.executor.memory", "8g") \
    .config("spark.executor.cores", "4") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "400") \
    .config("spark.default.parallelism", "200") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

# View current configuration
print(spark.sparkContext.getConf().getAll())</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-list"></i>
                        Important Configuration Parameters
                    </div>

                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Parameter</th>
                                    <th>Description</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>spark.app.name</code></td>
                                    <td>Application name</td>
                                    <td>"MyDataPipeline"</td>
                                </tr>
                                <tr>
                                    <td><code>spark.master</code></td>
                                    <td>Master URL</td>
                                    <td>local[*], yarn, spark://host:port</td>
                                </tr>
                                <tr>
                                    <td><code>spark.executor.memory</code></td>
                                    <td>Memory per executor</td>
                                    <td>"4g", "8g"</td>
                                </tr>
                                <tr>
                                    <td><code>spark.driver.memory</code></td>
                                    <td>Driver program memory</td>
                                    <td>"2g", "4g"</td>
                                </tr>
                                <tr>
                                    <td><code>spark.executor.cores</code></td>
                                    <td>Cores per executor</td>
                                    <td>2, 4, 8</td>
                                </tr>
                                <tr>
                                    <td><code>spark.sql.shuffle.partitions</code></td>
                                    <td>Partitions for shuffles</td>
                                    <td>200 (default), 400, 800</td>
                                </tr>
                                <tr>
                                    <td><code>spark.default.parallelism</code></td>
                                    <td>Default parallelism</td>
                                    <td>100, 200</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 11: Understanding RDDs -->
            <div class="question-content" id="q11">
                <div class="question-header">
                    <h1 class="question-title">What is an RDD in PySpark?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Intermediate</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Intermediate</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>RDD (Resilient Distributed Dataset)</strong> is the fundamental data structure of Apache Spark. It's an immutable, distributed collection of objects that can be processed in parallel across a cluster.</p>
                        <p style="margin-top: 1rem;"><strong>Resilient:</strong> Fault-tolerant, can recover from failures<br>
                        <strong>Distributed:</strong> Data split across multiple nodes<br>
                        <strong>Dataset:</strong> Collection of partitioned data</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Immutable</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Distributed</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Fault-Tolerant</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Parallel Processing</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Lazy Evaluation</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Partitioned</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-layer-group"></i>
                        Key Characteristics of RDD
                    </div>

                    <div style="display: grid; gap: 1rem;">
                        <div class="highlight-box">
                            <strong>1. Immutable</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Once created, RDDs cannot be changed. Transformations create new RDDs instead of modifying existing ones.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>2. Distributed</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Data is automatically partitioned and distributed across multiple nodes in the cluster.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>3. Resilient (Fault-Tolerant)</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Can recover lost data automatically using lineage information (DAG) - remembers how it was created.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>4. Lazy Evaluation</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Transformations are not executed immediately. Computation happens only when an action is called.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>5. Type-Safe</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">RDDs are type-safe at compile time (in Scala/Java, less so in Python).</p>
                        </div>

                        <div class="highlight-box">
                            <strong>6. In-Memory Computation</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Processed in memory across the cluster, making operations 10-100x faster than disk-based systems.</p>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        RDD Example
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-play"></i> Creating and Using RDDs</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkContext

sc = SparkContext("local", "RDD Example")

# Create RDD from a list
numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Transformations (lazy - not executed yet)
even_numbers = numbers.filter(lambda x: x % 2 == 0)
squared = even_numbers.map(lambda x: x ** 2)

# Action (triggers execution)
result = squared.collect()
print(result)  # [4, 16, 36, 64, 100]

# Another example: Word count
text = sc.textFile("data.txt")
words = text.flatMap(lambda line: line.split())
word_pairs = words.map(lambda word: (word, 1))
word_counts = word_pairs.reduceByKey(lambda a, b: a + b)

# Get top 10 words
top_words = word_counts.takeOrdered(10, key=lambda x: -x[1])
print(top_words)</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What does RDD stand for?</strong></span>
                            </div>
                            <div class="cross-answer">
                                RDD stands for <strong>Resilient Distributed Dataset</strong>. "Resilient" means fault-tolerant (can recover from failures), "Distributed" means data is spread across multiple nodes, and "Dataset" is a collection of data elements.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What is an RDD and why is it important?</strong></span>
                            </div>
                            <div class="cross-answer">
                                RDD is the fundamental building block of Spark. It's important because: 1) Enables distributed processing across clusters, 2) Provides fault tolerance through lineage, 3) Supports in-memory computation for speed, 4) Allows parallel processing, 5) Forms the foundation for DataFrames and Datasets. Without RDDs, Spark wouldn't have its distributed computing capabilities.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What does "Resilient" mean in RDD?</strong></span>
                            </div>
                            <div class="cross-answer">
                                "Resilient" means <strong>fault-tolerant</strong>. If a node fails or data is lost, Spark can automatically recreate the lost partition using the RDD's lineage (the sequence of transformations that created it). This DAG (Directed Acyclic Graph) of operations allows Spark to recompute only the lost data without reprocessing everything.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Is RDD mutable or immutable?</strong></span>
                            </div>
                            <div class="cross-answer">
                                RDDs are <strong>immutable</strong> (read-only). You cannot modify an existing RDD. Instead, transformations create NEW RDDs. This immutability enables: 1) Safe parallel processing, 2) Easy fault recovery through lineage, 3) Consistent distributed computing. Example: <code>rdd2 = rdd1.map(lambda x: x * 2)</code> creates a new RDD, doesn't modify rdd1.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What does immutability of RDD mean?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Immutability means once an RDD is created, its data cannot be changed. Every transformation (map, filter, etc.) creates a NEW RDD rather than modifying the original. This design choice enables: safe parallel execution (no race conditions), automatic fault recovery, and easier reasoning about data flow in distributed systems.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How is RDD different from a Python list?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Python List:</strong> Single machine, in-memory, mutable, sequential processing, limited by RAM, no fault tolerance. <strong>RDD:</strong> Distributed across cluster, immutable, parallel processing, can handle TBs of data, fault-tolerant, lazy evaluation. A list is like a single notebook; an RDD is like a library distributed across multiple buildings with automatic backup and recovery.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How is an RDD different from a DataFrame?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>RDD:</strong> Low-level API, no schema, type-safe at compile time, manual optimization, functional programming. <strong>DataFrame:</strong> High-level API, has schema (structured), SQL support, automatic optimization (Catalyst), better performance. DataFrames are built on top of RDDs. Use DataFrames for most tasks; use RDDs only when you need fine-grained control or custom partitioning.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How are RDDs fault-tolerant?</strong></span>
                            </div>
                            <div class="cross-answer">
                                RDDs achieve fault tolerance through <strong>lineage</strong> (DAG - Directed Acyclic Graph). Spark remembers the sequence of transformations used to create each RDD. If a partition is lost due to node failure, Spark automatically recomputes only that partition using its lineage. This is more efficient than data replication because it doesn't require storing multiple copies.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can we create RDDs manually?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Yes! You can create RDDs manually using: 1) <code>sc.parallelize([1,2,3])</code> - from a Python collection, 2) <code>sc.textFile("file.txt")</code> - from a text file, 3) <code>sc.wholeTextFiles("directory/")</code> - from multiple files, 4) From existing RDDs through transformations. The most common methods are parallelize() for in-memory data and textFile() for external data.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-image"></i>
                        Visual Understanding
                    </div>
                    <div class="image-container">
                        <svg width="800" height="500" viewBox="0 0 800 500">
                            <rect width="800" height="500" fill="#1e293b"/>
                            
                            <text x="400" y="35" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">RDD Structure: Distributed & Partitioned</text>
                            
                            <!-- Original Data -->
                            <rect x="50" y="70" width="700" height="80" fill="#334155" stroke="#0ea5e9" stroke-width="2" rx="8"/>
                            <text x="400" y="100" font-size="16" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Original Dataset</text>
                            <text x="400" y="125" font-size="13" fill="#cbd5e1" text-anchor="middle">[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]</text>
                            
                            <text x="400" y="175" font-size="14" fill="#10b981" text-anchor="middle" font-weight="bold"> Distributed into RDD Partitions </text>
                            
                            <!-- Partitions distributed across nodes -->
                            <!-- Node 1 -->
                            <g>
                                <rect x="50" y="200" width="160" height="230" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="8"/>
                                <text x="130" y="225" font-size="14" fill="#10b981" text-anchor="middle" font-weight="bold">Node 1</text>
                                
                                <rect x="65" y="240" width="130" height="50" fill="#334155" stroke="#10b981" stroke-width="1" rx="4"/>
                                <text x="130" y="260" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                                <text x="130" y="278" font-size="11" fill="#cbd5e1" text-anchor="middle">[1, 2, 3, 4]</text>
                            </g>
                            
                            <!-- Node 2 -->
                            <g>
                                <rect x="230" y="200" width="160" height="230" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="2" rx="8"/>
                                <text x="310" y="225" font-size="14" fill="#3b82f6" text-anchor="middle" font-weight="bold">Node 2</text>
                                
                                <rect x="245" y="240" width="130" height="50" fill="#334155" stroke="#3b82f6" stroke-width="1" rx="4"/>
                                <text x="310" y="260" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                                <text x="310" y="278" font-size="11" fill="#cbd5e1" text-anchor="middle">[5, 6, 7, 8]</text>
                            </g>
                            
                            <!-- Node 3 -->
                            <g>
                                <rect x="410" y="200" width="160" height="230" fill="rgba(245, 158, 11, 0.1)" stroke="#f59e0b" stroke-width="2" rx="8"/>
                                <text x="490" y="225" font-size="14" fill="#f59e0b" text-anchor="middle" font-weight="bold">Node 3</text>
                                
                                <rect x="425" y="240" width="130" height="50" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="4"/>
                                <text x="490" y="260" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 3</text>
                                <text x="490" y="278" font-size="11" fill="#cbd5e1" text-anchor="middle">[9, 10, 11, 12]</text>
                            </g>
                            
                            <!-- Node 4 -->
                            <g>
                                <rect x="590" y="200" width="160" height="230" fill="rgba(139, 92, 246, 0.1)" stroke="#8b5cf6" stroke-width="2" rx="8"/>
                                <text x="670" y="225" font-size="14" fill="#8b5cf6" text-anchor="middle" font-weight="bold">Node 4</text>
                                
                                <rect x="605" y="240" width="130" height="50" fill="#334155" stroke="#8b5cf6" stroke-width="1" rx="4"/>
                                <text x="670" y="260" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 4</text>
                                <text x="670" y="278" font-size="11" fill="#cbd5e1" text-anchor="middle">[13, 14, 15, 16]</text>
                            </g>
                            
                            <!-- Processing indicators -->
                            <text x="130" y="320" font-size="11" fill="#10b981" text-anchor="middle"> Process</text>
                            <circle cx="130" cy="340" r="15" fill="none" stroke="#10b981" stroke-width="2"/>
                            <text x="130" y="346" font-size="18" fill="#10b981" text-anchor="middle"></text>
                            
                            <text x="310" y="320" font-size="11" fill="#3b82f6" text-anchor="middle"> Process</text>
                            <circle cx="310" cy="340" r="15" fill="none" stroke="#3b82f6" stroke-width="2"/>
                            <text x="310" y="346" font-size="18" fill="#3b82f6" text-anchor="middle"></text>
                            
                            <text x="490" y="320" font-size="11" fill="#f59e0b" text-anchor="middle"> Process</text>
                            <circle cx="490" cy="340" r="15" fill="none" stroke="#f59e0b" stroke-width="2"/>
                            <text x="490" y="346" font-size="18" fill="#f59e0b" text-anchor="middle"></text>
                            
                            <text x="670" y="320" font-size="11" fill="#8b5cf6" text-anchor="middle"> Process</text>
                            <circle cx="670" cy="340" r="15" fill="none" stroke="#8b5cf6" stroke-width="2"/>
                            <text x="670" y="346" font-size="18" fill="#8b5cf6" text-anchor="middle"></text>
                            
                            <!-- Results -->
                            <text x="130" y="380" font-size="11" fill="#cbd5e1" text-anchor="middle">Result 1</text>
                            <text x="310" y="380" font-size="11" fill="#cbd5e1" text-anchor="middle">Result 2</text>
                            <text x="490" y="380" font-size="11" fill="#cbd5e1" text-anchor="middle">Result 3</text>
                            <text x="670" y="380" font-size="11" fill="#cbd5e1" text-anchor="middle">Result 4</text>
                            
                            <text x="130" y="400" font-size="10" fill="#94a3b8" text-anchor="middle">[2, 4, 6, 8]</text>
                            <text x="310" y="400" font-size="10" fill="#94a3b8" text-anchor="middle">[10, 12, 14, 16]</text>
                            <text x="490" y="400" font-size="10" fill="#94a3b8" text-anchor="middle">[18, 20, 22, 24]</text>
                            <text x="670" y="400" font-size="10" fill="#94a3b8" text-anchor="middle">[26, 28, 30, 32]</text>
                            
                            <!-- Footer -->
                            <rect x="50" y="445" width="700" height="40" fill="rgba(14, 165, 233, 0.1)" stroke="#0ea5e9" stroke-width="2" rx="8"/>
                            <text x="400" y="470" font-size="13" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Parallel Processing: All nodes process simultaneously</text>
                        </svg>
                        <p class="image-caption">RDD partitioned across 4 nodes - each processes data in parallel</p>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 12: Creating RDDs -->
            <div class="question-content" id="q12">
                <div class="question-header">
                    <h1 class="question-title">How can we create an RDD?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Intermediate</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Intermediate</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        RDD Creation Methods
                    </div>
                    <div class="definition-box">
                        <p>There are three main ways to create RDDs in PySpark:</p>
                        <ul style="margin-left: 2rem; margin-top: 1rem; color: var(--text-secondary); line-height: 1.8;">
                            <li><strong>1. Parallelizing an existing collection</strong> using <code>sc.parallelize()</code></li>
                            <li><strong>2. Loading external datasets</strong> using <code>sc.textFile()</code>, <code>sc.wholeTextFiles()</code></li>
                            <li><strong>3. Transforming existing RDDs</strong> to create new RDDs</li>
                        </ul>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> sc.parallelize()</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> sc.textFile()</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> sc.wholeTextFiles()</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Transformations</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Method 1: Parallelizing Collections
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-list"></i> sc.parallelize() Examples</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkContext

sc = SparkContext("local", "RDD Creation")

# Create RDD from a list
numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
print(numbers.collect())  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# Create RDD with specific number of partitions
numbers_partitioned = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8], 4)  # 4 partitions
print(f"Number of partitions: {numbers_partitioned.getNumPartitions()}")  # 4

# Create RDD from tuple
data = sc.parallelize([("Alice", 25), ("Bob", 30), ("Charlie", 35)])
print(data.collect())

# Create RDD from range
large_numbers = sc.parallelize(range(1, 1000000))
print(f"Count: {large_numbers.count()}")  # 999999</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-file"></i>
                        Method 2: Loading External Data
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-file-alt"></i> sc.textFile() Examples</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Read text file (each line becomes an element)
text_rdd = sc.textFile("input.txt")
print(text_rdd.take(5))  # First 5 lines

# Read multiple files with wildcard
logs_rdd = sc.textFile("logs/*.log")

# Read from HDFS
hdfs_rdd = sc.textFile("hdfs://namenode:8020/data/file.txt")

# Read from S3
s3_rdd = sc.textFile("s3a://bucket-name/data/*.csv")

# Read compressed files
gz_rdd = sc.textFile("data.txt.gz")  # Automatically decompresses

# wholeTextFiles - reads entire file as single record
files_rdd = sc.wholeTextFiles("directory/")
# Returns: [(filename1, content1), (filename2, content2), ...]</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-random"></i>
                        Method 3: From Existing RDDs
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-project-diagram"></i> Transformations Create New RDDs</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Create initial RDD
numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Create new RDDs through transformations
even_numbers = numbers.filter(lambda x: x % 2 == 0)
# RDD: [2, 4, 6, 8, 10]

squared_numbers = numbers.map(lambda x: x ** 2)
# RDD: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]

# From DataFrame
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
rdd_from_df = df.rdd  # Convert DataFrame to RDD
print(rdd_from_df.collect())  # [Row(id=1, name='Alice'), Row(id=2, name='Bob')]</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What's the difference between sc.parallelize() and sc.textFile()?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>sc.parallelize():</strong> Creates RDD from an in-memory Python collection (list, tuple, etc.) - use for small datasets or testing. <strong>sc.textFile():</strong> Reads external files (from disk, HDFS, S3) - use for large datasets in production. parallelize() loads data into memory first, while textFile() reads data lazily and is more memory-efficient.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can you create an RDD from a DataFrame?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Yes! Use <code>df.rdd</code> to convert a DataFrame to RDD. Example: <code>rdd = df.rdd</code>. Each row becomes an RDD element of type Row. You can also convert back: <code>df = rdd.toDF()</code> or <code>df = spark.createDataFrame(rdd, schema)</code>. However, DataFrames are generally preferred due to better optimization.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which method is more efficient  sc.parallelize() or reading from a file? Why?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Reading from a file is more efficient for large datasets</strong> because: 1) Data is read lazily (on-demand), not loaded entirely into memory. 2) Can process files larger than available RAM. 3) Utilizes distributed file systems (HDFS, S3) for parallel reading. <strong>sc.parallelize()</strong> requires data to fit in driver memory first, limiting its use to small datasets or testing. Use parallelize() only for small data or quick prototypes.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Can RDDs be created from a DataFrame?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Yes, identical to the earlier question. DataFrames can be easily converted to RDDs using <code>df.rdd</code>. This is useful when you need low-level control or want to use RDD-specific operations that aren't available in the DataFrame API. However, you lose the DataFrame's optimization benefits when converting to RDD.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 13: Lazy Evaluation -->
            <div class="question-content" id="q13">
                <div class="question-header">
                    <h1 class="question-title">What is lazy evaluation in PySpark?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Lazy Evaluation</strong> means that Spark delays executing transformations until an action is called. Instead of processing data immediately, Spark builds an execution plan (DAG - Directed Acyclic Graph) and optimizes it before running.</p>
                        <p style="margin-top: 1rem;">Think of it like a recipe: you write all steps (transformations) first, but cooking (execution) happens only when you're ready to serve (action).</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Deferred Execution</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> DAG Optimization</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Lineage Graph</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Query Optimization</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Lazy Evaluation Example
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-hourglass-half"></i> Demonstrating Lazy Evaluation</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkContext
import time

sc = SparkContext("local", "Lazy Evaluation Demo")

print("Creating RDD...")
start = time.time()
rdd = sc.parallelize(range(1, 10000000))  # 10 million numbers
print(f"RDD created in: {time.time() - start:.4f} seconds")  # ~0.0001 seconds (instant!)

print("\nApplying transformations...")
start = time.time()
# These are transformations - NOT executed yet!
filtered = rdd.filter(lambda x: x % 2 == 0)  # Instant
mapped = filtered.map(lambda x: x ** 2)      # Instant
reduced = mapped.filter(lambda x: x > 100)   # Instant
print(f"Transformations defined in: {time.time() - start:.4f} seconds")  # ~0.0001 seconds

print("\nCalling action (triggers execution)...")
start = time.time()
result = reduced.take(10)  # NOW everything executes!
print(f"Action completed in: {time.time() - start:.4f} seconds")  # ~2-3 seconds
print(f"Result: {result}")

# Key Point: Spark waited until take() to actually process data!
# It optimized all transformations into a single efficient execution plan.</code></pre>
                    </div>

                    <div class="highlight-box">
                        <strong>Key Insight:</strong> Transformations return instantly (just building plan). Only actions trigger actual computation!
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-lightbulb"></i>
                        Benefits of Lazy Evaluation
                    </div>

                    <div style="display: grid; gap: 1rem;">
                        <div class="highlight-box">
                            <strong>1. Query Optimization</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Spark can reorganize and optimize operations before execution, eliminating unnecessary steps and combining operations.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>2. Reduced Computation</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">If you only need a subset of data (like take(10)), Spark processes only what's needed, not the entire dataset.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>3. Memory Efficiency</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Intermediate results don't need to be stored unless explicitly cached. Data flows through the pipeline.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>4. Pipelining</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Multiple operations can be combined into a single pass over the data, reducing I/O operations.</p>
                        </div>

                        <div class="highlight-box">
                            <strong>5. Better Resource Utilization</strong>
                            <p style="margin-top: 0.5rem; color: var(--text-secondary);">Spark can allocate resources optimally after seeing the complete execution plan.</p>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Why is lazy evaluation beneficial?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Benefits: <strong>1) Optimization:</strong> Spark optimizes the entire pipeline before execution. <strong>2) Efficiency:</strong> Processes only required data (e.g., take(10) doesn't process entire dataset). <strong>3) Pipelining:</strong> Combines multiple operations into single pass. <strong>4) Memory savings:</strong> Doesn't store intermediate results unless cached. <strong>5) Resource management:</strong> Better allocation after seeing full plan. This makes Spark 10-100x faster than eager evaluation systems.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which operations trigger computation?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Actions</strong> trigger computation. Common actions include: <code>collect(), count(), take(), first(), reduce(), save(), show(), foreach()</code>. Transformations like <code>map(), filter(), flatMap(), groupBy()</code> do NOT trigger execution - they just build the execution plan. Only when an action is called does Spark execute all pending transformations.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>What are the advantages of lazy evaluation?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Same as "Why is lazy evaluation beneficial?" - it allows query optimization, reduces unnecessary computation, enables pipelining, saves memory, and improves resource utilization. Lazy evaluation is a core reason why Spark is so fast compared to earlier big data systems like Hadoop MapReduce.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Give me an example transformation that demonstrates this.</strong></span>
                            </div>
                            <div class="cross-answer">
                                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># These execute instantly (lazy)
rdd = sc.parallelize([1,2,3,4,5])
filtered = rdd.filter(lambda x: x > 2)  # Returns instantly
mapped = filtered.map(lambda x: x * 2)   # Returns instantly

# This triggers execution
result = mapped.collect()  # NOW Spark executes everything</code></pre>
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>When is the actual computation triggered?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Computation is triggered when an <strong>action</strong> is called. Actions include: collect(), count(), take(), first(), reduce(), foreach(), save(), saveAsTextFile(), countByKey(), show() (for DataFrames). Until an action is called, Spark just records transformations in a lineage graph (DAG) without processing any data.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which kind of operations trigger computation?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Actions</strong> trigger computation. Actions are operations that: 1) Return a value to the driver program (collect, count, first), 2) Write data to external storage (save, saveAsTextFile), 3) Perform side effects (foreach, foreachPartition). Transformations (map, filter, join, etc.) never trigger computation - they're always lazy.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-image"></i>
                        Visual Understanding
                    </div>
                    <div class="image-container">
                        <svg width="800" height="400" viewBox="0 0 800 400">
                            <rect width="800" height="400" fill="#1e293b"/>
                            
                            <text x="400" y="30" font-size="20" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Lazy Evaluation Workflow</text>
                            
                            <!-- Transformations (lazy) -->
                            <rect x="50" y="60" width="300" height="250" fill="rgba(245, 158, 11, 0.1)" stroke="#f59e0b" stroke-width="2" rx="8"/>
                            <text x="200" y="85" font-size="16" fill="#f59e0b" text-anchor="middle" font-weight="bold">Transformations (Lazy - Instant)</text>
                            
                            <rect x="70" y="100" width="260" height="40" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="4"/>
                            <text x="200" y="127" font-size="13" fill="#f1f5f9" text-anchor="middle">rdd.filter(x > 10)  Instant!</text>
                            
                            <rect x="70" y="155" width="260" height="40" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="4"/>
                            <text x="200" y="182" font-size="13" fill="#f1f5f9" text-anchor="middle">rdd.map(x * 2)  Instant!</text>
                            
                            <rect x="70" y="210" width="260" height="40" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="4"/>
                            <text x="200" y="237" font-size="13" fill="#f1f5f9" text-anchor="middle">rdd.filter(x < 100)  Instant!</text>
                            
                            <text x="200" y="275" font-size="12" fill="#94a3b8" text-anchor="middle" font-style="italic">Just building execution plan...</text>
                            <text x="200" y="295" font-size="12" fill="#94a3b8" text-anchor="middle" font-style="italic">No data processing yet!</text>
                            
                            <!-- Arrow -->
                            <path d="M 360 185 L 430 185" stroke="#10b981" stroke-width="3" marker-end="url(#arrowaction)"/>
                            <text x="395" y="175" font-size="13" fill="#10b981" text-anchor="middle" font-weight="bold">Action Called</text>
                            
                            <!-- Actions (eager) -->
                            <rect x="440" y="60" width="310" height="250" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="595" y="85" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">Actions (Eager - Executes All)</text>
                            
                            <rect x="460" y="110" width="270" height="50" fill="#334155" stroke="#10b981" stroke-width="1" rx="4"/>
                            <text x="595" y="133" font-size="13" fill="#f1f5f9" text-anchor="middle">result = rdd.collect()</text>
                            <text x="595" y="150" font-size="11" fill="#10b981" text-anchor="middle"> Triggers ALL transformations!</text>
                            
                            <text x="595" y="185" font-size="12" fill="#cbd5e1" text-anchor="middle"></text>
                            
                            <rect x="460" y="195" width="270" height="100" fill="rgba(14, 165, 233, 0.1)" stroke="#0ea5e9" stroke-width="2" rx="4"/>
                            <text x="595" y="218" font-size="13" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Spark Execution Engine</text>
                            <text x="595" y="240" font-size="11" fill="#cbd5e1" text-anchor="middle">1. Optimizes execution plan (DAG)</text>
                            <text x="595" y="258" font-size="11" fill="#cbd5e1" text-anchor="middle">2. Combines transformations</text>
                            <text x="595" y="276" font-size="11" fill="#cbd5e1" text-anchor="middle">3. Executes on cluster</text>
                            
                            <!-- Result -->
                            <rect x="50" y="340" width="700" height="45" fill="rgba(139, 92, 246, 0.1)" stroke="#8b5cf6" stroke-width="2" rx="8"/>
                            <text x="400" y="367" font-size="14" fill="#8b5cf6" text-anchor="middle" font-weight="bold">Result returned to driver: [12, 24, 36, 48, ...]</text>
                            
                            <defs>
                                <marker id="arrowaction" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                    <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                                </marker>
                            </defs>
                        </svg>
                        <p class="image-caption">Transformations build a plan; Actions trigger execution</p>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 14: Transformations and Actions -->
            <div class="question-content" id="q14">
                <div class="question-header">
                    <h1 class="question-title">What are transformations and actions in PySpark?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Transformations</strong> are lazy operations that create a new RDD from an existing one. They define how data should be transformed but don't execute immediately.</p>
                        <p style="margin-top: 1rem;"><strong>Actions</strong> are eager operations that trigger computation and return a result to the driver program or write data to external storage.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Lazy vs Eager</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> map(), filter()</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> collect(), count()</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> RDD Operations</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-random"></i>
                        Common Transformations
                    </div>

                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Transformation</th>
                                    <th>Description</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>map()</code></td>
                                    <td>Applies function to each element</td>
                                    <td><code>rdd.map(lambda x: x * 2)</code></td>
                                </tr>
                                <tr>
                                    <td><code>filter()</code></td>
                                    <td>Returns elements matching condition</td>
                                    <td><code>rdd.filter(lambda x: x > 10)</code></td>
                                </tr>
                                <tr>
                                    <td><code>flatMap()</code></td>
                                    <td>Maps and flattens results</td>
                                    <td><code>rdd.flatMap(lambda x: x.split())</code></td>
                                </tr>
                                <tr>
                                    <td><code>union()</code></td>
                                    <td>Combines two RDDs</td>
                                    <td><code>rdd1.union(rdd2)</code></td>
                                </tr>
                                <tr>
                                    <td><code>distinct()</code></td>
                                    <td>Returns unique elements</td>
                                    <td><code>rdd.distinct()</code></td>
                                </tr>
                                <tr>
                                    <td><code>groupByKey()</code></td>
                                    <td>Groups values by key</td>
                                    <td><code>pair_rdd.groupByKey()</code></td>
                                </tr>
                                <tr>
                                    <td><code>reduceByKey()</code></td>
                                    <td>Aggregates values by key</td>
                                    <td><code>pair_rdd.reduceByKey(lambda a,b: a+b)</code></td>
                                </tr>
                                <tr>
                                    <td><code>join()</code></td>
                                    <td>Joins two pair RDDs</td>
                                    <td><code>rdd1.join(rdd2)</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="example-box" style="margin-top: 1.5rem;">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-code"></i> Transformation Examples</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# map: Transform each element
doubled = rdd.map(lambda x: x * 2)
# [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]

# filter: Keep elements matching condition
evens = rdd.filter(lambda x: x % 2 == 0)
# [2, 4, 6, 8, 10]

# flatMap: Split and flatten
text = sc.parallelize(["hello world", "spark is awesome"])
words = text.flatMap(lambda line: line.split())
# ["hello", "world", "spark", "is", "awesome"]

# distinct: Remove duplicates
nums = sc.parallelize([1, 2, 2, 3, 3, 3, 4])
unique = nums.distinct()
# [1, 2, 3, 4]</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-bolt"></i>
                        Common Actions
                    </div>

                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Action</th>
                                    <th>Description</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>collect()</code></td>
                                    <td>Returns all elements to driver</td>
                                    <td><code>rdd.collect()</code></td>
                                </tr>
                                <tr>
                                    <td><code>count()</code></td>
                                    <td>Returns number of elements</td>
                                    <td><code>rdd.count()</code></td>
                                </tr>
                                <tr>
                                    <td><code>take(n)</code></td>
                                    <td>Returns first n elements</td>
                                    <td><code>rdd.take(5)</code></td>
                                </tr>
                                <tr>
                                    <td><code>first()</code></td>
                                    <td>Returns first element</td>
                                    <td><code>rdd.first()</code></td>
                                </tr>
                                <tr>
                                    <td><code>reduce()</code></td>
                                    <td>Aggregates elements using function</td>
                                    <td><code>rdd.reduce(lambda a, b: a + b)</code></td>
                                </tr>
                                <tr>
                                    <td><code>foreach()</code></td>
                                    <td>Applies function to each element</td>
                                    <td><code>rdd.foreach(print)</code></td>
                                </tr>
                                <tr>
                                    <td><code>saveAsTextFile()</code></td>
                                    <td>Saves RDD to text files</td>
                                    <td><code>rdd.saveAsTextFile("output/")</code></td>
                                </tr>
                                <tr>
                                    <td><code>countByKey()</code></td>
                                    <td>Counts elements per key</td>
                                    <td><code>pair_rdd.countByKey()</code></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="example-box" style="margin-top: 1.5rem;">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-code"></i> Action Examples</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>rdd = sc.parallelize([1, 2, 3, 4, 5])

# collect: Get all elements
all_elements = rdd.collect()
print(all_elements)  # [1, 2, 3, 4, 5]

# count: Number of elements
total = rdd.count()
print(total)  # 5

# take: First n elements
first_three = rdd.take(3)
print(first_three)  # [1, 2, 3]

# reduce: Aggregate
sum_all = rdd.reduce(lambda a, b: a + b)
print(sum_all)  # 15

# first: Get first element
first_elem = rdd.first()
print(first_elem)  # 1

# Save to file
rdd.saveAsTextFile("output_directory")</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Give 2 examples of transformations and 2 examples of actions.</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Transformations:</strong> 1) <code>map()</code> - transforms each element, 2) <code>filter()</code> - keeps elements matching condition. <strong>Actions:</strong> 1) <code>collect()</code> - returns all elements to driver, 2) <code>count()</code> - returns number of elements. Transformations are lazy (build plan), actions are eager (trigger execution).
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which one returns a value immediately?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Actions</strong> return values immediately (to the driver program) because they trigger execution. Transformations return RDD references immediately but don't process data - they just record the operation in the lineage. Examples: collect(), count(), take() all return results right away.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Give two examples of each.</strong></span>
                            </div>
                            <div class="cross-answer">
                                Same as the first cross-question. <strong>Transformations:</strong> map(), filter(), flatMap(), union(), distinct(). <strong>Actions:</strong> collect(), count(), take(), reduce(), save(). Remember: Transformations create new RDDs, actions return values or save data.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which type of operation builds a new RDD?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Transformations</strong> build new RDDs. Every transformation creates a new RDD because RDDs are immutable. For example, <code>rdd2 = rdd1.map(lambda x: x * 2)</code> creates a new RDD (rdd2) from rdd1. Actions don't create RDDs - they trigger computation and return results.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Is count() a transformation or an action?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <code>count()</code> is an <strong>action</strong>. It triggers execution of all pending transformations and returns the number of elements in the RDD to the driver program. You can verify this: calling count() takes time (processes data), while transformations return instantly.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Why does Spark differentiate between them?</strong></span>
                            </div>
                            <div class="cross-answer">
                                This differentiation enables <strong>lazy evaluation</strong>, which is key to Spark's performance. By separating transformations (planning) from actions (execution), Spark can: 1) Optimize the entire job before running, 2) Combine operations for efficiency, 3) Process only required data, 4) Enable fault tolerance through lineage. Without this distinction, Spark would be as slow as traditional systems.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 15: Actions Triggering Computation -->
            <div class="question-content" id="q15">
                <div class="question-header">
                    <h1 class="question-title">What are actions that trigger computation in Spark?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Actions</strong> are operations that trigger the execution of all pending transformations in the lineage graph (DAG). They either return data to the driver program or write data to external storage systems.</p>
                        <p style="margin-top: 1rem;">When an action is called, Spark's DAG scheduler creates an optimized execution plan and executes it across the cluster.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Eager Evaluation</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Job Execution</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> DAG Scheduler</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Results</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-list-check"></i>
                        Complete List of Actions
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem;">
                        <div class="highlight-box">
                            <strong>Data Retrieval Actions</strong>
                            <ul style="margin-left: 1.5rem; margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li><code>collect()</code> - All elements to driver</li>
                                <li><code>take(n)</code> - First n elements</li>
                                <li><code>first()</code> - First element</li>
                                <li><code>top(n)</code> - Top n elements</li>
                                <li><code>takeOrdered(n)</code> - First n ordered</li>
                                <li><code>takeSample()</code> - Random sample</li>
                            </ul>
                        </div>

                        <div class="highlight-box">
                            <strong>Aggregation Actions</strong>
                            <ul style="margin-left: 1.5rem; margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li><code>count()</code> - Number of elements</li>
                                <li><code>reduce()</code> - Aggregate all elements</li>
                                <li><code>fold()</code> - Like reduce with initial</li>
                                <li><code>aggregate()</code> - Custom aggregation</li>
                                <li><code>sum(), mean(), stdev()</code> - Statistics</li>
                            </ul>
                        </div>

                        <div class="highlight-box">
                            <strong>Key-Value Actions</strong>
                            <ul style="margin-left: 1.5rem; margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li><code>countByKey()</code> - Count per key</li>
                                <li><code>countByValue()</code> - Count per value</li>
                                <li><code>lookup(key)</code> - Values for key</li>
                                <li><code>collectAsMap()</code> - As dictionary</li>
                            </ul>
                        </div>

                        <div class="highlight-box">
                            <strong>I/O Actions</strong>
                            <ul style="margin-left: 1.5rem; margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8;">
                                <li><code>saveAsTextFile()</code> - Save as text</li>
                                <li><code>saveAsSequenceFile()</code> - Binary</li>
                                <li><code>saveAsObjectFile()</code> - Serialized</li>
                                <li><code>foreach()</code> - Execute function</li>
                                <li><code>foreachPartition()</code> - Per partition</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Action Examples
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-bolt"></i> Comprehensive Action Examples</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkContext

sc = SparkContext("local", "Actions Demo")
rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# 1. collect() - Get all elements (CAREFUL with large datasets!)
all_data = rdd.collect()
print(f"All elements: {all_data}")  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# 2. count() - Number of elements
total_count = rdd.count()
print(f"Total count: {total_count}")  # 10

# 3. take(n) - First n elements
first_five = rdd.take(5)
print(f"First 5: {first_five}")  # [1, 2, 3, 4, 5]

# 4. first() - Just the first element
first_element = rdd.first()
print(f"First element: {first_element}")  # 1

# 5. reduce() - Aggregate all elements
sum_all = rdd.reduce(lambda a, b: a + b)
print(f"Sum: {sum_all}")  # 55

# 6. top(n) - Top n elements
top_three = rdd.top(3)
print(f"Top 3: {top_three}")  # [10, 9, 8]

# 7. takeOrdered(n) - Smallest n elements
smallest_three = rdd.takeOrdered(3)
print(f"Smallest 3: {smallest_three}")  # [1, 2, 3]

# 8. countByValue() - Count occurrences
nums = sc.parallelize([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])
value_counts = nums.countByValue()
print(f"Value counts: {dict(value_counts)}")  # {1: 1, 2: 2, 3: 3, 4: 4}

# 9. saveAsTextFile() - Save to disk
rdd.saveAsTextFile("output_dir")

# 10. foreach() - Execute function on each element
rdd.foreach(lambda x: print(f"Processing: {x}"))

# Example with pair RDD
pair_rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4)])

# countByKey() - Count per key
key_counts = pair_rdd.countByKey()
print(f"Counts by key: {dict(key_counts)}")  # {'a': 2, 'b': 2}

# collectAsMap() - Convert to dictionary
as_dict = pair_rdd.collectAsMap()
print(f"As map: {as_dict}")  # Note: Only keeps last value per key</code></pre>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-exclamation-triangle"></i>
                        Important Warnings
                    </div>

                    <div class="warning-box">
                        <strong> collect() Warning:</strong> Never use collect() on large datasets! It brings ALL data to the driver, which can cause out-of-memory errors. Use take(n) or other actions for large datasets.
                    </div>

                    <div class="warning-box">
                        <strong> Multiple Actions:</strong> Each action triggers a complete re-execution of the DAG unless you cache/persist the RDD. If calling multiple actions on the same RDD, use <code>rdd.cache()</code> or <code>rdd.persist()</code>.
                    </div>

                    <div class="warning-box">
                        <strong> foreach() vs map():</strong> foreach() is an action (has side effects, returns nothing). map() is a transformation (returns new RDD). Don't confuse them!
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Give 3 examples of actions.</strong></span>
                            </div>
                            <div class="cross-answer">
                                Three common actions: <strong>1) collect()</strong> - returns all elements to driver (use cautiously), <strong>2) count()</strong> - returns number of elements, <strong>3) reduce()</strong> - aggregates elements using a function. Other examples: take(), first(), save(), foreach(). All these trigger immediate execution of pending transformations.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>How does Spark optimize before triggering an action?</strong></span>
                            </div>
                            <div class="cross-answer">
                                When an action is called, Spark optimizes through: <strong>1) DAG Analysis:</strong> Reviews entire lineage graph. <strong>2) Stage Division:</strong> Divides job into stages at shuffle boundaries. <strong>3) Task Generation:</strong> Creates parallel tasks per partition. <strong>4) Pipelining:</strong> Combines narrow transformations (map, filter) into single pass. <strong>5) Catalyst Optimizer:</strong> (for DataFrames) applies rule-based and cost-based optimizations. <strong>6) Predicate Pushdown:</strong> Applies filters early. This optimization is why Spark is 10-100x faster than traditional systems!
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-cogs"></i>
                        Optimization Example
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-magic"></i> How Spark Optimizes</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Your code (multiple transformations)
rdd = sc.parallelize(range(1, 1000000))
filtered = rdd.filter(lambda x: x % 2 == 0)
mapped = filtered.map(lambda x: x * 2)
reduced = mapped.filter(lambda x: x > 100)
result = reduced.take(10)  # Action triggers optimization

# What Spark does:
# 1. Sees you only need 10 elements (take(10))
# 2. Combines filter + map + filter into single pass
# 3. Stops processing after finding 10 matching elements
# 4. Doesn't process entire 1 million numbers!
#
# Without optimization: Process 1M  filter  map  filter  take 10
# With optimization: Process only until 10 matches found (maybe 20-30 elements)
#
# This is called "predicate pushdown" and "lazy evaluation" optimization!</code></pre>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')" disabled>
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>
            <!-- Question 16: Narrow vs Wide Transformations -->
            <div class="question-content active" id="q16">
                <div class="question-header">
                    <h1 class="question-title">Explain narrow vs wide transformations</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Intermediate</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Intermediate</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Narrow Transformations:</strong> Each input partition contributes to only one output partition. Data doesn't need to be shuffled across the cluster. Examples: map(), filter(), union()</p>
                        <p style="margin-top: 1rem;"><strong>Wide Transformations:</strong> Each input partition contributes to multiple output partitions. Requires shuffling data across the cluster. Examples: groupByKey(), reduceByKey(), join()</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Narrow Dependencies</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Wide Dependencies</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Shuffle Operations</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Partition Mapping</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-balance-scale"></i>
                        Detailed Comparison
                    </div>

                    <div class="comparison-grid">
                        <div class="comparison-card narrow">
                            <h4><i class="fas fa-check-circle"></i> Narrow Transformations</h4>
                            <p style="color: var(--text-secondary); margin-bottom: 1rem;">Fast, no shuffle required</p>
                            <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                                <li>1-to-1 partition mapping</li>
                                <li>No data movement across network</li>
                                <li>Executed within single stage</li>
                                <li>Can be pipelined together</li>
                                <li>Memory efficient</li>
                                <li>Low latency</li>
                            </ul>
                            <div class="example-box" style="margin-top: 1rem;">
                                <pre><code>map()
filter()
flatMap()
union()
mapPartitions()
sample()</code></pre>
                            </div>
                        </div>

                        <div class="comparison-card wide">
                            <h4><i class="fas fa-exclamation-triangle"></i> Wide Transformations</h4>
                            <p style="color: var(--text-secondary); margin-bottom: 1rem;">Slower, requires shuffle</p>
                            <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                                <li>Many-to-many partition mapping</li>
                                <li>Data shuffled across network</li>
                                <li>Creates new stage boundary</li>
                                <li>Cannot be pipelined easily</li>
                                <li>Memory & disk intensive</li>
                                <li>Higher latency</li>
                            </ul>
                            <div class="example-box" style="margin-top: 1rem;">
                                <pre><code>groupByKey()
reduceByKey()
join()
distinct()
repartition()
sortBy()</code></pre>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-code"></i>
                        Code Examples
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-check-circle" style="color: var(--success-color);"></i> Narrow Transformation Examples</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code>from pyspark import SparkContext

sc = SparkContext("local", "Narrow Transformations")

rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)  # 3 partitions

# 1. map() - Narrow (each element stays in same partition)
doubled = rdd.map(lambda x: x * 2)
# Partition 1: [1,2,3,4]  [2,4,6,8]
# Partition 2: [5,6,7]  [10,12,14]
# Partition 3: [8,9,10]  [16,18,20]

# 2. filter() - Narrow (filtering within partition)
evens = rdd.filter(lambda x: x % 2 == 0)
# Partition 1: [1,2,3,4]  [2,4]
# Partition 2: [5,6,7]  [6]
# Partition 3: [8,9,10]  [8,10]

# 3. flatMap() - Narrow
words = sc.parallelize(["hello world", "spark rocks"])
flat = words.flatMap(lambda line: line.split())
# Each partition's data stays in same partition

# 4. union() - Narrow (no shuffle needed)
rdd2 = sc.parallelize([11, 12, 13])
combined = rdd.union(rdd2)

print(f"Doubled: {doubled.collect()}")
print(f"Evens: {evens.collect()}")</code></pre>
                    </div>

                    <div class="example-box">
                        <div class="example-header">
                            <span class="example-title"><i class="fas fa-exclamation-triangle" style="color: var(--error-color);"></i> Wide Transformation Examples</span>
                            <button class="copy-btn" onclick="copyCode(this)">
                                <i class="fas fa-copy"></i> Copy
                            </button>
                        </div>
                        <pre><code># Wide transformations - require shuffle

# 1. groupByKey() - Wide (same keys must go to same partition)
pairs = sc.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4), ("c", 5)])
grouped = pairs.groupByKey()
# Before: ("a",1) in P1, ("a",3) in P2
# After shuffle: All "a" keys in same partition
# Result: [("a", [1,3]), ("b", [2,4]), ("c", [5])]

# 2. reduceByKey() - Wide (but optimized with local reduce)
word_counts = sc.parallelize([("hello", 1), ("world", 1), ("hello", 1)])
counts = word_counts.reduceByKey(lambda a, b: a + b)
# Shuffles data to combine same keys: [("hello", 2), ("world", 1)]

# 3. join() - Wide (needs to shuffle both RDDs)
rdd1 = sc.parallelize([("a", 1), ("b", 2)])
rdd2 = sc.parallelize([("a", 3), ("c", 4)])
joined = rdd1.join(rdd2)
# Shuffles to match keys: [("a", (1, 3))]

# 4. distinct() - Wide (needs to compare across partitions)
nums = sc.parallelize([1, 2, 2, 3, 3, 3, 4])
unique = nums.distinct()
# Shuffles to find duplicates across all partitions

# 5. sortBy() - Wide (needs global ordering)
unsorted = sc.parallelize([5, 2, 8, 1, 9])
sorted_rdd = unsorted.sortBy(lambda x: x)
# Shuffles to achieve global sort: [1, 2, 5, 8, 9]

print(f"Grouped: {grouped.mapValues(list).collect()}")
print(f"Counts: {counts.collect()}")</code></pre>
                    </div>

                    <div class="warning-box">
                        <strong> Performance Impact:</strong> Wide transformations are expensive! They trigger shuffle operations that involve disk I/O and network transfer. Use them wisely and consider optimizations like using reduceByKey() instead of groupByKey().
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Give an example of each.</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Narrow:</strong> <code>map()</code> - transforms each element independently, data stays in same partition. Example: <code>rdd.map(lambda x: x * 2)</code><br><br>
                                <strong>Wide:</strong> <code>groupByKey()</code> - groups data by key, requires shuffling all matching keys to same partition. Example: <code>pairs.groupByKey()</code>
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Why are wide transformations more expensive?</strong></span>
                            </div>
                            <div class="cross-answer">
                                Wide transformations are expensive because they require <strong>shuffling</strong>: 1) Data must be moved across network between nodes, 2) Intermediate data written to disk, 3) Requires serialization/deserialization, 4) Creates stage boundaries (can't pipeline), 5) Potential memory spills if shuffle data is large. A single wide transformation can be 10-100x slower than narrow transformations.
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which one triggers a shuffle?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Wide transformations</strong> trigger shuffle operations. Narrow transformations do NOT require shuffle because data stays in the same partition. Examples of shuffle-triggering operations: groupByKey(), reduceByKey(), join(), distinct(), repartition(), sortBy(), intersection().
                            </div>
                        </div>

                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Example of a wide transformation?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <code>reduceByKey()</code> is a classic wide transformation example:
                                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Wide transformation
pairs = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
result = pairs.reduceByKey(lambda x, y: x + y)
# Shuffles all "a" keys together, all "b" keys together
# Result: [("a", 4), ("b", 2)]</code></pre>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-image"></i>
                        Visual Understanding
                    </div>
                    <div class="image-container">
                        <svg width="800" height="600" viewBox="0 0 800 600">
                            <rect width="800" height="600" fill="#1e293b"/>
                            
                            <text x="400" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Narrow vs Wide Transformations</text>
                            
                            <!-- Narrow Transformation -->
                            <text x="200" y="70" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">Narrow Transformation (map)</text>
                            
                            <!-- Input Partitions -->
                            <rect x="50" y="90" width="100" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="100" y="115" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                            <text x="100" y="135" font-size="11" fill="#cbd5e1" text-anchor="middle">[1, 2, 3]</text>
                            
                            <rect x="50" y="190" width="100" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="100" y="215" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                            <text x="100" y="235" font-size="11" fill="#cbd5e1" text-anchor="middle">[4, 5, 6]</text>
                            
                            <!-- Arrow -->
                            <path d="M 160 130 L 230 130" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                            <path d="M 160 230 L 230 230" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                            <text x="195" y="155" font-size="11" fill="#10b981" text-anchor="middle">No Shuffle</text>
                            
                            <!-- Output Partitions -->
                            <rect x="240" y="90" width="100" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="290" y="115" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                            <text x="290" y="135" font-size="11" fill="#cbd5e1" text-anchor="middle">[2, 4, 6]</text>
                            
                            <rect x="240" y="190" width="100" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="290" y="215" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                            <text x="290" y="235" font-size="11" fill="#cbd5e1" text-anchor="middle">[8, 10, 12]</text>
                            
                            <!-- Wide Transformation -->
                            <text x="600" y="70" font-size="18" fill="#ef4444" text-anchor="middle" font-weight="bold">Wide Transformation (groupByKey)</text>
                            
                            <!-- Input Partitions -->
                            <rect x="450" y="90" width="120" height="80" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="510" y="115" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                            <text x="510" y="135" font-size="10" fill="#cbd5e1" text-anchor="middle">("a",1), ("b",2)</text>
                            
                            <rect x="450" y="190" width="120" height="80" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="510" y="215" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                            <text x="510" y="235" font-size="10" fill="#cbd5e1" text-anchor="middle">("a",3), ("c",4)</text>
                            
                            <!-- Shuffle Lines -->
                            <path d="M 580 120 Q 620 120, 640 140" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                            <path d="M 580 140 Q 620 160, 640 180" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                            <path d="M 580 220 Q 620 200, 640 180" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                            <path d="M 580 240 Q 620 240, 640 220" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                            
                            <text x="610" y="165" font-size="11" fill="#ef4444" text-anchor="middle" font-weight="bold">SHUFFLE</text>
                            
                            <!-- Output Partitions -->
                            <rect x="650" y="90" width="120" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="710" y="115" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                            <text x="710" y="135" font-size="10" fill="#cbd5e1" text-anchor="middle">("a",[1,3])</text>
                            
                            <rect x="650" y="165" width="120" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="710" y="190" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                            <text x="710" y="210" font-size="10" fill="#cbd5e1" text-anchor="middle">("b",[2])</text>
                            
                            <rect x="650" y="240" width="120" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="710" y="265" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 3</text>
                            <text x="710" y="285" font-size="10" fill="#cbd5e1" text-anchor="middle">("c",[4])</text>
                            
                            <!-- Performance Comparison -->
                            <rect x="50" y="320" width="700" height="240" fill="rgba(14, 165, 233, 0.05)" stroke="#0ea5e9" stroke-width="2" rx="12"/>
                            
                            <text x="400" y="350" font-size="18" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Performance Impact</text>
                            
                            <!-- Narrow Performance -->
                            <rect x="80" y="370" width="300" height="170" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="8"/>
                            <text x="230" y="395" font-size="15" fill="#10b981" text-anchor="middle" font-weight="bold">Narrow: Fast </text>
                            <text x="230" y="420" font-size="12" fill="#cbd5e1" text-anchor="middle"> No network transfer</text>
                            <text x="230" y="442" font-size="12" fill="#cbd5e1" text-anchor="middle"> No disk I/O</text>
                            <text x="230" y="464" font-size="12" fill="#cbd5e1" text-anchor="middle"> Can pipeline operations</text>
                            <text x="230" y="486" font-size="12" fill="#cbd5e1" text-anchor="middle"> Low memory overhead</text>
                            <text x="230" y="515" font-size="13" fill="#10b981" text-anchor="middle" font-weight="bold">Milliseconds</text>
                            
                            <!-- Wide Performance -->
                            <rect x="420" y="370" width="300" height="170" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="8"/>
                            <text x="570" y="395" font-size="15" fill="#ef4444" text-anchor="middle" font-weight="bold">Wide: Slow </text>
                            <text x="570" y="420" font-size="12" fill="#cbd5e1" text-anchor="middle"> Network data transfer</text>
                            <text x="570" y="442" font-size="12" fill="#cbd5e1" text-anchor="middle"> Disk I/O for shuffle</text>
                            <text x="570" y="464" font-size="12" fill="#cbd5e1" text-anchor="middle"> Stage boundaries</text>
                            <text x="570" y="486" font-size="12" fill="#cbd5e1" text-anchor="middle"> High memory usage</text>
                            <text x="570" y="515" font-size="13" fill="#ef4444" text-anchor="middle" font-weight="bold">Seconds/Minutes</text>
                            
                            <defs>
                                <marker id="arrowgreen" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                    <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                                </marker>
                                <marker id="arrowred" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                    <polygon points="0 0, 10 3, 0 6" fill="#ef4444"/>
                                </marker>
                            </defs>
                        </svg>
                        <p class="image-caption">Narrow transformations preserve partition locality; Wide transformations require shuffle</p>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')" disabled>
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

            <!-- Question 17: Narrow vs Wide Dependencies -->
            <div class="question-content" id="q17">
                <div class="question-header">
                    <h1 class="question-title">What is the difference between narrow and wide dependencies?</h1>
                    <div class="question-meta">
                        <span class="meta-tag"><i class="fas fa-layer-group"></i> Intermediate</span>
                        <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
                        <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Intermediate</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-book"></i>
                        Definition
                    </div>
                    <div class="definition-box">
                        <p><strong>Dependencies</strong> describe the relationship between parent RDD partitions and child RDD partitions in Spark's lineage graph (DAG).</p>
                        <p style="margin-top: 1rem;"><strong>Narrow Dependency:</strong> Each partition of the parent RDD is used by at most ONE partition of the child RDD.</p>
                        <p style="margin-top: 1rem;"><strong>Wide Dependency:</strong> Multiple child partitions depend on a single parent partition (or vice versa), requiring data shuffle.</p>
                    </div>

                    <div class="keyword-tags">
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Lineage Graph</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> DAG</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Partition Dependencies</span>
                        <span class="keyword-tag"><i class="fas fa-tag"></i> Fault Recovery</span>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-table"></i>
                        Comparison Table
                    </div>

                    <div class="table-container">
                        <table>
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Narrow Dependency</th>
                                    <th>Wide Dependency</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Partition Relationship</strong></td>
                                    <td>1-to-1 or N-to-1</td>
                                    <td>N-to-N (many-to-many)</td>
                                </tr>
                                <tr>
                                    <td><strong>Data Movement</strong></td>
                                    <td>No shuffle needed</td>
                                    <td>Shuffle required</td>
                                </tr>
                                <tr>
                                    <td><strong>Stage Boundary</strong></td>
                                    <td>Same stage</td>
                                    <td>Creates new stage</td>
                                </tr>
                                <tr>
                                    <td><strong>Failure Recovery</strong></td>
                                    <td>Fast (recompute lost partition only)</td>
                                    <td>Slow (may need to recompute multiple partitions)</td>
                                </tr>
                                <tr>
                                    <td><strong>Performance</strong></td>
                                    <td>Fast</td>
                                    <td>Slow</td>
                                </tr>
                                <tr>
                                    <td><strong>Examples</strong></td>
                                    <td>map, filter, union</td>
                                    <td>groupByKey, join, repartition</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-project-diagram"></i>
                        Dependency Patterns
                    </div>

                    <div class="comparison-grid">
                        <div class="comparison-card narrow">
                            <h4><i class="fas fa-stream"></i> Narrow Dependency Patterns</h4>
                            <div class="example-box" style="margin-top: 1rem;">
                                <pre><code># Pattern 1: One-to-One
# Each child partition depends on ONE parent partition
rdd2 = rdd1.map(lambda x: x * 2)

# Parent P1  Child P1
# Parent P2  Child P2
# Parent P3  Child P3

# Pattern 2: Range Dependency (union)
# Consecutive parent partitions  child partitions
rdd3 = rdd1.union(rdd2)

# Parent RDD1 partitions  Child P1, P2, P3
# Parent RDD2 partitions  Child P4, P5, P6</code></pre>
                            </div>
                        </div>

                        <div class="comparison-card wide">
                            <h4><i class="fas fa-project-diagram"></i> Wide Dependency Patterns</h4>
                            <div class="example-box" style="margin-top: 1rem;">
                                <pre><code># Pattern: Shuffle Dependency
# ALL parent partitions  EACH child partition
rdd2 = rdd1.groupByKey()

# All Parent Partitions (P1, P2, P3)
#        (shuffle)
# Each Child Partition (C1, C2, C3)

# Each child needs data from ALL parents
# because keys can be in any partition</code></pre>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-question-circle"></i>
                        Cross Questions
                    </div>
                    <div class="cross-questions">
                        <div class="cross-question-item">
                            <div class="cross-question-text">
                                <i class="fas fa-arrow-right"></i>
                                <span><strong>Which one triggers a shuffle and why?</strong></span>
                            </div>
                            <div class="cross-answer">
                                <strong>Wide dependencies</strong> trigger shuffle operations. Here's why: In narrow dependencies, each child partition needs data from only one parent partition - data can stay local. In wide dependencies, each child partition potentially needs data from ALL parent partitions (e.g., to group all "key=A" together), so data must be shuffled across the network. This shuffle involves serialization, network transfer, and disk I/O, making it expensive.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-card">
                    <div class="section-title">
                        <i class="fas fa-shield-alt"></i>
                        Fault Recovery Impact
                    </div>

                    <div class="scenario-box">
                        <div class="scenario-title">
                            <i class="fas fa-exclamation-circle"></i> Why Dependencies Matter for Fault Tolerance
                        </div>
                        <p><strong>Scenario:</strong> A node fails and loses Partition 2.</p>
                        
                        <div class="highlight-box" style="margin-top: 1rem;">
                            <strong>With Narrow Dependency (Fast Recovery):</strong>
                            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Original: rdd2 = rdd1.map(lambda x: x * 2)
# 
# Lost: rdd2 Partition 2
# 
# Recovery: Only recompute rdd1 Partition 2
# Time: Milliseconds
# 
# Partition 2 only depends on one parent partition!</code></pre>
                        </div>

                        <div class="warning-box" style="margin-top: 1rem;">
                            <strong>With Wide Dependency (Slow Recovery):</strong>
                            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Original: rdd2 = rdd1.groupByKey()
# 
# Lost: rdd2 Partition 2
# 
# Recovery: Must recompute from ALL parent partitions
# because Partition 2 could have data from any parent
# Time: Seconds/Minutes
# 
# May need to re-shuffle entire dataset!</code></pre>
                        </div>
                    </div>
                </div>

                <div class="navigation-buttons">
                    <button class="nav-btn prev" onclick="navigateQuestion('prev')">
                        <i class="fas fa-arrow-left"></i> Previous
                    </button>
                    <button class="nav-btn next" onclick="navigateQuestion('next')">
                        Next <i class="fas fa-arrow-right"></i>
                    </button>
                </div>
            </div>

<!-- Continue with remaining questions following the same pattern... Due to length constraints, I'll create a few more key questions -->

<!-- Question 18: PySpark DataFrame -->
<div class="question-content" id="q18">
    <div class="question-header">
        <h1 class="question-title">What is a PySpark DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>PySpark DataFrame</strong> is a distributed collection of data organized into named columns, similar to a table in a relational database or a pandas DataFrame. It's built on top of RDDs and provides a higher-level abstraction with optimized execution.</p>
            <p style="margin-top: 1rem;">DataFrames have a <strong>schema</strong> that defines column names and data types, enabling Spark to optimize query execution using the Catalyst optimizer.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Distributed Data Structure</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Schema</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Catalyst Optimizer</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> SQL Operations</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Named Columns</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Creating DataFrames
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-plus-circle"></i> Multiple Ways to Create DataFrames</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("DataFrame Demo").getOrCreate()

# Method 1: From Python list
data = [("Alice", 25, "Engineer"),
        ("Bob", 30, "Manager"),
        ("Charlie", 35, "Director")]

df = spark.createDataFrame(data, ["name", "age", "role"])
df.show()
# +-------+---+--------+
# |   name|age|    role|
# +-------+---+--------+
# |  Alice| 25|Engineer|
# |    Bob| 30| Manager|
# |Charlie| 35|Director|
# +-------+---+--------+

# Method 2: From CSV file
df_csv = spark.read.csv("data.csv", header=True, inferSchema=True)

# Method 3: From JSON
df_json = spark.read.json("data.json")

# Method 4: From Parquet (optimized format)
df_parquet = spark.read.parquet("data.parquet")

# Method 5: From Database
df_db = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost/db") \
    .option("dbtable", "users") \
    .option("user", "username") \
    .option("password", "password") \
    .load()

# Method 6: With explicit schema
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("salary", IntegerType(), True)
])

df_schema = spark.createDataFrame(data, schema)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-tasks"></i>
            DataFrame Operations
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-wrench"></i> Common DataFrame Operations</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Select columns
df.select("name", "age").show()

# Filter rows
df.filter(df.age > 25).show()
# or
df.where(df.age > 25).show()

# Add new column
from pyspark.sql.functions import col
df_with_bonus = df.withColumn("bonus", col("age") * 1000)

# Group by and aggregate
df.groupBy("role").count().show()

# Sort
df.orderBy("age", ascending=False).show()

# Join
df2 = spark.createDataFrame([("Alice", "USA"), ("Bob", "UK")], ["name", "country"])
joined = df.join(df2, "name", "inner")

# SQL queries
df.createOrReplaceTempView("employees")
result = spark.sql("SELECT name, age FROM employees WHERE age > 25")

# Drop duplicates
df.dropDuplicates(["name"]).show()

# Drop nulls
df.dropna().show()

# Fill nulls
df.fillna({"age": 0, "role": "Unknown"}).show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Is DataFrame schema fixed or dynamic?</strong></span>
                </div>
                <div class="cross-answer">
                    DataFrame schema is <strong>fixed at creation time</strong>. Once created, you cannot change the schema (column names/types) directly. However, you can create a NEW DataFrame with a different schema using transformations like <code>withColumn()</code>, <code>withColumnRenamed()</code>, or <code>cast()</code>. The immutability ensures consistency and enables Catalyst optimizer to plan execution efficiently.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What data sources can you create a DataFrame from?</strong></span>
                </div>
                <div class="cross-answer">
                    PySpark can create DataFrames from: <strong>1) Files:</strong> CSV, JSON, Parquet, ORC, Avro, Text. <strong>2) Databases:</strong> JDBC connections (PostgreSQL, MySQL, Oracle, etc.). <strong>3) Cloud Storage:</strong> S3, Azure Blob, Google Cloud Storage. <strong>4) Streaming:</strong> Kafka, Kinesis, Socket. <strong>5) Hive Tables:</strong> Direct integration. <strong>6) Python Collections:</strong> Lists, Pandas DataFrames. <strong>7) RDDs:</strong> Convert existing RDDs. <strong>8) APIs:</strong> Custom data sources via DataSource API.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How is it different from RDD?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>DataFrame:</strong> Has schema (structured), SQL support, Catalyst optimization, declarative API, better performance. <strong>RDD:</strong> No schema (unstructured), no SQL, manual optimization, functional API, lower-level control. Use DataFrames for 95% of workloads - they're faster and easier. Use RDDs only when you need fine-grained control or custom partitioning logic.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How is it similar to a SQL table?</strong></span>
                </div>
                <div class="cross-answer">
                    DataFrames are very similar to SQL tables: <strong>1) Structure:</strong> Both have named columns with typed data. <strong>2) Operations:</strong> Support SELECT, WHERE, GROUP BY, JOIN, etc. <strong>3) Query Language:</strong> Can use SQL directly on DataFrames. <strong>4) Optimization:</strong> Both use query optimizers (Catalyst for Spark). Key difference: DataFrames are distributed across cluster while SQL tables are typically on single database server.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Due to space constraints, I'll add placeholder divs for remaining questions with basic structure -->
<!-- Questions 19-30 would follow the same comprehensive pattern -->

<!-- Question 19: DataFrame vs RDD -->
<div class="question-content" id="q19">
    <div class="question-header">
        <h1 class="question-title">Difference between DataFrame and RDD?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>RDD (Resilient Distributed Dataset):</strong> Low-level distributed data structure without schema. Provides functional programming API with fine-grained control.</p>
            <p style="margin-top: 1rem;"><strong>DataFrame:</strong> High-level distributed data structure with schema (named columns). Built on top of RDDs with SQL-like operations and automatic optimization.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Schema</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Optimization</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> API Level</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Type Safety</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Comprehensive Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>RDD</th>
                        <th>DataFrame</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Schema</strong></td>
                        <td>No schema (unstructured)</td>
                        <td>Has schema (structured with column names & types)</td>
                    </tr>
                    <tr>
                        <td><strong>Optimization</strong></td>
                        <td>No automatic optimization</td>
                        <td>Catalyst optimizer + Tungsten execution engine</td>
                    </tr>
                    <tr>
                        <td><strong>API Level</strong></td>
                        <td>Low-level functional API</td>
                        <td>High-level declarative API</td>
                    </tr>
                    <tr>
                        <td><strong>Type Safety</strong></td>
                        <td>Compile-time type safety (Scala/Java)</td>
                        <td>Runtime type safety</td>
                    </tr>
                    <tr>
                        <td><strong>Performance</strong></td>
                        <td>Slower (no optimization)</td>
                        <td>2-10x faster (optimized execution)</td>
                    </tr>
                    <tr>
                        <td><strong>SQL Support</strong></td>
                        <td>No built-in SQL</td>
                        <td>Native SQL queries supported</td>
                    </tr>
                    <tr>
                        <td><strong>Serialization</strong></td>
                        <td>Java serialization (slower)</td>
                        <td>Off-heap storage with Tungsten (faster)</td>
                    </tr>
                    <tr>
                        <td><strong>Memory Management</strong></td>
                        <td>JVM heap (GC overhead)</td>
                        <td>Off-heap binary format (less GC)</td>
                    </tr>
                    <tr>
                        <td><strong>Operations</strong></td>
                        <td>map, filter, reduce, etc.</td>
                        <td>select, where, groupBy, join, etc.</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Custom logic, unstructured data</td>
                        <td>Structured/semi-structured data, SQL operations</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Comparison
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-cog"></i> RDD Example - Word Count</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># RDD - Lower level, functional programming
from pyspark import SparkContext

sc = SparkContext("local", "RDD Example")

# Read text file as RDD
text_rdd = sc.textFile("data.txt")

# Word count using RDD transformations
word_counts = text_rdd.flatMap(lambda line: line.split()) \
                      .map(lambda word: (word, 1)) \
                      .reduceByKey(lambda a, b: a + b)

result = word_counts.collect()
print(result)  # [('hello', 3), ('world', 2), ...]

# No automatic optimization
# Manual control over operations
# Type is (String, Int) but not enforced by schema</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-table"></i> DataFrame Example - Same Task</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># DataFrame - Higher level, declarative
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, col

spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()

# Read text file as DataFrame
df = spark.read.text("data.txt")

# Word count using DataFrame operations
word_counts = df.select(explode(split(col("value"), " ")).alias("word")) \
                .groupBy("word") \
                .count() \
                .orderBy("count", ascending=False)

word_counts.show()
# +-----+-----+
# | word|count|
# +-----+-----+
# |hello|    3|
# |world|    2|
# +-----+-----+

# Automatic Catalyst optimization
# SQL support available
# Schema enforced: (word: String, count: Long)</code></pre>
        </div>

        <div class="highlight-box">
            <strong>Performance Difference:</strong> The DataFrame version is typically 2-5x faster due to Catalyst optimizer rewriting the execution plan before running!
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When would you prefer to use RDD instead of DataFrame?</strong></span>
                </div>
                <div class="cross-answer">
                    Use RDD when: <strong>1) Unstructured data:</strong> Text, binary data without clear schema. <strong>2) Fine-grained control:</strong> Need custom partitioning logic. <strong>3) Complex transformations:</strong> Operations not easily expressible in DataFrame API. <strong>4) Low-level control:</strong> Need to control exactly how data is processed. <strong>5) Legacy code:</strong> Maintaining older Spark applications. <strong>However,</strong> in 95% of cases, DataFrames are the better choice due to performance and ease of use.
                </div>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Visual Understanding
        </div>
        <div class="image-container">
            <svg width="800" height="400" viewBox="0 0 800 400">
                <rect width="800" height="400" fill="#1e293b"/>
                
                <text x="400" y="30" font-size="20" fill="#0ea5e9" text-anchor="middle" font-weight="bold">RDD vs DataFrame Architecture</text>
                
                <!-- RDD Side -->
                <rect x="50" y="60" width="320" height="310" fill="rgba(245, 158, 11, 0.1)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="210" y="90" font-size="18" fill="#f59e0b" text-anchor="middle" font-weight="bold">RDD</text>
                
                <text x="210" y="120" font-size="13" fill="#cbd5e1" text-anchor="middle">Low-Level API</text>
                
                <rect x="80" y="140" width="260" height="60" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="8"/>
                <text x="210" y="165" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">No Schema</text>
                <text x="210" y="185" font-size="11" fill="#cbd5e1" text-anchor="middle">[Row1, Row2, Row3, ...]</text>
                
                <rect x="80" y="220" width="260" height="60" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="8"/>
                <text x="210" y="245" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Functional Operations</text>
                <text x="210" y="265" font-size="11" fill="#cbd5e1" text-anchor="middle">map, filter, reduce</text>
                
                <rect x="80" y="300" width="260" height="50" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="8"/>
                <text x="210" y="320" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">No Optimization</text>
                <text x="210" y="338" font-size="11" fill="#cbd5e1" text-anchor="middle">Manual execution plan</text>
                
                <!-- DataFrame Side -->
                <rect x="430" y="60" width="320" height="310" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="590" y="90" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">DataFrame</text>
                
                <text x="590" y="120" font-size="13" fill="#cbd5e1" text-anchor="middle">High-Level API</text>
                
                <rect x="460" y="140" width="260" height="60" fill="#334155" stroke="#10b981" stroke-width="1" rx="8"/>
                <text x="590" y="165" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Structured Schema</text>
                <text x="590" y="185" font-size="11" fill="#cbd5e1" text-anchor="middle">name:String, age:Int, ...</text>
                
                <rect x="460" y="220" width="260" height="60" fill="#334155" stroke="#10b981" stroke-width="1" rx="8"/>
                <text x="590" y="245" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">SQL-like Operations</text>
                <text x="590" y="265" font-size="11" fill="#cbd5e1" text-anchor="middle">select, where, groupBy</text>
                
                <rect x="460" y="300" width="260" height="50" fill="#334155" stroke="#10b981" stroke-width="1" rx="8"/>
                <text x="590" y="320" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Catalyst Optimizer</text>
                <text x="590" y="338" font-size="11" fill="#cbd5e1" text-anchor="middle">Automatic optimization</text>
            </svg>
            <p class="image-caption">DataFrames provide higher-level abstraction with automatic optimization</p>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 20: DataFrame vs Dataset APIs -->
<div class="question-content" id="q20">
    <div class="question-header">
        <h1 class="question-title">What's the difference between DataFrame and Dataset APIs?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Dataset:</strong> Strongly-typed, object-oriented API available in Scala and Java. Provides compile-time type safety with schema.</p>
            <p style="margin-top: 1rem;"><strong>DataFrame:</strong> Untyped API (or Dataset[Row]) available in all languages including Python and R. Type checking happens at runtime.</p>
            <p style="margin-top: 1rem;"><strong>Key Point:</strong> DataFrame = Dataset[Row] in Scala/Java. In PySpark, we only have DataFrames (no Dataset API).</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Type Safety</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Compile-time vs Runtime</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Language Support</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Dataset[Row]</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Detailed Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>DataFrame</th>
                        <th>Dataset</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Type Safety</strong></td>
                        <td>Runtime type checking</td>
                        <td>Compile-time type checking</td>
                    </tr>
                    <tr>
                        <td><strong>Language Support</strong></td>
                        <td>Scala, Java, Python, R, SQL</td>
                        <td>Only Scala and Java</td>
                    </tr>
                    <tr>
                        <td><strong>API Style</strong></td>
                        <td>Untyped (column names as strings)</td>
                        <td>Typed (case classes/POJOs)</td>
                    </tr>
                    <tr>
                        <td><strong>Performance</strong></td>
                        <td>Slightly faster (no serialization overhead)</td>
                        <td>Slightly slower (object serialization)</td>
                    </tr>
                    <tr>
                        <td><strong>Error Detection</strong></td>
                        <td>At runtime when query executes</td>
                        <td>At compile time before running</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Data engineering, ETL, SQL operations</td>
                        <td>Complex domain objects, type safety needed</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Examples (Scala)
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-table"></i> DataFrame Example (Untyped)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>// Scala DataFrame Example
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder().getOrCreate()

// DataFrame (untyped)
val df = spark.read.json("people.json")

// Column names are strings - no compile-time checking
df.select("name", "age")  // Typo in "age" not caught until runtime
  .filter($"age" > 21)
  .show()

// If you mistype a column name, it fails at RUNTIME
df.select("naem")  // Compiles fine, fails when executed!</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-shield-alt"></i> Dataset Example (Typed)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>// Scala Dataset Example
case class Person(name: String, age: Int)

val spark = SparkSession.builder().getOrCreate()

// Dataset (strongly typed)
val ds: Dataset[Person] = spark.read.json("people.json").as[Person]

// Type-safe operations - compile-time checking
ds.filter(p => p.age > 21)  // 'age' is checked at compile time
  .map(p => p.name.toUpperCase)
  .show()

// If you mistype a property, it fails at COMPILE TIME
ds.map(p => p.naem)  // Compile error: 'naem' doesn't exist!

// Benefits: Catch errors before running, IDE autocomplete works</code></pre>
        </div>

        <div class="note-box">
            <strong>Note:</strong> PySpark only has DataFrame API. Python is dynamically typed, so compile-time type safety isn't possible. For type safety in PySpark, use type hints and tools like mypy.
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which one is type-safe?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Dataset</strong> is type-safe at compile time (in Scala/Java). Datasets use case classes/POJOs, so the compiler can check if you're accessing valid fields. DataFrame is untyped - column names are strings, so typos are only caught at runtime. In PySpark, we only have DataFrames which are runtime-typed.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Why does PySpark primarily use DataFrame API?</strong></span>
                </div>
                <div class="cross-answer">
                    PySpark only has DataFrame API because: <strong>1) Python is dynamically typed:</strong> Compile-time type safety isn't possible. <strong>2) Performance:</strong> Dataset's serialization overhead would be worse in Python. <strong>3) Simplicity:</strong> One API is easier to learn and maintain. <strong>4) Flexibility:</strong> DataFrames work well with Python's dynamic nature. Dataset API is only available in statically-typed languages (Scala/Java).
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 21: Role of Schema -->
<div class="question-content" id="q21">
    <div class="question-header">
        <h1 class="question-title">What is the role of schema in Spark DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Expert</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Schema</strong> defines the structure of a DataFrame - the names and data types of columns. It's like a blueprint that tells Spark how to interpret the data.</p>
            <p style="margin-top: 1rem;">Schema is critical for: <strong>1) Type safety,</strong> <strong>2) Query optimization,</strong> <strong>3) Early error detection,</strong> <strong>4) Memory efficiency,</strong> and <strong>5) Consistent data interpretation.</strong></p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> StructType</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> StructField</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Types</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Column Names</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Nullable</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-tasks"></i>
            Key Roles of Schema
        </div>

        <div style="display: grid; gap: 1rem;">
            <div class="highlight-box">
                <strong>1. Query Optimization</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">Catalyst optimizer uses schema to generate efficient execution plans. Knowing data types allows predicate pushdown, column pruning, and better memory allocation.</p>
            </div>

            <div class="highlight-box">
                <strong>2. Type Safety & Validation</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">Schema ensures data conforms to expected types. Operations on columns are validated against schema, catching errors early.</p>
            </div>

            <div class="highlight-box">
                <strong>3. Memory Efficiency</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">With known types, Spark uses efficient binary format (Tungsten) instead of Java objects, reducing memory footprint and GC overhead.</p>
            </div>

            <div class="highlight-box">
                <strong>4. Performance</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">Explicit schema avoids expensive schema inference and enables faster reads from Parquet, ORC, and other columnar formats.</p>
            </div>

            <div class="highlight-box">
                <strong>5. Data Quality</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">Schema acts as a contract, ensuring data consistency across different stages of processing.</p>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Working with Schemas
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-wrench"></i> Viewing Schema</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Schema Demo").getOrCreate()

# Create DataFrame
data = [("Alice", 25, 50000), ("Bob", 30, 60000)]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# View schema
df.printSchema()
# root
#  |-- name: string (nullable = true)
#  |-- age: long (nullable = true)
#  |-- salary: long (nullable = true)

# Get schema object
schema = df.schema
print(type(schema))  # <class 'pyspark.sql.types.StructType'>

# Get column names
print(df.columns)  # ['name', 'age', 'salary']

# Get data types
print(df.dtypes)  # [('name', 'string'), ('age', 'bigint'), ('salary', 'bigint')]</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How do you define a custom schema manually?</strong></span>
                </div>
                <div class="cross-answer">
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Define custom schema
schema = StructType([
    StructField("name", StringType(), True),  # True = nullable
    StructField("age", IntegerType(), False), # False = not nullable
    StructField("salary", IntegerType(), True)
])

# Use schema when creating DataFrame
df = spark.createDataFrame(data, schema)

# Or when reading files
df = spark.read.schema(schema).csv("data.csv")</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens if inferSchema is set to false?</strong></span>
                </div>
                <div class="cross-answer">
                    When <code>inferSchema=False</code> (default for CSV), all columns are read as <strong>StringType</strong>. This is fast but requires manual casting for numeric operations. Example: <code>df.read.csv("data.csv", header=True, inferSchema=False)</code> reads all columns as strings. You'll need to cast: <code>df.withColumn("age", col("age").cast("int"))</code>. Without proper types, Catalyst optimizer cannot optimize effectively.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 22: Schema Inference -->
<div class="question-content" id="q22">
    <div class="question-header">
        <h1 class="question-title">What is schema inference?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Expert</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Schema Inference</strong> is the process where Spark automatically determines the data types and structure of a DataFrame by scanning the data. This happens when you read files without providing an explicit schema.</p>
            <p style="margin-top: 1rem;">Spark samples the data (reads a portion) to infer column types like String, Integer, Double, etc.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Automatic Detection</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Sampling</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Type Inference</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> inferSchema Option</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Schema Inference Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-magic"></i> Automatic Schema Inference</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># CSV file: data.csv
# name,age,salary
# Alice,25,50000
# Bob,30,60000

# WITH schema inference (inferSchema=True)
df_inferred = spark.read.csv("data.csv", header=True, inferSchema=True)

df_inferred.printSchema()
# root
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)       Inferred as integer
#  |-- salary: integer (nullable = true)    Inferred as integer

# WITHOUT schema inference (inferSchema=False, default)
df_no_infer = spark.read.csv("data.csv", header=True, inferSchema=False)

df_no_infer.printSchema()
# root
#  |-- name: string (nullable = true)
#  |-- age: string (nullable = true)        Read as string
#  |-- salary: string (nullable = true)     Read as string</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            Pros and Cons
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-check-circle"></i> Advantages</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Convenience:</strong> No need to define schema manually</li>
                    <li><strong>Quick prototyping:</strong> Fast data exploration</li>
                    <li><strong>Automatic types:</strong> Correctly infers most common types</li>
                </ul>
            </div>

            <div class="comparison-card wide">
                <h4><i class="fas fa-exclamation-triangle"></i> Disadvantages</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Performance cost:</strong> Extra scan pass over data</li>
                    <li><strong>Inaccuracy:</strong> May infer wrong types</li>
                    <li><strong>Instability:</strong> Schema may change if data changes</li>
                    <li><strong>Large files:</strong> Very slow for big datasets</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When should you disable schema inference?</strong></span>
                </div>
                <div class="cross-answer">
                    Disable schema inference when: <strong>1) Performance matters:</strong> Inference requires extra data scan. <strong>2) Large files:</strong> Inference can take minutes on TBs of data. <strong>3) Production environments:</strong> Explicit schema is more reliable. <strong>4) Known schema:</strong> Why infer when you already know? <strong>5) Data quality:</strong> Enforce specific types. <strong>Best Practice:</strong> Always use explicit schema in production!
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to define schema manually?</strong></span>
                </div>
                <div class="cross-answer">
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.types import *

# Method 1: Using StructType
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), False),
    StructField("salary", DoubleType(), True)
])

df = spark.read.schema(schema).csv("data.csv", header=True)

# Method 2: Using DDL string (simpler)
schema_ddl = "name STRING, age INT, salary DOUBLE"
df = spark.read.schema(schema_ddl).csv("data.csv", header=True)</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What are the drawbacks of automatic schema inference?</strong></span>
                </div>
                <div class="cross-answer">
                    Drawbacks: <strong>1) Performance:</strong> Requires full data scan before reading. <strong>2) Inaccuracy:</strong> May infer "123" as string if first rows have non-numeric data. <strong>3) Inconsistency:</strong> Different data samples may infer different schemas. <strong>4) Memory:</strong> Loads data twice (once for inference, once for actual read). <strong>5) Production risk:</strong> Schema changes if data changes. <strong>Solution:</strong> Always define explicit schema for production jobs!
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How can you define schema manually?</strong></span>
                </div>
                <div class="cross-answer">
                    Same answer as "How to define schema manually?" above - use StructType with StructField objects or DDL string notation.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 23: View DataFrame Schema -->
<div class="question-content" id="q23">
    <div class="question-header">
        <h1 class="question-title">How do you view the schema of a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Methods to View Schema
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-eye"></i> Multiple Ways</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Method 1: printSchema() - Most readable
df.printSchema()
# root
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)
#  |-- salary: double (nullable = true)

# Method 2: schema property - Returns StructType object
schema_obj = df.schema
print(schema_obj)
# StructType(List(StructField(name,StringType,true),...))

# Method 3: columns - Just column names
print(df.columns)
# ['name', 'age', 'salary']

# Method 4: dtypes - Column names with types
print(df.dtypes)
# [('name', 'string'), ('age', 'int'), ('salary', 'double')]</code></pre>
        </div>

        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the difference between printSchema() and schema?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>printSchema():</strong> Prints schema to console in tree format (human-readable). Returns None. Use for debugging/exploration. <strong>schema:</strong> Returns StructType object that you can programmatically inspect and manipulate. Use when you need to work with schema in code.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 24: Get Column Names & Data Types -->
<div class="question-content" id="q24">
    <div class="question-header">
        <h1 class="question-title">How do you get column names and data types of a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Getting Column Information
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-list"></i> Column Names & Types</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Get column names
column_names = df.columns
print(column_names)  # ['name', 'age', 'salary']

# Get data types
data_types = df.dtypes
print(data_types)
# [('name', 'string'), ('age', 'int'), ('salary', 'double')]

# Iterate through columns and types
for col_name, col_type in df.dtypes:
    print(f"{col_name}: {col_type}")
# name: string
# age: int
# salary: double

# Get specific column type
from pyspark.sql.types import *
age_type = df.schema["age"].dataType
print(age_type)  # IntegerType

# Check if column exists
if "age" in df.columns:
    print("Age column exists!")

# Count columns
print(f"Number of columns: {len(df.columns)}")</code></pre>
        </div>

        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between dtypes and printSchema()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>dtypes:</strong> Returns list of tuples [(col_name, type)]. Programmatic access. Good for iteration. <strong>printSchema():</strong> Prints tree structure to console. Shows nested structure better. Human-readable. Returns None. Use dtypes when you need to process column information in code.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 25: Using show() -->
<div class="question-content" id="q25">
    <div class="question-header">
        <h1 class="question-title">What is show() used for?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>show()</strong> is an action that displays the first N rows of a DataFrame in a tabular format in the console. Default is 20 rows.</p>
            <p style="margin-top: 1rem;">It's primarily used for data exploration and debugging during development.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Using show()
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-table"></i> show() Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Basic show() - displays 20 rows by default
df.show()
# +-----+---+------+
# | name|age|salary|
# +-----+---+------+
# |Alice| 25| 50000|
# |  Bob| 30| 60000|
# +-----+---+------+

# Show specific number of rows
df.show(5)  # Show first 5 rows

# Show without truncation
df.show(truncate=False)  # Show full column content

# Show with custom truncation
df.show(truncate=10)  # Truncate columns to 10 characters

# Show vertically (good for wide DataFrames)
df.show(vertical=True)
# -RECORD 0------------------
#  name   | Alice
#  age    | 25
#  salary | 50000
# -RECORD 1------------------
#  name   | Bob
#  age    | 30
#  salary | 60000

# Combine options
df.show(n=10, truncate=False, vertical=True)</code></pre>
        </div>

        <div class="warning-box">
            <strong> Warning:</strong> show() is an action that triggers computation. Use it only for debugging/exploration, not in production code for large datasets!
        </div>

        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you limit the number of rows shown in the output?</strong></span>
                </div>
                <div class="cross-answer">
                    Yes! Use <code>df.show(n)</code> where n is the number of rows. Example: <code>df.show(10)</code> shows first 10 rows. Default is 20 rows if you don't specify.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to display full column content instead of truncated values?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>df.show(truncate=False)</code>. By default, show() truncates columns to 20 characters. Setting truncate=False displays full content. You can also set custom truncation: <code>df.show(truncate=50)</code> truncates at 50 characters.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 26: View First Few Rows -->
<div class="question-content" id="q26">
    <div class="question-header">
        <h1 class="question-title">How can you view first few rows of a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Multiple Methods
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-list-ol"></i> Different Ways to View Rows</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Method 1: show(n) - Display in table format
df.show(5)  # Show first 5 rows in console

# Method 2: head(n) - Return as list of Row objects
first_5 = df.head(5)
print(first_5)
# [Row(name='Alice', age=25), Row(name='Bob', age=30), ...]

# Method 3: take(n) - Same as head()
first_3 = df.take(3)
print(first_3)

# Method 4: first() - Just the first row
first_row = df.first()
print(first_row)  # Row(name='Alice', age=25, salary=50000)

# Method 5: limit(n).show() - Create new DF with n rows
df.limit(10).show()

# Access values from Row object
row = df.first()
print(row.name)      # Alice
print(row['age'])    # 25
print(row[2])        # 50000 (by index)</code></pre>
        </div>

        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the difference between show() and head()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>show():</strong> Prints to console in table format. Returns None. For viewing only. <strong>head():</strong> Returns list of Row objects. Can be stored in variable and processed programmatically. Use show() for quick viewing, head() when you need to work with the data in Python code.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 27: View Data from RDD/DataFrame -->
<div class="question-content" id="q27">
    <div class="question-header">
        <h1 class="question-title">How can you view data from an RDD or DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Viewing Methods Comparison
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-cogs"></i> RDD Methods</h4>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># RDD viewing methods
rdd = sc.parallelize([1,2,3,4,5])

# 1. collect() - All data
all_data = rdd.collect()
print(all_data)  # [1,2,3,4,5]

# 2. take(n) - First n elements
first_3 = rdd.take(3)
print(first_3)  # [1,2,3]

# 3. first() - First element
first = rdd.first()
print(first)  # 1

# 4. top(n) - Top n elements
top_2 = rdd.top(2)
print(top_2)  # [5,4]

# 5. foreach() - Print each
rdd.foreach(lambda x: print(x))</code></pre>
                </div>
            </div>

            <div class="comparison-card narrow">
                <h4><i class="fas fa-table"></i> DataFrame Methods</h4>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># DataFrame viewing methods
# (All from previous questions)

# 1. show() - Table display
df.show()

# 2. collect() - All rows
all_rows = df.collect()

# 3. take(n)/head(n)
first_rows = df.take(5)

# 4. first() - First row
first_row = df.first()

# 5. toPandas() - Convert to pandas
pdf = df.limit(100).toPandas()
print(pdf.head())</code></pre>
                </div>
            </div>
        </div>

        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the difference between .show() and .collect()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>show():</strong> DataFrame only. Prints to console. Returns None. Only brings limited rows to driver. Safe for large data. <strong>collect():</strong> Both RDD and DataFrame. Returns data as Python list/array. Brings ALL data to driver. Dangerous for large datasets (can cause OOM). Use show() for viewing, collect() only when you need all data in driver (small datasets only).
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Why is .collect() risky on large datasets?</strong></span>
                </div>
                <div class="cross-answer">
                    <code>collect()</code> brings ALL data from executors to the driver node. If you have 1TB of data, it tries to load 1TB into driver's memory  Out of Memory error! Driver crashes. Data loss. Use take(n) or show() instead for large datasets. Only use collect() when dataset is small (< 1GB) and you need all data in driver.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 28: collect() vs take() vs show() -->
<div class="question-content" id="q28">
    <div class="question-header">
        <h1 class="question-title">Difference between collect(), take(), and show()</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Comprehensive Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>collect()</th>
                        <th>take(n)</th>
                        <th>show(n)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Availability</strong></td>
                        <td>RDD & DataFrame</td>
                        <td>RDD & DataFrame</td>
                        <td>DataFrame only</td>
                    </tr>
                    <tr>
                        <td><strong>Returns</strong></td>
                        <td>All data as list</td>
                        <td>First n elements as list</td>
                        <td>None (prints to console)</td>
                    </tr>
                    <tr>
                        <td><strong>Data to Driver</strong></td>
                        <td>ALL data</td>
                        <td>Only n elements</td>
                        <td>Only n rows (default 20)</td>
                    </tr>
                    <tr>
                        <td><strong>Safety</strong></td>
                        <td>Dangerous for large data</td>
                        <td>Safe (limited data)</td>
                        <td>Safe (limited data)</td>
                    </tr>
                    <tr>
                        <td><strong>Output Format</strong></td>
                        <td>Python list/array</td>
                        <td>Python list/array</td>
                        <td>Formatted table</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Process all data in driver</td>
                        <td>Get sample for processing</td>
                        <td>Quick data inspection</td>
                    </tr>
                    <tr>
                        <td><strong>Memory Risk</strong></td>
                        <td>HIGH (OOM possible)</td>
                        <td>LOW</td>
                        <td>LOW</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Side-by-Side Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code-compare"></i> Comparing All Three</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["name", "age"])

# 1. collect() - Returns ALL data as list
all_data = df.collect()
print(type(all_data))  # <class 'list'>
print(all_data)
# [Row(name='Alice', age=25), Row(name='Bob', age=30), Row(name='Charlie', age=35)]

# Can iterate and process
for row in all_data:
    print(f"{row.name}: {row.age}")

# 2. take(n) - Returns FIRST n rows as list
first_2 = df.take(2)
print(type(first_2))  # <class 'list'>
print(first_2)
# [Row(name='Alice', age=25), Row(name='Bob', age=30)]

# Same as collect() but limited
for row in first_2:
    print(f"{row.name}: {row.age}")

# 3. show(n) - Prints to console, returns None
result = df.show(2)
# +-------+---+
# |   name|age|
# +-------+---+
# |  Alice| 25|
# |    Bob| 30|
# +-------+---+

print(type(result))  # <class 'NoneType'>
# Cannot assign or process - just for viewing!

# Risk Demonstration
big_df = spark.range(1, 10000000)  # 10 million rows

# DANGER: This can crash!
# all_data = big_df.collect()  # DON'T DO THIS!

# SAFE: Limited data
sample = big_df.take(10)  # OK - only 10 rows
big_df.show(10)           # OK - only displays 10 rows</code></pre>
        </div>

        <div class="warning-box">
            <strong> Critical Rule:</strong> Never use collect() on large datasets! Always prefer take(n) or show(n) for data inspection.
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 29: take() vs collect() -->
<div class="question-content" id="q29">
    <div class="question-header">
        <h1 class="question-title">What is the difference between take() and collect()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Key Differences
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-hand-paper"></i> take(n)</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem; margin-top: 1rem;">
                    <li><strong>Limited:</strong> Returns only first n elements</li>
                    <li><strong>Safe:</strong> Controls memory usage</li>
                    <li><strong>Efficient:</strong> Stops after finding n elements</li>
                    <li><strong>Predictable:</strong> Always same size (n elements)</li>
                    <li><strong>Use case:</strong> Sampling, inspection, testing</li>
                </ul>
                <div class="highlight-box" style="margin-top: 1rem;">
                    <strong>Memory:</strong> O(n) - Only n elements in driver memory
                </div>
            </div>

            <div class="comparison-card wide">
                <h4><i class="fas fa-exclamation-triangle"></i> collect()</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem; margin-top: 1rem;">
                    <li><strong>Unlimited:</strong> Returns ALL elements</li>
                    <li><strong>Risky:</strong> Can cause OOM errors</li>
                    <li><strong>Expensive:</strong> Processes entire dataset</li>
                    <li><strong>Variable:</strong> Size depends on dataset</li>
                    <li><strong>Use case:</strong> Small datasets only</li>
                </ul>
                <div class="warning-box" style="margin-top: 1rem;">
                    <strong>Memory:</strong> O(dataset_size) - Can be GBs/TBs!
                </div>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Practical Example
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-balance-scale"></i> Safe vs Unsafe</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Scenario: 1 billion row dataset
large_df = spark.range(1, 1000000000)  # 1 billion rows

# SAFE: take(n) - Only brings 100 rows to driver
sample = large_df.take(100)
print(f"Got {len(sample)} rows")  # Got 100 rows
# Memory used: ~few KB

# UNSAFE: collect() - Tries to bring 1 billion rows!
# all_data = large_df.collect()  # DON'T DO THIS!
# Result: OutOfMemoryError, driver crash!

# Real-world example: Get top 10 customers by revenue
top_10 = sales_df.orderBy("revenue", ascending=False).take(10)
#  Safe: Only 10 rows to driver

# WRONG approach:
# all_sales = sales_df.collect()  # Brings millions of rows!
# sorted_sales = sorted(all_sales, key=lambda x: x.revenue)
# top_10 = sorted_sales[-10:]  # Inefficient and dangerous!

# Rule of thumb:
# - Dataset < 1GB  collect() is OK
# - Dataset > 1GB  NEVER use collect(), use take(n)</code></pre>
        </div>

        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Why is collect() considered risky for large datasets?</strong></span>
                </div>
                <div class="cross-answer">
                    collect() is risky because: <strong>1) Memory:</strong> Brings ALL data to single driver node. If data is 100GB, driver needs 100GB+ RAM. <strong>2) Network:</strong> Massive data transfer from executors to driver. <strong>3) Crash:</strong> OOM error kills driver and entire application. <strong>4) Performance:</strong> Serialization/deserialization of huge dataset is slow. <strong>5) Loss:</strong> All progress lost if driver crashes. Always use take(n) or write to storage instead!
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 30: Using collect() -->
<div class="question-content" id="q30">
    <div class="question-header">
        <h1 class="question-title">What is the use of .collect()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition & Purpose
        </div>
        <div class="definition-box">
            <p><strong>collect()</strong> is an action that retrieves ALL data from a distributed RDD/DataFrame to the driver program as a Python list. It triggers execution of all pending transformations.</p>
            <p style="margin-top: 1rem;"><strong>Purpose:</strong> To work with data in driver program using Python code (e.g., visualization, further processing with Python libraries).</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Action</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Driver Program</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Retrieval</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Memory Risk</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-check-circle"></i>
            When to Use collect()
        </div>

        <div style="display: grid; gap: 1rem;">
            <div class="highlight-box">
                <strong> Small Result Sets</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">After aggregations that produce small results (e.g., group by that results in 100 rows)</p>
            </div>

            <div class="highlight-box">
                <strong> Visualization</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">Need data in driver for plotting with matplotlib, plotly, etc.</p>
            </div>

            <div class="highlight-box">
                <strong> Testing & Development</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">Working with small sample datasets during development</p>
            </div>

            <div class="highlight-box">
                <strong> Final Results</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">Collecting final aggregated metrics (e.g., total count, average)</p>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-times-circle"></i>
            When NOT to Use collect()
        </div>

        <div style="display: grid; gap: 1rem;">
            <div class="warning-box">
                <strong> Large Datasets</strong>
                <p style="margin-top: 0.5rem;">Never collect millions of rows - use take(n) or write to storage</p>
            </div>

            <div class="warning-box">
                <strong> Production Pipelines</strong>
                <p style="margin-top: 0.5rem;">Don't collect in production ETL jobs - process distributed and write results</p>
            </div>

            <div class="warning-box">
                <strong> Before Aggregation</strong>
                <p style="margin-top: 0.5rem;">Don't collect raw data and aggregate in Python - do aggregation in Spark first</p>
            </div>

            <div class="warning-box">
                <strong> Iterative Processing</strong>
                <p style="margin-top: 0.5rem;">Don't collect for row-by-row processing - use Spark transformations instead</p>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Good vs Bad Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-check-circle" style="color: var(--success-color);"></i> GOOD Use Cases</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example 1: Small aggregated result
revenue_by_country = df.groupBy("country").sum("revenue")
results = revenue_by_country.collect()  #  OK - few rows after grouping

# Example 2: Visualization
import matplotlib.pyplot as plt
summary = df.groupBy("category").count().collect()
categories = [row.category for row in summary]
counts = [row['count'] for row in summary]
plt.bar(categories, counts)  #  OK - small data for plotting

# Example 3: Final metrics
total_sales = df.agg({"revenue": "sum"}).collect()[0][0]
print(f"Total: ${total_sales}")  #  OK - single value

# Example 4: Testing with sample
sample_df = df.limit(100)
sample_data = sample_df.collect()  #  OK - limited to 100 rows</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-times-circle" style="color: var(--error-color);"></i> BAD Use Cases</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example 1: Collecting large raw data
all_transactions = transactions_df.collect()  #  BAD - millions of rows!

# Example 2: Python processing instead of Spark
all_data = df.collect()  #  BAD
filtered = [row for row in all_data if row.age > 25]  # Should use df.filter()!

# Example 3: Collecting in loop
for country in countries:
    data = df.filter(df.country == country).collect()  #  BAD - multiple collects!
    # Should aggregate in Spark first!

# CORRECT Approach:
# Aggregate in Spark, then collect results
result = df.groupBy("country").agg({"revenue": "sum"})
final_data = result.collect()  #  OK - small aggregated data</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-exclamation-triangle"></i>
            Safety Guidelines
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-shield-alt"></i> collect() Safety Checklist
            </div>
            <ol style="color: var(--text-secondary); line-height: 2; margin-left: 1.5rem; margin-top: 1rem;">
                <li><strong>Size Check:</strong> Count rows first: <code>if df.count() < 10000: df.collect()</code></li>
                <li><strong>Limit First:</strong> Use limit() before collect: <code>df.limit(1000).collect()</code></li>
                <li><strong>Aggregate First:</strong> Reduce data size before collecting</li>
                <li><strong>Sample:</strong> Use sample() for large datasets: <code>df.sample(0.01).collect()</code></li>
                <li><strong>Monitor Memory:</strong> Check driver memory before and after collect()</li>
                <li><strong>Use Alternatives:</strong> Consider take(), show(), or writing to storage</li>
            </ol>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')" disabled>
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

        </div>
    <!-- Question 31: collectAsMap() -->
<div class="question-content" id="q31">
    <div class="question-header">
        <h1 class="question-title">What does collectAsMap() do in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>collectAsMap()</strong> is an action available on PairRDDs (key-value RDDs) that returns the data as a Python dictionary (hashmap) to the driver program. The keys must be unique.</p>
            <p style="margin-top: 1rem;">It's useful when you want quick lookup access to key-value pairs in Python code.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> PairRDD</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Dictionary</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Key-Value Pairs</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Lookup</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Usage Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-map"></i> collectAsMap() Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark import SparkContext

sc = SparkContext("local", "collectAsMap Demo")

# Create PairRDD (key-value pairs)
pairs = sc.parallelize([
    ("Alice", 25),
    ("Bob", 30),
    ("Charlie", 35),
    ("David", 40)
])

# collectAsMap() - Returns Python dictionary
name_age_dict = pairs.collectAsMap()
print(type(name_age_dict))  # <class 'dict'>
print(name_age_dict)
# {'Alice': 25, 'Bob': 30, 'Charlie': 35, 'David': 40}

# Fast O(1) lookup
print(f"Alice's age: {name_age_dict['Alice']}")  # 25
print(f"Bob's age: {name_age_dict.get('Bob')}")   # 30

# Example 2: Product prices
products = sc.parallelize([
    ("laptop", 1200),
    ("phone", 800),
    ("tablet", 500)
])

price_map = products.collectAsMap()
print(f"Laptop price: ${price_map['laptop']}")  # $1200

# Example 3: Country capitals
capitals = sc.parallelize([
    ("USA", "Washington"),
    ("UK", "London"),
    ("France", "Paris"),
    ("Germany", "Berlin")
])

capital_dict = capitals.collectAsMap()
print(f"Capital of France: {capital_dict['France']}")  # Paris</code></pre>
        </div>

        <div class="warning-box">
            <strong> Important:</strong> Keys must be unique! If there are duplicate keys, only the last value for each key is retained. Use <code>groupByKey()</code> if you need to keep all values for duplicate keys.
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            Comparison: collectAsMap() vs collect()
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-map"></i> collectAsMap()</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Returns:</strong> Python dict</li>
                    <li><strong>Structure:</strong> {key: value}</li>
                    <li><strong>Lookup:</strong> O(1) - Fast</li>
                    <li><strong>Requirement:</strong> PairRDD only</li>
                    <li><strong>Unique keys:</strong> Required</li>
                    <li><strong>Use case:</strong> Quick lookups</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code>result = rdd.collectAsMap()
age = result["Alice"]  # O(1)</code></pre>
                </div>
            </div>

            <div class="comparison-card wide">
                <h4><i class="fas fa-list"></i> collect()</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Returns:</strong> Python list</li>
                    <li><strong>Structure:</strong> [(key, value), ...]</li>
                    <li><strong>Lookup:</strong> O(n) - Linear search</li>
                    <li><strong>Requirement:</strong> Any RDD</li>
                    <li><strong>Duplicates:</strong> Allowed</li>
                    <li><strong>Use case:</strong> Iteration, all data</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code>result = rdd.collect()
# Need to search: O(n)
for k, v in result:
    if k == "Alice":
        age = v</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When is it better to use collectAsMap() instead of collect()?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <strong>collectAsMap()</strong> when: <strong>1) Lookup operations:</strong> Need to frequently look up values by key (O(1) vs O(n)). <strong>2) Small key-value data:</strong> Result fits in memory and has unique keys. <strong>3) Dictionary needed:</strong> Your code naturally works with dictionaries. <strong>Example:</strong> Loading configuration (key=setting name, value=setting value), product pricing lookups, user ID to name mappings.<br><br>Use <strong>collect()</strong> when: <strong>1) Duplicate keys:</strong> Need to preserve all values. <strong>2) Order matters:</strong> Need to maintain sequence. <strong>3) Not key-value:</strong> RDD doesn't have (key, value) structure.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens if there are duplicate keys?</strong></span>
                </div>
                <div class="cross-answer">
                    If there are duplicate keys, <strong>only the last value is retained</strong> in the dictionary:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>pairs = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
result = pairs.collectAsMap()
print(result)  # {'a': 3, 'b': 2}   Only last 'a' value (3) is kept!

# To keep all values, use groupByKey():
grouped = pairs.groupByKey().mapValues(list).collectAsMap()
print(grouped)  # {'a': [1, 3], 'b': [2]}   All values preserved!</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 32: Reading CSV Files -->
<div class="question-content" id="q32">
    <div class="question-header">
        <h1 class="question-title">How do you read a CSV file into a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Basic Syntax
        </div>
        <div class="definition-box">
            <p><strong>Reading CSV files</strong> is done using <code>spark.read.csv()</code> or <code>spark.read.format("csv").load()</code>. Both methods are equivalent.</p>
            <p style="margin-top: 1rem;">CSV (Comma-Separated Values) is one of the most common file formats for structured data.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> CSV Format</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Header Option</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Schema Inference</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Delimiter</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Reading CSV Files
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-file-csv"></i> Basic CSV Reading</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("CSV Reading").getOrCreate()

# Method 1: Using .csv() - Simplest
df = spark.read.csv("data.csv")

# Method 2: Using .format() and .load() - More flexible
df = spark.read.format("csv").load("data.csv")

# Method 3: With common options
df = spark.read.csv(
    "data.csv",
    header=True,        # First row contains column names
    inferSchema=True,   # Automatically infer data types
    sep=","             # Column separator (default is comma)
)

# Method 4: With all useful options
df = spark.read.csv(
    "data.csv",
    header=True,
    inferSchema=True,
    sep=",",
    quote='"',                    # Quote character for strings
    escape="\\",                  # Escape character
    nullValue="NULL",             # String to interpret as null
    dateFormat="yyyy-MM-dd",      # Date format
    timestampFormat="yyyy-MM-dd HH:mm:ss",
    mode="PERMISSIVE",            # Error handling: PERMISSIVE/DROPMALFORMED/FAILFAST
    encoding="UTF-8",             # File encoding
    comment="#"                   # Skip lines starting with this
)

df.show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-cogs"></i> CSV Options Explained</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example CSV file: employees.csv
# name,age,salary,department
# Alice,25,50000,Engineering
# Bob,30,60000,Marketing
# Charlie,35,75000,Sales

# WITHOUT header=True (default)
df_no_header = spark.read.csv("employees.csv")
df_no_header.show()
# +-------+---+-----+-----------+
# |    _c0|_c1|  _c2|        _c3|
# +-------+---+-----+-----------+
# |   name|age|salary|department|   Header treated as data!
# |  Alice| 25|50000|Engineering|
# +-------+---+-----+-----------+

# WITH header=True
df_with_header = spark.read.csv("employees.csv", header=True)
df_with_header.show()
# +-------+---+------+-----------+
# |   name|age|salary| department|   Correct headers
# +-------+---+------+-----------+
# |  Alice| 25| 50000|Engineering|
# |    Bob| 30| 60000|  Marketing|
# +-------+---+------+-----------+

# WITHOUT inferSchema (default - all columns as string)
df_no_infer = spark.read.csv("employees.csv", header=True)
df_no_infer.printSchema()
# root
#  |-- name: string
#  |-- age: string       String instead of int!
#  |-- salary: string    String instead of int!
#  |-- department: string

# WITH inferSchema=True
df_with_infer = spark.read.csv("employees.csv", header=True, inferSchema=True)
df_with_infer.printSchema()
# root
#  |-- name: string
#  |-- age: integer      Correctly inferred!
#  |-- salary: integer   Correctly inferred!
#  |-- department: string</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Common CSV Options
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Option</th>
                        <th>Default</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>header</strong></td>
                        <td>false</td>
                        <td>First row is column names</td>
                        <td>header=True</td>
                    </tr>
                    <tr>
                        <td><strong>inferSchema</strong></td>
                        <td>false</td>
                        <td>Automatically detect types</td>
                        <td>inferSchema=True</td>
                    </tr>
                    <tr>
                        <td><strong>sep</strong></td>
                        <td>,</td>
                        <td>Column delimiter</td>
                        <td>sep="|" or sep="\t"</td>
                    </tr>
                    <tr>
                        <td><strong>quote</strong></td>
                        <td>"</td>
                        <td>Quote character</td>
                        <td>quote="'"</td>
                    </tr>
                    <tr>
                        <td><strong>escape</strong></td>
                        <td>\</td>
                        <td>Escape character</td>
                        <td>escape="\\"</td>
                    </tr>
                    <tr>
                        <td><strong>nullValue</strong></td>
                        <td>""</td>
                        <td>String for null</td>
                        <td>nullValue="NA"</td>
                    </tr>
                    <tr>
                        <td><strong>dateFormat</strong></td>
                        <td>yyyy-MM-dd</td>
                        <td>Date format</td>
                        <td>dateFormat="dd/MM/yyyy"</td>
                    </tr>
                    <tr>
                        <td><strong>mode</strong></td>
                        <td>PERMISSIVE</td>
                        <td>Error handling</td>
                        <td>mode="DROPMALFORMED"</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the difference between .read.csv() and .load()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>.read.csv():</strong> Shortcut method specifically for CSV files. Cleaner syntax.<br>
                    <strong>.format("csv").load():</strong> Generic method that works with any format. More flexible for dynamic format selection.<br><br>
                    Both are functionally identical for CSV:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># These are equivalent:
df1 = spark.read.csv("data.csv", header=True)
df2 = spark.read.format("csv").option("header", True).load("data.csv")

# .load() is more flexible:
format_type = "csv"  # Could be "parquet", "json", etc.
df = spark.read.format(format_type).load("data." + format_type)</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How do you handle header and infer schema options?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>header=True:</strong> Tells Spark that the first row contains column names. Without it, columns are named _c0, _c1, etc.<br>
                    <strong>inferSchema=True:</strong> Spark scans data to automatically determine data types. Without it, all columns are read as strings.<br><br>
                    <strong>Best practice:</strong> Use <code>header=True, inferSchema=True</code> for exploration. For production, define explicit schema (faster and more reliable).
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What are common options used in .read.csv()?</strong></span>
                </div>
                <div class="cross-answer">
                    Most common options: <code>header=True</code> (use first row as column names), <code>inferSchema=True</code> (auto-detect types), <code>sep=","</code> (delimiter, use "\t" for TSV), <code>nullValue="NA"</code> (treat "NA" as null), <code>dateFormat="yyyy-MM-dd"</code> (date parsing), <code>mode="DROPMALFORMED"</code> (skip bad rows). See table above for complete list.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens if you don't use header=True?</strong></span>
                </div>
                <div class="cross-answer">
                    Without <code>header=True</code>, Spark treats the first row as data (not column names) and generates default column names: _c0, _c1, _c2, etc. This means your actual header row becomes a data row, and you lose the meaningful column names.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What parameters would you add if the CSV had headers?</strong></span>
                </div>
                <div class="cross-answer">
                    Add <code>header=True</code> parameter. Optionally also add <code>inferSchema=True</code> to automatically detect data types. Example: <code>spark.read.csv("data.csv", header=True, inferSchema=True)</code>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to infer schema automatically?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>inferSchema=True</code> option: <code>spark.read.csv("data.csv", inferSchema=True)</code>. Spark will scan the data to determine the appropriate data type for each column (string, integer, double, etc.). Note: This adds overhead as it requires an extra pass through the data.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 33: Reading JSON Data -->
<div class="question-content" id="q33">
    <div class="question-header">
        <h1 class="question-title">How can you read JSON data in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Reading JSON</strong> in PySpark is done using <code>spark.read.json()</code>. Spark automatically infers schema from JSON structure.</p>
            <p style="margin-top: 1rem;">JSON (JavaScript Object Notation) is ideal for semi-structured data with nested fields.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> JSON Format</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Nested Data</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Schema Inference</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Semi-structured</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Reading JSON Files
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-file-code"></i> JSON Reading Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example JSON file: employees.json (one JSON object per line)
# {"name": "Alice", "age": 25, "department": "Engineering"}
# {"name": "Bob", "age": 30, "department": "Marketing"}

# Method 1: Basic JSON reading
df = spark.read.json("employees.json")
df.show()
# +---+-----------+-----+
# |age| department| name|
# +---+-----------+-----+
# | 25|Engineering|Alice|
# | 30|  Marketing|  Bob|
# +---+-----------+-----+

# Schema is automatically inferred
df.printSchema()
# root
#  |-- age: long
#  |-- department: string
#  |-- name: string

# Method 2: JSON Lines format (default)
df = spark.read.json("data.jsonl")  # One JSON per line

# Method 3: Multi-line JSON (single JSON array/object)
df = spark.read.option("multiline", True).json("data.json")

# Method 4: With options
df = spark.read.json(
    "data.json",
    multiLine=True,                    # Read entire file as one JSON
    allowComments=True,                # Allow // comments
    allowUnquotedFieldNames=True,      # Allow unquoted field names
    allowSingleQuotes=True,            # Allow single quotes
    primitivesAsString=False,          # Keep primitive types
    mode="PERMISSIVE"                  # Error handling
)

# Reading from multiple JSON files
df = spark.read.json("data/*.json")

# Reading JSON from string
from pyspark.sql.functions import from_json, schema_of_json

json_string = '{"name": "Alice", "age": 25}'
df = spark.read.json(spark.sparkContext.parallelize([json_string]))</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-sitemap"></i> Nested JSON Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example nested JSON: users.json
# {
#   "name": "Alice",
#   "age": 25,
#   "address": {
#     "street": "123 Main St",
#     "city": "New York",
#     "zip": "10001"
#   },
#   "phones": ["555-1234", "555-5678"]
# }

# Read nested JSON
df = spark.read.option("multiline", True).json("users.json")

df.printSchema()
# root
#  |-- age: long
#  |-- address: struct
#  |    |-- city: string
#  |    |-- street: string
#  |    |-- zip: string
#  |-- name: string
#  |-- phones: array
#  |    |-- element: string

df.show(truncate=False)
# +---+----------------------------------+-----+----------------------+
# |age|address                           |name |phones                |
# +---+----------------------------------+-----+----------------------+
# |25 |{New York, 123 Main St, 10001}    |Alice|[555-1234, 555-5678]  |
# +---+----------------------------------+-----+----------------------+

# Access nested fields
df.select("name", "address.city", "address.zip").show()
# +-----+--------+-----+
# | name|    city|  zip|
# +-----+--------+-----+
# |Alice|New York|10001|
# +-----+--------+-----+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What if JSON has nested structures  how do you handle them?</strong></span>
                </div>
                <div class="cross-answer">
                    Spark automatically handles nested JSON and creates <strong>StructType</strong> (for objects) and <strong>ArrayType</strong> (for arrays) in the schema. Access nested fields using dot notation: <code>df.select("address.city")</code>. For arrays, use <code>explode()</code> function to flatten them:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import explode

# Flatten array
df.select("name", explode("phones").alias("phone")).show()
# +-----+---------+
# | name|    phone|
# +-----+---------+
# |Alice|555-1234 |
# |Alice|555-5678 |
# +-----+---------+

# Access nested struct fields
df.select("name", "address.city", "address.street").show()</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 34: Reading Nested JSON -->
<div class="question-content" id="q34">
    <div class="question-header">
        <h1 class="question-title">How do you read a JSON file with nested fields?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Advanced</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Working with Nested JSON
        </div>
        <div class="definition-box">
            <p><strong>Nested JSON</strong> contains objects within objects or arrays within objects. PySpark automatically handles nested structures and represents them as <strong>StructType</strong> (for nested objects) and <strong>ArrayType</strong> (for arrays).</p>
            <p style="margin-top: 1rem;">You can access nested fields using dot notation or the <code>getField()</code> method.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Nested Structures</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> StructType</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> ArrayType</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> explode()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Flattening</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Reading and Processing Nested JSON
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-sitemap"></i> Complex Nested JSON Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example complex nested JSON: orders.json
# {
#   "order_id": "ORD-001",
#   "customer": {
#     "name": "Alice",
#     "email": "alice@example.com",
#     "address": {
#       "street": "123 Main St",
#       "city": "New York",
#       "zip": "10001"
#     }
#   },
#   "items": [
#     {"product": "Laptop", "price": 1200, "quantity": 1},
#     {"product": "Mouse", "price": 25, "quantity": 2}
#   ],
#   "total": 1250
# }

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode

spark = SparkSession.builder.appName("Nested JSON").getOrCreate()

# Read nested JSON
df = spark.read.option("multiline", True).json("orders.json")

# View schema
df.printSchema()
# root
#  |-- order_id: string
#  |-- customer: struct
#  |    |-- name: string
#  |    |-- email: string
#  |    |-- address: struct
#  |    |    |-- city: string
#  |    |    |-- street: string
#  |    |    |-- zip: string
#  |-- items: array
#  |    |-- element: struct
#  |    |    |-- product: string
#  |    |    |-- price: long
#  |    |    |-- quantity: long
#  |-- total: long

# Method 1: Access nested fields using dot notation
df.select(
    "order_id",
    "customer.name",
    "customer.email",
    "customer.address.city"
).show(truncate=False)
# +--------+-----+------------------+--------+
# |order_id|name |email             |city    |
# +--------+-----+------------------+--------+
# |ORD-001 |Alice|alice@example.com |New York|
# +--------+-----+------------------+--------+

# Method 2: Using getField() - More explicit
df.select(
    col("order_id"),
    col("customer").getField("name").alias("customer_name"),
    col("customer").getField("address").getField("city").alias("city")
).show()

# Method 3: Access with col() and brackets
df.select(
    col("order_id"),
    col("customer.name"),
    col("customer.address.city")
).show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-expand-arrows-alt"></i> Exploding Nested Arrays</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import explode, col

# Original nested structure
df.select("order_id", "items").show(truncate=False)
# +--------+-------------------------------------------------------+
# |order_id|items                                                  |
# +--------+-------------------------------------------------------+
# |ORD-001 |[{Laptop, 1200, 1}, {Mouse, 25, 2}]                   |
# +--------+-------------------------------------------------------+

# Explode array - creates one row per array element
df_exploded = df.select(
    "order_id",
    explode("items").alias("item")
)

df_exploded.show(truncate=False)
# +--------+-------------------+
# |order_id|item               |
# +--------+-------------------+
# |ORD-001 |{Laptop, 1200, 1}  |
# |ORD-001 |{Mouse, 25, 2}     |
# +--------+-------------------+

# Access fields from exploded struct
df_flat = df.select(
    "order_id",
    explode("items").alias("item")
).select(
    "order_id",
    col("item.product").alias("product"),
    col("item.price").alias("price"),
    col("item.quantity").alias("quantity")
)

df_flat.show()
# +--------+-------+-----+--------+
# |order_id|product|price|quantity|
# +--------+-------+-----+--------+
# |ORD-001 | Laptop| 1200|       1|
# |ORD-001 |  Mouse|   25|       2|
# +--------+-------+-----+--------+

# Calculate total per item
df_flat.withColumn(
    "item_total",
    col("price") * col("quantity")
).show()
# +--------+-------+-----+--------+----------+
# |order_id|product|price|quantity|item_total|
# +--------+-------+-----+--------+----------+
# |ORD-001 | Laptop| 1200|       1|      1200|
# |ORD-001 |  Mouse|   25|       2|        50|
# +--------+-------+-----+--------+----------+</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-layer-group"></i> Complete Flattening Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Fully flatten the nested JSON
df_completely_flat = df.select(
    "order_id",
    col("customer.name").alias("customer_name"),
    col("customer.email").alias("customer_email"),
    col("customer.address.street").alias("street"),
    col("customer.address.city").alias("city"),
    col("customer.address.zip").alias("zip"),
    explode("items").alias("item")
).select(
    "order_id",
    "customer_name",
    "customer_email",
    "street",
    "city",
    "zip",
    col("item.product").alias("product"),
    col("item.price").alias("price"),
    col("item.quantity").alias("quantity")
)

df_completely_flat.show(truncate=False)
# +--------+-------------+------------------+------------+--------+-----+-------+-----+--------+
# |order_id|customer_name|customer_email    |street      |city    |zip  |product|price|quantity|
# +--------+-------------+------------------+------------+--------+-----+-------+-----+--------+
# |ORD-001 |Alice        |alice@example.com |123 Main St |New York|10001|Laptop |1200 |1       |
# |ORD-001 |Alice        |alice@example.com |123 Main St |New York|10001|Mouse  |25   |2       |
# +--------+-------------+------------------+------------+--------+-----+-------+-----+--------+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How do you explode nested JSON columns?</strong></span>
                </div>
                <div class="cross-answer">
                    Use the <code>explode()</code> function to convert array elements into separate rows:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import explode

# Before: One row with array
# |order_id|items                          |
# |ORD-001 |[{item1}, {item2}, {item3}]    |

# After explode: Multiple rows
df_exploded = df.select("order_id", explode("items").alias("item"))
# |order_id|item   |
# |ORD-001 |{item1}|
# |ORD-001 |{item2}|
# |ORD-001 |{item3}|

# For nested struct + array, chain operations:
df.select("id", explode("array_col").alias("elem")) \
  .select("id", "elem.field1", "elem.field2")</code></pre>
                    Alternative: <code>explode_outer()</code> keeps rows even if array is null/empty.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the difference between explode() and posexplode()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>explode():</strong> Creates one row per array element, no position info.<br>
                    <strong>posexplode():</strong> Creates one row per array element WITH position (index).<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import posexplode

# posexplode returns (pos, col)
df.select("id", posexplode("items").alias("pos", "item")).show()
# +--+---+-------+
# |id|pos|item   |
# +--+---+-------+
# | 1|  0|item1  |
# | 1|  1|item2  |
# | 1|  2|item3  |
# +--+---+-------+</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 35: Data Sources -->
<div class="question-content" id="q35">
    <div class="question-header">
        <h1 class="question-title">What are common data sources supported by Spark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Advanced</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Overview
        </div>
        <div class="definition-box">
            <p><strong>Spark supports various data sources</strong> through its unified DataFrameReader/Writer API. You can read from files, databases, cloud storage, streaming sources, and more.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> File Formats</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Databases</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Cloud Storage</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Streaming</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-database"></i>
            Supported Data Sources
        </div>

        <div style="display: grid; gap: 1.5rem;">
            <div class="highlight-box">
                <strong>1. File-Based Sources</strong>
                <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>CSV:</strong> Comma-separated values - <code>spark.read.csv()</code></li>
                    <li><strong>JSON:</strong> JavaScript Object Notation - <code>spark.read.json()</code></li>
                    <li><strong>Parquet:</strong> Columnar format (best for analytics) - <code>spark.read.parquet()</code></li>
                    <li><strong>ORC:</strong> Optimized Row Columnar - <code>spark.read.orc()</code></li>
                    <li><strong>Avro:</strong> Binary format with schema - <code>spark.read.format("avro").load()</code></li>
                    <li><strong>Text:</strong> Plain text files - <code>spark.read.text()</code></li>
                </ul>
            </div>

            <div class="highlight-box">
                <strong>2. Database Sources (JDBC)</strong>
                <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>MySQL:</strong> Relational database</li>
                    <li><strong>PostgreSQL:</strong> Advanced relational database</li>
                    <li><strong>Oracle:</strong> Enterprise database</li>
                    <li><strong>SQL Server:</strong> Microsoft database</li>
                    <li><strong>SQLite:</strong> Embedded database</li>
                    <li><strong>Hive:</strong> Data warehouse - <code>spark.read.table()</code></li>
                </ul>
            </div>

            <div class="highlight-box">
                <strong>3. Cloud Storage</strong>
                <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Amazon S3:</strong> s3://bucket/path</li>
                    <li><strong>Azure Blob Storage:</strong> wasbs://container@account.blob.core.windows.net/</li>
                    <li><strong>Google Cloud Storage:</strong> gs://bucket/path</li>
                    <li><strong>HDFS:</strong> Hadoop Distributed File System - hdfs://namenode/path</li>
                </ul>
            </div>

            <div class="highlight-box">
                <strong>4. Streaming Sources</strong>
                <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Kafka:</strong> Distributed streaming platform</li>
                    <li><strong>Kinesis:</strong> AWS streaming service</li>
                    <li><strong>Socket:</strong> TCP socket streaming</li>
                    <li><strong>File Stream:</strong> Monitor directory for new files</li>
                </ul>
            </div>

            <div class="highlight-box">
                <strong>5. NoSQL Databases</strong>
                <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>MongoDB:</strong> Document database</li>
                    <li><strong>Cassandra:</strong> Wide-column store</li>
                    <li><strong>HBase:</strong> Hadoop database</li>
                    <li><strong>Elasticsearch:</strong> Search and analytics engine</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-file"></i> File Sources</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># CSV
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# JSON
df = spark.read.json("data.json")

# Parquet (columnar - best for analytics)
df = spark.read.parquet("data.parquet")

# ORC
df = spark.read.orc("data.orc")

# Avro
df = spark.read.format("avro").load("data.avro")

# Text file
df = spark.read.text("data.txt")</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-database"></i> Database Sources (JDBC)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># PostgreSQL
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/mydb") \
    .option("dbtable", "employees") \
    .option("user", "username") \
    .option("password", "password") \
    .option("driver", "org.postgresql.Driver") \
    .load()

# MySQL
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:mysql://localhost:3306/mydb") \
    .option("dbtable", "users") \
    .option("user", "root") \
    .option("password", "password") \
    .load()

# Custom query
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/mydb") \
    .option("query", "SELECT * FROM employees WHERE salary > 50000") \
    .option("user", "username") \
    .option("password", "password") \
    .load()

# Hive table
df = spark.read.table("database_name.table_name")</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-cloud"></i> Cloud Storage</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Amazon S3
df = spark.read.parquet("s3a://my-bucket/data/")

# Azure Blob Storage
df = spark.read.csv("wasbs://container@account.blob.core.windows.net/data.csv")

# Google Cloud Storage
df = spark.read.json("gs://my-bucket/data/*.json")

# HDFS
df = spark.read.parquet("hdfs://namenode:9000/user/data/")</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can Spark read from MySQL or PostgreSQL?</strong></span>
                </div>
                <div class="cross-answer">
                    Yes! Spark can read from any JDBC-compatible database including MySQL and PostgreSQL using the JDBC connector. You need the appropriate JDBC driver JAR file. Example:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># PostgreSQL
df = spark.read.format("jdbc") \
    .option("url", "jdbc:postgresql://host:5432/database") \
    .option("dbtable", "table_name") \
    .option("user", "username") \
    .option("password", "password") \
    .load()

# MySQL
df = spark.read.format("jdbc") \
    .option("url", "jdbc:mysql://host:3306/database") \
    .option("dbtable", "table_name") \
    .option("user", "username") \
    .option("password", "password") \
    .load()</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What connector is required for that?</strong></span>
                </div>
                <div class="cross-answer">
                    You need the <strong>JDBC driver</strong> for the specific database:<br><br>
                    <strong>PostgreSQL:</strong> <code>org.postgresql:postgresql:42.x.x</code><br>
                    <strong>MySQL:</strong> <code>mysql:mysql-connector-java:8.x.x</code><br><br>
                    Add driver when starting Spark:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Using spark-submit
spark-submit --jars postgresql-42.5.0.jar your_script.py

# In pyspark shell
pyspark --packages org.postgresql:postgresql:42.5.0

# Programmatically
spark = SparkSession.builder \
    .appName("JDBC App") \
    .config("spark.jars", "/path/to/postgresql.jar") \
    .getOrCreate()</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 36: File Formats -->
<div class="question-content" id="q36">
    <div class="question-header">
        <h1 class="question-title">What are the common file formats supported by PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Overview
        </div>
        <div class="definition-box">
            <p><strong>PySpark supports multiple file formats</strong>, each optimized for different use cases. The choice of format significantly impacts performance, storage, and compatibility.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Parquet</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> CSV</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> JSON</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> ORC</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Avro</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            File Format Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Format</th>
                        <th>Type</th>
                        <th>Compression</th>
                        <th>Schema</th>
                        <th>Best For</th>
                        <th>Performance</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Parquet</strong></td>
                        <td>Columnar</td>
                        <td>Excellent</td>
                        <td>Embedded</td>
                        <td>Analytics, Big Data</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td><strong>ORC</strong></td>
                        <td>Columnar</td>
                        <td>Excellent</td>
                        <td>Embedded</td>
                        <td>Hive, Analytics</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td><strong>Avro</strong></td>
                        <td>Row-based</td>
                        <td>Good</td>
                        <td>Embedded</td>
                        <td>Streaming, Evolution</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td><strong>CSV</strong></td>
                        <td>Text</td>
                        <td>Poor</td>
                        <td>External</td>
                        <td>Interchange, Simple</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td><strong>JSON</strong></td>
                        <td>Text</td>
                        <td>Poor</td>
                        <td>Self-describing</td>
                        <td>APIs, Nested data</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td><strong>Text</strong></td>
                        <td>Text</td>
                        <td>Basic</td>
                        <td>None</td>
                        <td>Logs, Raw text</td>
                        <td></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-info-circle"></i>
            Detailed Format Descriptions
        </div>

        <div style="display: grid; gap: 1.5rem;">
            <div class="highlight-box">
                <strong>1. Parquet ( Recommended for Analytics)</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Type:</strong> Columnar binary format<br>
                    <strong>Pros:</strong> Excellent compression (5-10x smaller), fast columnar reads, schema embedded, predicate pushdown, column pruning<br>
                    <strong>Cons:</strong> Not human-readable, slower writes<br>
                    <strong>Use Cases:</strong> Data warehousing, analytics, OLAP queries, production pipelines
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Read
df = spark.read.parquet("data.parquet")

# Write
df.write.parquet("output.parquet", mode="overwrite")</code></pre>
            </div>

            <div class="highlight-box">
                <strong>2. ORC (Optimized Row Columnar)</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Type:</strong> Columnar binary format<br>
                    <strong>Pros:</strong> Excellent compression, very fast, optimized for Hive, built-in indexes<br>
                    <strong>Cons:</strong> Less ecosystem support than Parquet<br>
                    <strong>Use Cases:</strong> Hive tables, large-scale analytics
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Read
df = spark.read.orc("data.orc")

# Write
df.write.orc("output.orc")</code></pre>
            </div>

            <div class="highlight-box">
                <strong>3. Avro</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Type:</strong> Row-based binary format<br>
                    <strong>Pros:</strong> Schema evolution, compact, good for streaming, splittable<br>
                    <strong>Cons:</strong> Not columnar (slower analytics)<br>
                    <strong>Use Cases:</strong> Kafka, streaming, schema evolution
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Read
df = spark.read.format("avro").load("data.avro")

# Write
df.write.format("avro").save("output.avro")</code></pre>
            </div>

            <div class="note-box">
                <strong>4. CSV (Comma-Separated Values)</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Type:</strong> Text format<br>
                    <strong>Pros:</strong> Human-readable, universal support, easy to edit<br>
                    <strong>Cons:</strong> No compression, no schema, slow, parsing overhead<br>
                    <strong>Use Cases:</strong> Data interchange, small datasets, Excel compatibility
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Read
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Write
df.write.csv("output.csv", header=True)</code></pre>
            </div>

            <div class="note-box">
                <strong>5. JSON (JavaScript Object Notation)</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Type:</strong> Text format<br>
                    <strong>Pros:</strong> Human-readable, self-describing, supports nested structures<br>
                    <strong>Cons:</strong> Verbose, slow parsing, large file sizes<br>
                    <strong>Use Cases:</strong> APIs, nested/hierarchical data, configuration
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Read
df = spark.read.json("data.json")

# Write
df.write.json("output.json")</code></pre>
            </div>

            <div class="note-box">
                <strong>6. Text</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Type:</strong> Plain text<br>
                    <strong>Pros:</strong> Simple, universal<br>
                    <strong>Cons:</strong> No structure, no schema<br>
                    <strong>Use Cases:</strong> Log files, raw text processing
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Read
df = spark.read.text("data.txt")

# Write
df.write.text("output.txt")</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-chart-bar"></i>
            Performance Comparison
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-tachometer-alt"></i> Storage & Performance Comparison (1GB CSV baseline)
            </div>
            <div class="table-container" style="margin-top: 1rem;">
                <table>
                    <thead>
                        <tr>
                            <th>Format</th>
                            <th>File Size</th>
                            <th>Read Speed</th>
                            <th>Write Speed</th>
                            <th>Query Performance</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>CSV</strong></td>
                            <td>1.0 GB (100%)</td>
                            <td>Slow</td>
                            <td>Fast</td>
                            <td>Poor</td>
                        </tr>
                        <tr>
                            <td><strong>JSON</strong></td>
                            <td>1.2 GB (120%)</td>
                            <td>Slow</td>
                            <td>Medium</td>
                            <td>Poor</td>
                        </tr>
                        <tr>
                            <td><strong>Avro</strong></td>
                            <td>0.4 GB (40%)</td>
                            <td>Fast</td>
                            <td>Fast</td>
                            <td>Good</td>
                        </tr>
                        <tr>
                            <td><strong>Parquet</strong></td>
                            <td>0.15 GB (15%)</td>
                            <td>Very Fast</td>
                            <td>Medium</td>
                            <td>Excellent</td>
                        </tr>
                        <tr>
                            <td><strong>ORC</strong></td>
                            <td>0.12 GB (12%)</td>
                            <td>Very Fast</td>
                            <td>Medium</td>
                            <td>Excellent</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which one is best suited for analytical workloads and why?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Parquet</strong> is best for analytical workloads because:<br><br>
                    <strong>1. Columnar storage:</strong> Reads only needed columns (column pruning), not entire rows. If query needs 3 out of 100 columns, only reads those 3.<br>
                    <strong>2. Excellent compression:</strong> 5-10x smaller than CSV. Columnar data compresses better (similar values together).<br>
                    <strong>3. Predicate pushdown:</strong> Filters data at read time using embedded statistics, skipping irrelevant data.<br>
                    <strong>4. Schema embedded:</strong> No schema inference overhead, type-safe.<br>
                    <strong>5. Splittable:</strong> Parallel processing across partitions.<br>
                    <strong>6. Ecosystem support:</strong> Works with Spark, Hive, Impala, Presto, etc.<br><br>
                    <strong>Example:</strong> A 10GB CSV becomes 1-2GB Parquet, and queries run 10-100x faster!<br><br>
                    <strong>ORC</strong> is also excellent for analytics, especially in Hive environments. Choose Parquet for broader ecosystem support.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When should you use CSV vs Parquet?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Use CSV when:</strong> Need human-readable format, data interchange with non-technical users, Excel compatibility, small datasets (&lt;100MB), quick prototyping.<br><br>
                    <strong>Use Parquet when:</strong> Production workloads, large datasets (&gt;1GB), analytical queries, need performance, long-term storage, multiple reads on same data.<br><br>
                    <strong>Best Practice:</strong> Ingest as CSV  Convert to Parquet for processing  Export as CSV if needed for end users.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 37: Reading Multiple CSVs -->
<div class="question-content" id="q37">
    <div class="question-header">
        <h1 class="question-title">How do you read data from multiple CSVs in a folder?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Reading Multiple Files
        </div>
        <div class="definition-box">
            <p><strong>Spark can read multiple files</strong> from a directory using wildcards or by specifying the directory path. All files are combined into a single DataFrame.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Wildcards</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Directory Reading</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Pattern Matching</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Union</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Methods to Read Multiple CSVs
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-folder-open"></i> Reading Multiple CSV Files</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Folder structure:
# data/
#    sales_2021.csv
#    sales_2022.csv
#    sales_2023.csv
#    sales_2024.csv

# Method 1: Read entire directory
# Combines all CSV files in the folder
df = spark.read.csv("data/", header=True, inferSchema=True)

# Method 2: Using wildcard - all CSV files
df = spark.read.csv("data/*.csv", header=True, inferSchema=True)

# Method 3: Pattern matching - specific files
df = spark.read.csv("data/sales_202*.csv", header=True, inferSchema=True)

# Method 4: Multiple specific files as list
files = [
    "data/sales_2021.csv",
    "data/sales_2022.csv",
    "data/sales_2023.csv"
]
df = spark.read.csv(files, header=True, inferSchema=True)

# Method 5: Comma-separated paths
df = spark.read.csv(
    "data/sales_2021.csv,data/sales_2022.csv,data/sales_2023.csv",
    header=True,
    inferSchema=True
)

# Method 6: Read with file source column (track which file data came from)
df = spark.read \
    .option("header", True) \
    .option("inferSchema", True) \
    .csv("data/*.csv") \
    .withColumn("source_file", spark_sql.functions.input_file_name())

df.select("source_file").distinct().show(truncate=False)
# +------------------------------------------+
# |source_file                               |
# +------------------------------------------+
# |file:///path/data/sales_2021.csv         |
# |file:///path/data/sales_2022.csv         |
# +------------------------------------------+</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-filter"></i> Advanced Pattern Matching</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Read files from multiple directories
df = spark.read.csv("data/2021/*.csv,data/2022/*.csv", header=True)

# Read files with specific pattern
df = spark.read.csv("data/sales_*.csv", header=True)

# Read files in nested directories (recursive)
df = spark.read.csv("data/**/*.csv", header=True)  # All CSV in subdirectories

# Exclude certain files (need to filter after reading or use union)
# Read all except test files
all_files = ["data/sales_2021.csv", "data/sales_2022.csv"]
df = spark.read.csv(all_files, header=True)

# Add metadata columns
from pyspark.sql.functions import input_file_name, current_timestamp

df = spark.read.csv("data/*.csv", header=True) \
    .withColumn("source_file", input_file_name()) \
    .withColumn("load_timestamp", current_timestamp())

df.show(truncate=False)
# +----+------+-------+-------------------------------------+-------------------+
# |id  |name  |amount |source_file                          |load_timestamp     |
# +----+------+-------+-------------------------------------+-------------------+
# |1   |Alice |100    |file:///path/data/sales_2021.csv    |2024-01-15 10:30:00|
# |2   |Bob   |200    |file:///path/data/sales_2022.csv    |2024-01-15 10:30:00|
# +----+------+-------+-------------------------------------+-------------------+</code></pre>
        </div>

        <div class="warning-box">
            <strong> Important:</strong> All CSV files must have the same schema (same columns in same order). If schemas differ, Spark will use the union schema and fill missing columns with null.
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Best Practices
        </div>

        <div class="highlight-box">
            <strong>1. Use Wildcards for Similar Files:</strong> <code>data/sales_*.csv</code> - cleaner than listing each file
        </div>

        <div class="highlight-box">
            <strong>2. Track Source File:</strong> Add <code>input_file_name()</code> column to know which file each row came from
        </div>

        <div class="highlight-box">
            <strong>3. Consistent Schema:</strong> Ensure all files have identical column structure
        </div>

        <div class="highlight-box">
            <strong>4. Use Partitioned Directories:</strong> Organize by date/category: <code>data/year=2024/month=01/*.csv</code>
        </div>

        <div class="highlight-box">
            <strong>5. Consider Merging:</strong> For production, combine small files into larger ones (avoid small file problem)
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 38: select() vs selectExpr() -->
<div class="question-content" id="q38">
    <div class="question-header">
        <h1 class="question-title">What's the difference between select() and selectExpr()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
            <span class="meta-tag"><i class="fas fa-signal"></i> Difficulty: Expert</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>select():</strong> Selects columns using column objects or column names. Uses DataFrame API syntax.</p>
            <p style="margin-top: 1rem;"><strong>selectExpr():</strong> Selects columns using SQL expressions as strings. More flexible, allows SQL syntax directly.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Column Selection</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> SQL Expressions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> DataFrame API</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Transformations</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Comparison Table
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>select()</th>
                        <th>selectExpr()</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Syntax</strong></td>
                        <td>Column objects or strings</td>
                        <td>SQL expressions as strings</td>
                    </tr>
                    <tr>
                        <td><strong>Flexibility</strong></td>
                        <td>DataFrame API functions</td>
                        <td>Any SQL expression</td>
                    </tr>
                    <tr>
                        <td><strong>Complex Logic</strong></td>
                        <td>Need to import functions</td>
                        <td>Write directly as SQL</td>
                    </tr>
                    <tr>
                        <td><strong>Type Safety</strong></td>
                        <td>Better (column objects)</td>
                        <td>Less (strings)</td>
                    </tr>
                    <tr>
                        <td><strong>Learning Curve</strong></td>
                        <td>Need DataFrame API knowledge</td>
                        <td>SQL knowledge sufficient</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Simple selections, programmatic</td>
                        <td>Complex expressions, SQL users</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Side-by-Side Comparison
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-columns"></i> select() Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, upper, concat, lit

# Sample data
data = [("Alice", 25, 50000), ("Bob", 30, 60000), ("Charlie", 35, 75000)]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# 1. Simple column selection
df.select("name", "age").show()
# Or with col()
df.select(col("name"), col("age")).show()

# 2. Select with transformation
df.select(
    col("name"),
    (col("salary") * 1.1).alias("new_salary")
).show()

# 3. Multiple operations
df.select(
    upper(col("name")).alias("upper_name"),
    (col("age") + 5).alias("age_plus_5"),
    (col("salary") / 12).alias("monthly_salary")
).show()

# 4. Complex transformations
df.select(
    col("name"),
    concat(col("name"), lit(" - "), col("age").cast("string")).alias("name_age")
).show()

# 5. All columns plus new one
df.select("*", (col("salary") * 0.1).alias("bonus")).show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> selectExpr() Examples - Same Operations</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Same operations using selectExpr() - SQL syntax

# 1. Simple column selection
df.selectExpr("name", "age").show()

# 2. Select with transformation
df.selectExpr(
    "name",
    "salary * 1.1 AS new_salary"
).show()

# 3. Multiple operations - Much cleaner!
df.selectExpr(
    "UPPER(name) AS upper_name",
    "age + 5 AS age_plus_5",
    "salary / 12 AS monthly_salary"
).show()

# 4. Complex transformations - Easier to read
df.selectExpr(
    "name",
    "CONCAT(name, ' - ', CAST(age AS STRING)) AS name_age"
).show()

# 5. All columns plus new one
df.selectExpr("*", "salary * 0.1 AS bonus").show()

# 6. Complex SQL expressions
df.selectExpr(
    "name",
    "CASE WHEN salary > 60000 THEN 'High' ELSE 'Normal' END AS salary_category",
    "salary * 0.15 AS tax",
    "ROUND(salary / 12, 2) AS monthly_salary"
).show()

# 7. Aggregations (works like SQL)
df.selectExpr(
    "COUNT(*) AS total_count",
    "AVG(salary) AS avg_salary",
    "MAX(age) AS max_age"
).show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            When to Use Each
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-check-circle"></i> Use select() When:</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>Simple column selections</li>
                    <li>Need type safety</li>
                    <li>Building programmatically</li>
                    <li>Using complex PySpark functions</li>
                    <li>Want IDE autocomplete</li>
                    <li>Working with column objects</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># Good for programmatic logic
cols = ["name", "age"]
df.select(*cols)

# Good for complex functions
from pyspark.sql.functions import *
df.select(
    col("name"),
    when(col("age") > 30, "Senior")
      .otherwise("Junior")
)</code></pre>
                </div>
            </div>

            <div class="comparison-card narrow">
                <h4><i class="fas fa-check-circle"></i> Use selectExpr() When:</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>Complex SQL expressions</li>
                    <li>CASE WHEN statements</li>
                    <li>SQL users on team</li>
                    <li>Quick transformations</li>
                    <li>Arithmetic operations</li>
                    <li>Cleaner code for expressions</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># Perfect for SQL expressions
df.selectExpr(
    "*",
    "CASE WHEN age > 30 THEN 'Senior' ELSE 'Junior' END AS level"
)

# Clean arithmetic
df.selectExpr(
    "salary * 1.1 AS new_salary",
    "salary * 0.15 AS tax"
)</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When would you use selectExpr()?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>selectExpr()</code> when: <strong>1) SQL expressions:</strong> Complex CASE WHEN, calculations, string operations. <strong>2) SQL expertise:</strong> Team knows SQL better than DataFrame API. <strong>3) Cleaner code:</strong> SQL syntax is more readable for the operation. <strong>4) Quick transformations:</strong> Faster to write than importing multiple functions.<br><br>
                    Example: <code>df.selectExpr("CASE WHEN age > 30 THEN 'Senior' ELSE 'Junior' END AS category")</code> is cleaner than the equivalent <code>when().otherwise()</code> chain.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you use SQL-like syntax for selection?</strong></span>
                </div>
                <div class="cross-answer">
                    Yes! <code>selectExpr()</code> accepts SQL expressions as strings. You can use any valid SQL syntax including CASE WHEN, CAST, CONCAT, mathematical operations, aggregate functions, etc. Example: <code>df.selectExpr("name", "salary * 0.9 AS discounted", "UPPER(name) AS upper_name")</code>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 39: Selecting Specific Columns -->
<div class="question-content" id="q39">
    <div class="question-header">
        <h1 class="question-title">How do you select specific columns from a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Column Selection Methods
        </div>
        <div class="definition-box">
            <p><strong>Column selection</strong> is done using the <code>select()</code> method. There are multiple ways to specify columns.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> select()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Column Names</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> col()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Projection</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Multiple Ways to Select Columns
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-list"></i> Column Selection Methods</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

# Sample DataFrame
data = [("Alice", 25, 50000, "Engineering"),
        ("Bob", 30, 60000, "Marketing"),
        ("Charlie", 35, 75000, "Sales")]
df = spark.createDataFrame(data, ["name", "age", "salary", "department"])

# Method 1: Column names as strings
df.select("name", "age").show()
# +-------+---+
# |   name|age|
# +-------+---+
# |  Alice| 25|
# |    Bob| 30|
# |Charlie| 35|
# +-------+---+

# Method 2: Using col() function
df.select(col("name"), col("salary")).show()

# Method 3: Using DataFrame column syntax
df.select(df.name, df.age).show()
df.select(df["name"], df["age"]).show()  # Bracket notation

# Method 4: Select all columns
df.select("*").show()

# Method 5: Select columns from a list
columns = ["name", "age", "salary"]
df.select(*columns).show()  # Unpack list with *

# Method 6: Select with expressions
df.select(
    col("name"),
    (col("salary") * 1.1).alias("salary_with_bonus")
).show()

# Method 7: Select and rename
df.select(
    col("name").alias("employee_name"),
    col("age").alias("employee_age")
).show()

# Method 8: Select all except some columns
# Drop unwanted columns
df.drop("department").show()  # All columns except department

# Or select explicitly
all_cols = df.columns
cols_to_select = [c for c in all_cols if c != "department"]
df.select(*cols_to_select).show()

# Method 9: Select columns matching pattern
import re
# Select columns starting with 's'
s_cols = [c for c in df.columns if c.startswith('s')]
df.select(*s_cols).show()  # Selects 'salary' only

# Method 10: selectExpr for SQL-like selection
df.selectExpr("name", "age", "salary * 0.1 AS bonus").show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-tools"></i> Advanced Column Selection</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Dynamic column selection
def select_numeric_columns(df):
    """Select only numeric columns"""
    numeric_cols = [field.name for field in df.schema.fields 
                    if str(field.dataType) in ['IntegerType', 'LongType', 'DoubleType', 'FloatType']]
    return df.select(*numeric_cols)

numeric_df = select_numeric_columns(df)
numeric_df.show()
# +---+------+
# |age|salary|
# +---+------+
# | 25| 50000|
# | 30| 60000|
# +---+------+

# Select columns by type
from pyspark.sql.types import StringType, IntegerType

string_cols = [field.name for field in df.schema.fields 
               if isinstance(field.dataType, StringType)]
df.select(*string_cols).show()

# Select with conditional logic
df.select(
    "name",
    col("salary"),
    when(col("age") > 30, "Senior").otherwise("Junior").alias("level")
).show()

# Select nested columns (if DataFrame has struct types)
# df.select("address.city", "address.zip")

# Select and cast
df.select(
    "name",
    col("salary").cast("double").alias("salary_double")
).show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you use SQL-like syntax for selection?</strong></span>
                </div>
                <div class="cross-answer">
                    Yes! Use <code>selectExpr()</code> for SQL syntax: <code>df.selectExpr("name", "age * 2 AS double_age", "UPPER(department) AS dept")</code>. This allows you to write SQL expressions directly as strings, making it easier for SQL users.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between select() and selectExpr()?</strong></span>
                </div>
                <div class="cross-answer">
                    <code>select()</code> uses column objects and DataFrame API functions. <code>selectExpr()</code> uses SQL expression strings. selectExpr is more flexible for complex SQL logic like CASE WHEN. See Question 38 for detailed comparison.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 40: Filtering in PySpark -->
<div class="question-content" id="q40">
    <div class="question-header">
        <h1 class="question-title">How do you perform filtering in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Filtering</strong> in PySpark is done using <code>filter()</code> or <code>where()</code> methods. Both are identical - use whichever you prefer. They return rows that match the specified condition.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> filter()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> where()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Conditions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Boolean Logic</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Filtering Methods
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-filter"></i> Basic Filtering</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

# Sample data
data = [("Alice", 25, 50000, "Engineering"),
        ("Bob", 30, 60000, "Marketing"),
        ("Charlie", 35, 75000, "Sales"),
        ("David", 28, 55000, "Engineering")]
df = spark.createDataFrame(data, ["name", "age", "salary", "department"])

# Method 1: filter() with SQL string
df.filter("age > 30").show()
# +-------+---+------+----------+
# |   name|age|salary|department|
# +-------+---+------+----------+
# |Charlie| 35| 75000|     Sales|
# +-------+---+------+----------+

# Method 2: where() with SQL string (identical to filter)
df.where("salary >= 60000").show()
# +-------+---+------+----------+
# |   name|age|salary|department|
# +-------+---+------+----------+
# |    Bob| 30| 60000| Marketing|
# |Charlie| 35| 75000|     Sales|
# +-------+---+------+----------+

# Method 3: filter() with Column object
df.filter(col("age") > 30).show()

# Method 4: filter() with DataFrame column
df.filter(df.age > 30).show()
df.filter(df["age"] > 30).show()

# Method 5: Multiple conditions with & (AND)
df.filter((col("age") > 25) & (col("salary") > 55000)).show()
# +-------+---+------+----------+
# |   name|age|salary|department|
# +-------+---+------+----------+
# |    Bob| 30| 60000| Marketing|
# |Charlie| 35| 75000|     Sales|
# +-------+---+------+----------+

# Method 6: Multiple conditions with | (OR)
df.filter((col("age") < 26) | (col("salary") > 70000)).show()
# +-------+---+------+-----------+
# |   name|age|salary| department|
# +-------+---+------+-----------+
# |  Alice| 25| 50000|Engineering|
# |Charlie| 35| 75000|      Sales|
# +-------+---+------+-----------+

# Method 7: NOT condition with ~
df.filter(~(col("department") == "Engineering")).show()
# +-------+---+------+----------+
# |   name|age|salary|department|
# +-------+---+------+----------+
# |    Bob| 30| 60000| Marketing|
# |Charlie| 35| 75000|     Sales|
# +-------+---+------+----------+

# Method 8: Using isin() for multiple values
df.filter(col("department").isin("Engineering", "Sales")).show()

# Method 9: String operations
df.filter(col("name").startswith("A")).show()
df.filter(col("name").contains("har")).show()

# Method 10: NULL filtering
df.filter(col("name").isNotNull()).show()
df.filter(col("age").isNull()).show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-layer-group"></i> Advanced Filtering</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, length, upper, lower

# Complex AND conditions
df.filter(
    (col("age") > 25) & 
    (col("salary") > 50000) & 
    (col("department") == "Engineering")
).show()

# Complex OR conditions  
df.filter(
    (col("age") > 35) | 
    (col("salary") < 55000) | 
    (col("department") == "Marketing")
).show()

# Combining AND and OR (use parentheses!)
df.filter(
    ((col("age") > 30) & (col("salary") > 60000)) | 
    (col("department") == "Engineering")
).show()

# Range filtering (BETWEEN)
df.filter(col("age").between(25, 30)).show()
# Same as:
df.filter((col("age") >= 25) & (col("age") <= 30)).show()

# Pattern matching with LIKE
df.filter(col("name").like("A%")).show()  # Starts with A
df.filter(col("name").like("%e%")).show()  # Contains e

# Regular expressions
df.filter(col("name").rlike("^[AC]")).show()  # Starts with A or C

# Filter based on string length
df.filter(length(col("name")) > 5).show()

# Case-insensitive filtering
df.filter(lower(col("department")) == "engineering").show()

# Filter with multiple values NOT in list
df.filter(~col("department").isin("Sales", "Marketing")).show()

# Chaining multiple filters (executed sequentially)
df.filter(col("age") > 25) \
  .filter(col("salary") > 50000) \
  .filter(col("department") != "Sales") \
  .show()

# Using SQL expressions with filter
df.filter("age > 25 AND salary > 50000 AND department = 'Engineering'").show()

# Filter with substrings
df.filter(col("name").substr(1, 1) == "A").show()  # First letter is A</code></pre>
        </div>

        <div class="warning-box">
            <strong> Important:</strong> When combining conditions, use <code>&</code> (AND), <code>|</code> (OR), <code>~</code> (NOT). Always use parentheses around each condition: <code>(col("age") > 30) & (col("salary") > 50000)</code>. Do NOT use Python's <code>and</code>, <code>or</code>, <code>not</code> - they won't work!
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the difference between filter() and where()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>No difference!</strong> <code>filter()</code> and <code>where()</code> are aliases - they are 100% identical. Use whichever you prefer. <code>filter()</code> is more "DataFrame-like", while <code>where()</code> is more "SQL-like". Example: <code>df.filter("age > 30")</code> is exactly the same as <code>df.where("age > 30")</code>.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How do you apply multiple conditions?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <strong>&</strong> for AND, <strong>|</strong> for OR, <strong>~</strong> for NOT. Always wrap each condition in parentheses:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># AND: Both conditions must be true
df.filter((col("age") > 30) & (col("salary") > 60000))

# OR: At least one condition must be true
df.filter((col("age") > 35) | (col("department") == "Sales"))

# NOT: Negate condition
df.filter(~(col("department") == "Engineering"))

# Complex: Combine multiple
df.filter(
    ((col("age") > 30) & (col("salary") > 50000)) |
    (col("department") == "Engineering")
)</code></pre>
                    <strong>DON'T use:</strong> <code>and</code>, <code>or</code>, <code>not</code> (Python keywords - won't work with Spark!)
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 41: Filter DataFrame Rows -->
<div class="question-content" id="q41">
    <div class="question-header">
        <h1 class="question-title">How do you filter rows in a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Row Filtering
        </div>
        <div class="definition-box">
            <p><strong>Row filtering</strong> selects rows that satisfy a condition. Use <code>filter()</code> or <code>where()</code> - they are identical.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Common Filtering Patterns
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-filter"></i> DataFrame Row Filtering</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

data = [("Alice", 25, 50000), ("Bob", 30, 60000), ("Charlie", 35, 75000)]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# Basic filtering
filtered = df.filter(col("age") > 30)
filtered.show()
# +-------+---+------+
# |   name|age|salary|
# +-------+---+------+
# |Charlie| 35| 75000|
# +-------+---+------+

# Multiple conditions
high_earners = df.filter((col("age") > 25) & (col("salary") >= 60000))
high_earners.show()

# String matching
df.filter(col("name").startswith("A")).show()

# IN clause
df.filter(col("name").isin("Alice", "Charlie")).show()

# NULL handling
df.filter(col("salary").isNotNull()).show()

# Chaining filters
result = df.filter(col("age") > 25) \
           .filter(col("salary") > 50000) \
           .filter(col("name") != "Bob")
result.show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between filter() and where()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>No difference!</strong> They are aliases and completely interchangeable. Both filter rows based on conditions. Use whichever you prefer: <code>df.filter(condition)</code> or <code>df.where(condition)</code>. The behavior and performance are identical.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 42: Filter RDD -->
<div class="question-content" id="q42">
    <div class="question-header">
        <h1 class="question-title">How do you filter an RDD?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            RDD Filtering
        </div>
        <div class="definition-box">
            <p><strong>RDD filtering</strong> uses the <code>filter()</code> transformation with a lambda function that returns True/False. Elements returning True are kept.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> RDD</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Lambda Functions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> filter()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Narrow Transformation</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            RDD Filter Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-cogs"></i> Filtering RDDs</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark import SparkContext

sc = SparkContext("local", "RDD Filter")

# Example 1: Filter numbers
numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Keep only even numbers
evens = numbers.filter(lambda x: x % 2 == 0)
print(evens.collect())  # [2, 4, 6, 8, 10]

# Keep numbers greater than 5
greater_than_5 = numbers.filter(lambda x: x > 5)
print(greater_than_5.collect())  # [6, 7, 8, 9, 10]

# Example 2: Filter strings
words = sc.parallelize(["hello", "world", "spark", "python", "big", "data"])

# Keep words longer than 4 characters
long_words = words.filter(lambda word: len(word) > 4)
print(long_words.collect())  # ['hello', 'world', 'spark', 'python']

# Keep words starting with 's'
s_words = words.filter(lambda word: word.startswith('s'))
print(s_words.collect())  # ['spark']

# Example 3: Filter tuples (key-value pairs)
pairs = sc.parallelize([
    ("Alice", 25),
    ("Bob", 30),
    ("Charlie", 35),
    ("David", 28)
])

# Keep pairs where age > 30
older = pairs.filter(lambda pair: pair[1] > 30)
print(older.collect())  # [('Charlie', 35)]

# Keep pairs where name starts with 'A' or 'B'
ab_names = pairs.filter(lambda pair: pair[0][0] in ['A', 'B'])
print(ab_names.collect())  # [('Alice', 25), ('Bob', 30)]

# Example 4: Multiple conditions
# Keep even numbers between 3 and 8
filtered = numbers.filter(lambda x: x % 2 == 0 and 3 <= x <= 8)
print(filtered.collect())  # [4, 6, 8]

# Example 5: Complex filtering with functions
def is_valid_user(user):
    name, age, salary = user
    return age >= 25 and salary > 50000

users = sc.parallelize([
    ("Alice", 25, 50000),
    ("Bob", 30, 60000),
    ("Charlie", 22, 45000),
    ("David", 28, 55000)
])

valid_users = users.filter(is_valid_user)
print(valid_users.collect())
# [('Bob', 30, 60000), ('David', 28, 55000)]

# Example 6: Filter with external data
valid_departments = {"Engineering", "Sales"}

employees = sc.parallelize([
    ("Alice", "Engineering"),
    ("Bob", "Marketing"),
    ("Charlie", "Sales")
])

filtered_employees = employees.filter(lambda x: x[1] in valid_departments)
print(filtered_employees.collect())
# [('Alice', 'Engineering'), ('Charlie', 'Sales')]

# Example 7: Chaining filters
result = numbers.filter(lambda x: x > 3) \
                .filter(lambda x: x < 8) \
                .filter(lambda x: x % 2 == 0)
print(result.collect())  # [4, 6]</code></pre>
        </div>

        <div class="comparison-grid" style="margin-top: 1.5rem;">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-cogs"></i> RDD filter()</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>Uses lambda functions</li>
                    <li>Works on any data type</li>
                    <li>Returns boolean</li>
                    <li>No column names</li>
                    <li>More flexible</li>
                </ul>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>rdd.filter(lambda x: x > 5)</code></pre>
            </div>

            <div class="comparison-card narrow">
                <h4><i class="fas fa-table"></i> DataFrame filter()</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>Uses column expressions</li>
                    <li>Structured data only</li>
                    <li>SQL-like syntax</li>
                    <li>Named columns</li>
                    <li>Better optimization</li>
                </ul>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.filter(col("age") > 5)</code></pre>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 43: Adding New Column -->
<div class="question-content" id="q43">
    <div class="question-header">
        <h1 class="question-title">How to add a new column?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Adding Columns
        </div>
        <div class="definition-box">
            <p><strong>Adding columns</strong> is done using <code>withColumn()</code> method. It returns a new DataFrame with the added column (DataFrames are immutable).</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> withColumn()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Column Creation</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Immutability</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Transformations</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Adding Columns with withColumn()
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-plus-circle"></i> withColumn() Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, lit, when, concat, upper, current_timestamp

# Sample data
data = [("Alice", 25, 50000), ("Bob", 30, 60000), ("Charlie", 35, 75000)]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# 1. Add literal value column
df_with_country = df.withColumn("country", lit("USA"))
df_with_country.show()
# +-------+---+------+-------+
# |   name|age|salary|country|
# +-------+---+------+-------+
# |  Alice| 25| 50000|    USA|
# |    Bob| 30| 60000|    USA|
# |Charlie| 35| 75000|    USA|
# +-------+---+------+-------+

# 2. Add calculated column
df_with_bonus = df.withColumn("bonus", col("salary") * 0.1)
df_with_bonus.show()
# +-------+---+------+------+
# |   name|age|salary| bonus|
# +-------+---+------+------+
# |  Alice| 25| 50000|5000.0|
# |    Bob| 30| 60000|6000.0|
# |Charlie| 35| 75000|7500.0|
# +-------+---+------+------+

# 3. Add column based on existing columns
df_with_monthly = df.withColumn("monthly_salary", col("salary") / 12)

# 4. Add multiple columns (chain withColumn)
df_enhanced = df.withColumn("bonus", col("salary") * 0.1) \
                .withColumn("total", col("salary") + col("bonus")) \
                .withColumn("tax", col("total") * 0.15)
df_enhanced.show()

# 5. Add conditional column
df_with_category = df.withColumn(
    "salary_category",
    when(col("salary") > 60000, "High")
    .when(col("salary") > 50000, "Medium")
    .otherwise("Low")
)
df_with_category.show()
# +-------+---+------+---------------+
# |   name|age|salary|salary_category|
# +-------+---+------+---------------+
# |  Alice| 25| 50000|            Low|
# |    Bob| 30| 60000|         Medium|
# |Charlie| 35| 75000|           High|
# +-------+---+------+---------------+

# 6. Add column with string operations
df_with_upper = df.withColumn("name_upper", upper(col("name")))

# 7. Add column by combining columns
df_with_concat = df.withColumn(
    "name_age",
    concat(col("name"), lit(" - "), col("age").cast("string"))
)
df_with_concat.show(truncate=False)
# +-------+---+------+-----------+
# |name   |age|salary|name_age   |
# +-------+---+------+-----------+
# |Alice  |25 |50000 |Alice - 25 |
# |Bob    |30 |60000 |Bob - 30   |
# |Charlie|35 |75000 |Charlie - 35|
# +-------+---+------+-----------+

# 8. Add timestamp column
df_with_ts = df.withColumn("created_at", current_timestamp())

# 9. Add column with UDF (User-Defined Function)
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

@udf(returnType=StringType())
def categorize_age(age):
    if age < 30:
        return "Young"
    elif age < 40:
        return "Middle"
    else:
        return "Senior"

df_with_age_cat = df.withColumn("age_category", categorize_age(col("age")))
df_with_age_cat.show()

# 10. Add incrementing ID column
from pyspark.sql.functions import monotonically_increasing_id, row_number
from pyspark.sql.window import Window

df_with_id = df.withColumn("id", monotonically_increasing_id())

# Or sequential ID starting from 1
df_with_seq_id = df.withColumn("row_num", row_number().over(Window.orderBy("name")))</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between withColumn() and selectExpr() for column creation?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>withColumn():</strong> Adds/modifies ONE column while keeping all other columns. Returns DataFrame with all original columns + new column.<br><br>
                    <strong>selectExpr():</strong> Projects specified columns only. Must explicitly list columns you want to keep.<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># withColumn - keeps all columns, adds new one
df.withColumn("bonus", col("salary") * 0.1)
# Result: name, age, salary, bonus

# selectExpr - only selected columns
df.selectExpr("name", "age", "salary", "salary * 0.1 AS bonus")
# Result: name, age, salary, bonus (explicit selection)

# selectExpr without original columns
df.selectExpr("salary * 0.1 AS bonus")
# Result: bonus (only new column!)</code></pre>
                    <strong>Use withColumn()</strong> when you want to keep all existing columns.<br>
                    <strong>Use selectExpr()</strong> when you want to select specific columns with transformations.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 44: Create New Column -->
<div class="question-content" id="q44">
    <div class="question-header">
        <h1 class="question-title">How do you create a new column in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Column Creation Methods
        </div>
        <div class="definition-box">
            <p><strong>Creating columns</strong> can be done using <code>withColumn()</code>, <code>select()</code>, or <code>selectExpr()</code>. withColumn() is most common for adding columns.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Column Creation Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-plus"></i> Creating Columns</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, lit

data = [("Alice", 25, 50000), ("Bob", 30, 60000)]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# Method 1: withColumn() - most common
df1 = df.withColumn("bonus", col("salary") * 0.1)

# Method 2: select() with alias
df2 = df.select("*", (col("salary") * 0.1).alias("bonus"))

# Method 3: selectExpr() - SQL syntax
df3 = df.selectExpr("*", "salary * 0.1 AS bonus")

# All three methods produce the same result
df1.show()
df2.show()
df3.show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What is the use of withColumn()?</strong></span>
                </div>
                <div class="cross-answer">
                    <code>withColumn()</code> adds a new column or replaces an existing column in a DataFrame. It keeps all other columns intact. Returns a new DataFrame (immutable). Syntax: <code>df.withColumn("new_col", expression)</code>. Use it to add calculated fields, transformations, or literal values.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you perform arithmetic operations between columns?</strong></span>
                </div>
                <div class="cross-answer">
                    Yes! You can perform arithmetic operations (+, -, *, /, %) between columns:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Addition
df.withColumn("total", col("salary") + col("bonus"))

# Subtraction
df.withColumn("net", col("salary") - col("tax"))

# Multiplication
df.withColumn("annual", col("monthly_salary") * 12)

# Division
df.withColumn("daily", col("salary") / 365)

# Modulo
df.withColumn("remainder", col("amount") % 10)

# Complex expression
df.withColumn(
    "final_amount",
    (col("salary") * 1.1) - (col("salary") * 0.15)
)</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 45: Conditional Column Operations -->
<div class="question-content" id="q45">
    <div class="question-header">
        <h1 class="question-title">How to add or modify a column based on condition?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Conditional Column Operations
        </div>
        <div class="definition-box">
            <p><strong>Conditional columns</strong> are created using <code>when().otherwise()</code> function (similar to SQL CASE WHEN). This allows different values based on conditions.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> when()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> otherwise()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> CASE WHEN</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Conditional Logic</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Conditional Column Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code-branch"></i> Using when().otherwise()</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import when, col

data = [("Alice", 25, 50000), ("Bob", 30, 60000), ("Charlie", 35, 75000), ("David", 28, 55000)]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# 1. Simple if-else (two conditions)
df_simple = df.withColumn(
    "age_group",
    when(col("age") >= 30, "Senior")
    .otherwise("Junior")
)
df_simple.show()
# +-------+---+------+---------+
# |   name|age|salary|age_group|
# +-------+---+------+---------+
# |  Alice| 25| 50000|   Junior|
# |    Bob| 30| 60000|   Senior|
# |Charlie| 35| 75000|   Senior|
# |  David| 28| 55000|   Junior|
# +-------+---+------+---------+

# 2. Multiple conditions (if-elif-else)
df_multi = df.withColumn(
    "salary_band",
    when(col("salary") >= 70000, "High")
    .when(col("salary") >= 55000, "Medium")
    .otherwise("Low")
)
df_multi.show()
# +-------+---+------+-----------+
# |   name|age|salary|salary_band|
# +-------+---+------+-----------+
# |  Alice| 25| 50000|        Low|
# |    Bob| 30| 60000|     Medium|
# |Charlie| 35| 75000|       High|
# |  David| 28| 55000|     Medium|
# +-------+---+------+-----------+

# 3. Multiple column conditions (AND)
df_and = df.withColumn(
    "status",
    when((col("age") > 30) & (col("salary") > 60000), "Senior High Earner")
    .when((col("age") > 30) & (col("salary") <= 60000), "Senior Normal")
    .when((col("age") <= 30) & (col("salary") > 60000), "Junior High Earner")
    .otherwise("Junior Normal")
)
df_and.show(truncate=False)

# 4. Nested conditions
df_nested = df.withColumn(
    "category",
    when(col("age") > 30,
        when(col("salary") > 70000, "Senior-High")
        .otherwise("Senior-Normal")
    ).otherwise(
        when(col("salary") > 55000, "Junior-High")
        .otherwise("Junior-Normal")
    )
)
df_nested.show()

# 5. Modify existing column conditionally
df_modified = df.withColumn(
    "salary",
    when(col("age") > 30, col("salary") * 1.1)  # 10% raise for seniors
    .otherwise(col("salary"))
)
df_modified.show()
# +-------+---+-------+
# |   name|age| salary|
# +-------+---+-------+
# |  Alice| 25|50000.0|
# |    Bob| 30|60000.0|
# |Charlie| 35|82500.0|   Modified!
# |  David| 28|55000.0|
# +-------+---+-------+

# 6. Using selectExpr with CASE WHEN (SQL style)
df_sql = df.selectExpr(
    "*",
    """
    CASE 
        WHEN age > 30 AND salary > 60000 THEN 'A'
        WHEN age > 30 THEN 'B'
        WHEN salary > 55000 THEN 'C'
        ELSE 'D'
    END AS grade
    """
)
df_sql.show()

# 7. Conditional column with NULL handling
from pyspark.sql.functions import lit

df_with_null = df.withColumn("bonus", lit(None).cast("integer"))

df_safe = df_with_null.withColumn(
    "final_bonus",
    when(col("bonus").isNull(), col("salary") * 0.05)  # Default bonus if null
    .otherwise(col("bonus"))
)

# 8. Multiple columns with same condition
df_bulk = df.withColumn("is_senior", col("age") > 30) \
            .withColumn("bonus", 
                when(col("is_senior"), col("salary") * 0.15)
                .otherwise(col("salary") * 0.10)
            ) \
            .withColumn("tax_rate",
                when(col("is_senior"), 0.25)
                .otherwise(0.20)
            )
df_bulk.show()

# 9. Complex business logic
df_complex = df.withColumn(
    "bonus",
    when(
        (col("age") > 30) & (col("salary") < 60000),
        col("salary") * 0.20  # High bonus for senior underpaid
    ).when(
        (col("age") > 30) & (col("salary") >= 60000),
        col("salary") * 0.15  # Normal senior bonus
    ).when(
        (col("age") <= 30) & (col("salary") > 55000),
        col("salary") * 0.12  # Good junior performance
    ).otherwise(
        col("salary") * 0.08  # Standard junior bonus
    )
)
df_complex.show()</code></pre>
        </div>

        <div class="highlight-box">
            <strong> Best Practice:</strong> For complex conditions, consider using <code>selectExpr()</code> with CASE WHEN for better readability, or extract logic into a UDF for very complex business rules.
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 46: Renaming Columns -->
<div class="question-content" id="q46">
    <div class="question-header">
        <h1 class="question-title">How to rename a column in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Column Renaming
        </div>
        <div class="definition-box">
            <p><strong>Renaming columns</strong> can be done using <code>withColumnRenamed()</code>, <code>alias()</code>, or <code>toDF()</code>. Choose based on whether you're renaming one column, multiple columns, or all columns.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> withColumnRenamed()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> alias()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> toDF()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Column Names</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Renaming Methods
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-edit"></i> Column Renaming Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

data = [("Alice", 25, 50000), ("Bob", 30, 60000)]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# Method 1: withColumnRenamed() - Rename single column
df_renamed1 = df.withColumnRenamed("name", "employee_name")
df_renamed1.show()
# +-------------+---+------+
# |employee_name|age|salary|
# +-------------+---+------+
# |        Alice| 25| 50000|
# +-------------+---+------+

# Method 2: Chain withColumnRenamed() for multiple columns
df_renamed2 = df.withColumnRenamed("name", "employee_name") \
                .withColumnRenamed("age", "employee_age") \
                .withColumnRenamed("salary", "employee_salary")
df_renamed2.show()

# Method 3: Using select() with alias()
df_renamed3 = df.select(
    col("name").alias("employee_name"),
    col("age").alias("employee_age"),
    col("salary").alias("employee_salary")
)
df_renamed3.show()

# Method 4: Using selectExpr()
df_renamed4 = df.selectExpr(
    "name AS employee_name",
    "age AS employee_age",
    "salary AS employee_salary"
)
df_renamed4.show()

# Method 5: toDF() - Rename all columns at once
df_renamed5 = df.toDF("employee_name", "employee_age", "employee_salary")
df_renamed5.show()

# Method 6: Using dictionary for multiple renames (most efficient)
rename_dict = {
    "name": "employee_name",
    "age": "employee_age",
    "salary": "employee_salary"
}

# Apply all renames
df_renamed6 = df
for old_name, new_name in rename_dict.items():
    df_renamed6 = df_renamed6.withColumnRenamed(old_name, new_name)
df_renamed6.show()

# Method 7: Programmatic renaming (add prefix/suffix)
# Add prefix to all columns
df_with_prefix = df.select([col(c).alias(f"emp_{c}") for c in df.columns])
df_with_prefix.show()
# +--------+-------+----------+
# |emp_name|emp_age|emp_salary|
# +--------+-------+----------+

# Add suffix to all columns
df_with_suffix = df.select([col(c).alias(f"{c}_new") for c in df.columns])
df_with_suffix.show()

# Method 8: Replace spaces in column names
df_spaces = spark.createDataFrame([(1, 2, 3)], ["first name", "last name", "age value"])
df_clean = df_spaces.select([col(c).alias(c.replace(" ", "_")) for c in df_spaces.columns])
df_clean.show()
# +----------+---------+---------+
# |first_name|last_name|age_value|
# +----------+---------+---------+

# Method 9: Convert to lowercase
df_lower = df.select([col(c).alias(c.lower()) for c in df.columns])
df_lower.show()

# Method 10: Convert to uppercase
df_upper = df.select([col(c).alias(c.upper()) for c in df.columns])
df_upper.show()
# +-----+---+------+
# | NAME|AGE|SALARY|
# +-----+---+------+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between withColumnRenamed() and using alias()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>withColumnRenamed():</strong> Dedicated method for renaming. Keeps all columns, just changes name. Easy to use for single renames. Less efficient for multiple renames (need to chain).<br><br>
                    <strong>alias():</strong> Used with select(). More flexible - can combine renaming with transformations. Better for multiple renames. Can apply functions while renaming.<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># withColumnRenamed - simple rename
df.withColumnRenamed("name", "emp_name")

# alias - can transform while renaming
df.select(
    upper(col("name")).alias("emp_name"),  # Transform AND rename
    col("age").alias("emp_age")
)</code></pre>
                    <strong>Best Practice:</strong> Use <code>withColumnRenamed()</code> for simple renames. Use <code>alias()</code> when transforming or selecting specific columns.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 47: Rename Column Methods -->
<div class="question-content" id="q47">
    <div class="question-header">
        <h1 class="question-title">How do you rename a column?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Quick Reference
        </div>
        <div class="definition-box">
            <p>Three main methods: <code>withColumnRenamed()</code>, <code>alias()</code> with <code>select()</code>, and <code>toDF()</code> for all columns.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Quick Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-bolt"></i> Renaming Methods</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Method 1: withColumnRenamed()
df.withColumnRenamed("old_name", "new_name")

# Method 2: alias() with select()
df.select(col("old_name").alias("new_name"))

# Method 3: toDF() for all columns
df.toDF("new_name1", "new_name2", "new_name3")</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the use of alias() vs withColumnRenamed()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>withColumnRenamed():</strong> Renames existing column, keeps all other columns.<br>
                    <strong>alias():</strong> Used within select(), can transform and rename simultaneously.<br><br>
                    <code>withColumnRenamed()</code> = Simple renaming<br>
                    <code>alias()</code> = Renaming + selection/transformation<br><br>
                    See Question 46 for detailed comparison.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 48: Rename DataFrame Columns -->
<div class="question-content" id="q48">
    <div class="question-header">
        <h1 class="question-title">How to rename columns in a PySpark DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Summary
        </div>
        <div class="definition-box">
            <p>Use <code>withColumnRenamed()</code> for single renames, <code>toDF()</code> for renaming all columns, or <code>select()</code> with <code>alias()</code> for selective renaming with transformations.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Best Practices
        </div>

        <div class="highlight-box">
            <strong>Single Column:</strong> <code>df.withColumnRenamed("old", "new")</code>
        </div>

        <div class="highlight-box">
            <strong>Multiple Columns:</strong> Chain <code>withColumnRenamed()</code> or use dictionary with loop
        </div>

        <div class="highlight-box">
            <strong>All Columns:</strong> <code>df.toDF("name1", "name2", "name3")</code>
        </div>

        <div class="highlight-box">
            <strong>With Transformation:</strong> <code>df.select(col("old").alias("new"))</code>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 49: Casting Column Data Types -->
<div class="question-content" id="q49">
    <div class="question-header">
        <h1 class="question-title">How do you cast column data types in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Type Casting
        </div>
        <div class="definition-box">
            <p><strong>Type casting</strong> converts column data types using the <code>cast()</code> method. Common when reading data without schema inference or when types need to change.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> cast()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Types</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Type Conversion</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Schema</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Casting Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-exchange-alt"></i> Type Casting Methods</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType, StringType, DoubleType, DateType, TimestampType

# Sample data - all strings
data = [("1", "25", "50000.50", "2024-01-15"),
        ("2", "30", "60000.75", "2024-01-16")]
df = spark.createDataFrame(data, ["id", "age", "salary", "date"])

df.printSchema()
# root
#  |-- id: string
#  |-- age: string
#  |-- salary: string
#  |-- date: string

# Method 1: cast() with type string
df_casted1 = df.withColumn("age", col("age").cast("int")) \
               .withColumn("salary", col("salary").cast("double"))

df_casted1.printSchema()
# root
#  |-- id: string
#  |-- age: integer       Converted!
#  |-- salary: double     Converted!
#  |-- date: string

# Method 2: cast() with type objects (more explicit)
df_casted2 = df.withColumn("age", col("age").cast(IntegerType())) \
               .withColumn("salary", col("salary").cast(DoubleType())) \
               .withColumn("date", col("date").cast(DateType()))

df_casted2.printSchema()
# root
#  |-- id: string
#  |-- age: integer
#  |-- salary: double
#  |-- date: date         Converted to date!

# Method 3: Cast multiple columns at once
cast_dict = {
    "age": "int",
    "salary": "double",
    "id": "int"
}

df_casted3 = df
for col_name, dtype in cast_dict.items():
    df_casted3 = df_casted3.withColumn(col_name, col(col_name).cast(dtype))

df_casted3.printSchema()

# Method 4: Using selectExpr() for casting
df_casted4 = df.selectExpr(
    "CAST(id AS INT) AS id",
    "CAST(age AS INT) AS age",
    "CAST(salary AS DOUBLE) AS salary",
    "CAST(date AS DATE) AS date"
)

df_casted4.printSchema()

# Common type conversions:

# String to Integer
df.withColumn("age_int", col("age").cast("int"))

# String to Double/Float
df.withColumn("salary_double", col("salary").cast("double"))
df.withColumn("salary_float", col("salary").cast("float"))

# String to Date
df.withColumn("date_converted", col("date").cast("date"))

# String to Timestamp
df.withColumn("timestamp", col("date").cast("timestamp"))

# Integer to String
df.withColumn("age_str", col("age").cast("string"))

# Double to Integer (truncates decimal)
df.withColumn("salary_int", col("salary").cast("int"))

# Boolean conversion
df_bool = spark.createDataFrame([("true",), ("false",), ("1",), ("0",)], ["value"])
df_bool.withColumn("bool", col("value").cast("boolean")).show()
# +-----+-----+
# |value| bool|
# +-----+-----+
# | true| true|
# |false|false|
# |    1| true|
# |    0|false|
# +-----+-----+

# Example: Clean and cast CSV data
csv_df = spark.read.csv("data.csv", header=True)  # All columns are strings
csv_clean = csv_df.withColumn("id", col("id").cast("int")) \
                  .withColumn("age", col("age").cast("int")) \
                  .withColumn("salary", col("salary").cast("double")) \
                  .withColumn("hire_date", col("hire_date").cast("date"))

# Handle failed casts (become null)
df_with_nulls = df.withColumn("age_int", col("age").cast("int"))
# If age contains "abc", it becomes null after cast

# Check for casting errors
from pyspark.sql.functions import isnan, isnull, when, count

df_casted = df.withColumn("salary_double", col("salary").cast("double"))
df_casted.select(
    count(when(col("salary_double").isNull(), True)).alias("null_count")
).show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-list"></i> Common Data Types</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Available data types for casting:

# Numeric Types:
"byte", "short", "int", "integer", "long", "bigint"
"float", "double", "decimal"

# String Types:
"string"

# Boolean:
"boolean"

# Date/Time Types:
"date"           # YYYY-MM-DD
"timestamp"      # YYYY-MM-DD HH:MM:SS

# Binary:
"binary"

# Complex Types:
"array<type>", "map<key_type, value_type>", "struct<...>"

# Example usage:
df.withColumn("col", col("col").cast("int"))
df.withColumn("col", col("col").cast("double"))
df.withColumn("col", col("col").cast("string"))
df.withColumn("col", col("col").cast("date"))
df.withColumn("col", col("col").cast("timestamp"))
df.withColumn("col", col("col").cast("boolean"))</code></pre>
        </div>

        <div class="warning-box">
            <strong> Important:</strong> Failed casts result in <code>null</code> values. Always validate data before casting or check for nulls after casting. Example: casting "abc" to integer returns null, not an error.
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 50: Dropping Columns -->
<div class="question-content" id="q50">
    <div class="question-header">
        <h1 class="question-title">How do you drop a column?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Dropping Columns
        </div>
        <div class="definition-box">
            <p><strong>Dropping columns</strong> removes one or more columns from a DataFrame using the <code>drop()</code> method. Returns a new DataFrame without the specified columns.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> drop()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Column Removal</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> DataFrame Schema</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Dropping Column Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-trash-alt"></i> drop() Methods</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

data = [("Alice", 25, 50000, "Engineering", "USA"),
        ("Bob", 30, 60000, "Marketing", "UK")]
df = spark.createDataFrame(data, ["name", "age", "salary", "department", "country"])

# Method 1: Drop single column by name
df_drop1 = df.drop("country")
df_drop1.show()
# +-----+---+------+-----------+
# | name|age|salary| department|
# +-----+---+------+-----------+
# |Alice| 25| 50000|Engineering|
# |  Bob| 30| 60000|  Marketing|
# +-----+---+------+-----------+

# Method 2: Drop multiple columns
df_drop2 = df.drop("country", "department")
df_drop2.show()
# +-----+---+------+
# | name|age|salary|
# +-----+---+------+
# |Alice| 25| 50000|
# |  Bob| 30| 60000|
# +-----+---+------+

# Method 3: Drop using column object
df_drop3 = df.drop(col("country"))

# Method 4: Drop from a list
cols_to_drop = ["country", "department"]
df_drop4 = df.drop(*cols_to_drop)  # Unpack list with *

# Method 5: Chain multiple drops
df_drop5 = df.drop("country").drop("department")

# Method 6: Drop columns conditionally (programmatic)
# Drop columns with specific pattern
cols_to_remove = [c for c in df.columns if c.startswith("c")]
df_drop6 = df.drop(*cols_to_remove)  # Drops 'country'

# Method 7: Keep only specific columns (opposite of drop)
cols_to_keep = ["name", "salary"]
df_keep = df.select(*cols_to_keep)
df_keep.show()
# +-----+------+
# | name|salary|
# +-----+------+
# |Alice| 50000|
# |  Bob| 60000|
# +-----+------+

# Method 8: Drop all columns except specified
all_cols = df.columns
cols_to_drop = [c for c in all_cols if c not in ["name", "age"]]
df_drop8 = df.drop(*cols_to_drop)

# Method 9: Drop columns by type
from pyspark.sql.types import StringType

# Drop all string columns
string_cols = [field.name for field in df.schema.fields 
               if isinstance(field.dataType, StringType)]
df_drop_strings = df.drop(*string_cols)
df_drop_strings.show()
# +---+------+
# |age|salary|
# +---+------+
# | 25| 50000|
# | 30| 60000|
# +---+------+

# Method 10: Drop columns with null values (if ALL values are null)
# First check which columns are all null
from pyspark.sql.functions import count, when, isnull

# Count non-null values in each column
df.select([count(when(col(c).isNotNull(), c)).alias(c) for c in df.columns]).show()

# Drop columns where all values are null (custom logic)
null_cols = []
for col_name in df.columns:
    null_count = df.filter(col(col_name).isNull()).count()
    if null_count == df.count():  # All values are null
        null_cols.append(col_name)

df_no_nulls = df.drop(*null_cols) if null_cols else df

# Example: Real-world scenario
# Remove PII (Personally Identifiable Information) columns
pii_columns = ["ssn", "credit_card", "phone"]
# Only drop if they exist
existing_pii = [c for c in pii_columns if c in df.columns]
df_clean = df.drop(*existing_pii)

print(f"Dropped columns: {existing_pii}")
print(f"Remaining columns: {df_clean.columns}")</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens internally when you drop a column (does it modify data)?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>No physical data modification!</strong> <code>drop()</code> is a <strong>transformation</strong>, not an action. It only modifies the DataFrame's logical plan.<br><br>
                    <strong>Internally:</strong><br>
                    1. Creates new DataFrame with updated schema (column list)<br>
                    2. Original data remains unchanged in memory/disk<br>
                    3. When action is triggered, Spark's execution plan simply skips reading dropped columns<br>
                    4. If data is columnar (Parquet), dropped columns are never read from disk (column pruning)<br><br>
                    <strong>Key Points:</strong><br>
                     DataFrames are immutable - drop() returns NEW DataFrame<br>
                     No data copying or rewriting happens<br>
                     Very cheap operation (just metadata change)<br>
                     Original DataFrame still has all columns<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1 = df.drop("column_name")  # df1 = new DataFrame without column
# df still has all columns - it's unchanged!

df.show()  # Still has column_name
df1.show() # Does not have column_name</code></pre>
                    <strong>Performance:</strong> When reading Parquet files, dropping columns before action means those columns are never read from disk - saves I/O!
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')" disabled>
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 51: Drop Duplicates -->
<div class="question-content" id="q51">
    <div class="question-header">
        <h1 class="question-title">How to drop duplicates in a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Dropping Duplicates
        </div>
        <div class="definition-box">
            <p><strong>Dropping duplicates</strong> removes duplicate rows from a DataFrame using <code>dropDuplicates()</code> or <code>distinct()</code>. These methods keep only unique rows.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> dropDuplicates()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> distinct()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Unique Rows</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Deduplication</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Methods to Remove Duplicates
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-filter"></i> dropDuplicates() Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Sample data with duplicates
data = [
    ("Alice", 25, "Engineering"),
    ("Bob", 30, "Marketing"),
    ("Alice", 25, "Engineering"),  # Duplicate
    ("Charlie", 35, "Sales"),
    ("Bob", 30, "Marketing"),       # Duplicate
    ("David", 28, "Engineering")
]
df = spark.createDataFrame(data, ["name", "age", "department"])

print(f"Original count: {df.count()}")  # 6 rows
df.show()
# +-------+---+-----------+
# |   name|age| department|
# +-------+---+-----------+
# |  Alice| 25|Engineering|
# |    Bob| 30|  Marketing|
# |  Alice| 25|Engineering|   Duplicate
# |Charlie| 35|      Sales|
# |    Bob| 30|  Marketing|   Duplicate
# |  David| 28|Engineering|
# +-------+---+-----------+

# Method 1: dropDuplicates() - removes ALL duplicate rows
df_unique = df.dropDuplicates()
print(f"After dropDuplicates: {df_unique.count()}")  # 4 rows
df_unique.show()
# +-------+---+-----------+
# |   name|age| department|
# +-------+---+-----------+
# |  Alice| 25|Engineering|
# |    Bob| 30|  Marketing|
# |Charlie| 35|      Sales|
# |  David| 28|Engineering|
# +-------+---+-----------+

# Method 2: distinct() - same as dropDuplicates()
df_distinct = df.distinct()
print(f"After distinct: {df_distinct.count()}")  # 4 rows
df_distinct.show()

# Method 3: dropDuplicates() on specific columns
# Remove duplicates based on 'name' column only
df_unique_name = df.dropDuplicates(["name"])
print(f"Unique by name: {df_unique_name.count()}")  # 4 rows
df_unique_name.show()
# +-------+---+-----------+
# |   name|age| department|
# +-------+---+-----------+
# |  Alice| 25|Engineering|   First occurrence kept
# |    Bob| 30|  Marketing|
# |Charlie| 35|      Sales|
# |  David| 28|Engineering|
# +-------+---+-----------+

# Method 4: dropDuplicates() on multiple columns
# Remove duplicates based on name AND age
df_unique_name_age = df.dropDuplicates(["name", "age"])
df_unique_name_age.show()

# Example with more complex duplicates
data2 = [
    ("Alice", 25, "Engineering", 50000),
    ("Alice", 25, "Engineering", 55000),  # Same name, age, dept; diff salary
    ("Alice", 25, "Sales", 50000),        # Same name, age; diff dept
    ("Bob", 30, "Marketing", 60000)
]
df2 = spark.createDataFrame(data2, ["name", "age", "department", "salary"])

print("Original:")
df2.show()
# +-----+---+-----------+------+
# | name|age| department|salary|
# +-----+---+-----------+------+
# |Alice| 25|Engineering| 50000|
# |Alice| 25|Engineering| 55000|
# |Alice| 25|      Sales| 50000|
# |  Bob| 30|  Marketing| 60000|
# +-----+---+-----------+------+

# Drop duplicates based on name and age only (keeps first occurrence)
df2_unique = df2.dropDuplicates(["name", "age"])
print("Unique by name and age:")
df2_unique.show()
# +-----+---+-----------+------+
# | name|age| department|salary|
# +-----+---+-----------+------+
# |Alice| 25|Engineering| 50000|   First occurrence kept
# |  Bob| 30|  Marketing| 60000|
# +-----+---+-----------+------+

# Drop duplicates from all columns
df2_completely_unique = df2.dropDuplicates()
print("Completely unique rows:")
df2_completely_unique.show()
# +-----+---+-----------+------+
# | name|age| department|salary|
# +-----+---+-----------+------+
# |Alice| 25|Engineering| 50000|
# |Alice| 25|Engineering| 55000|
# |Alice| 25|      Sales| 50000|
# |  Bob| 30|  Marketing| 60000|
# +-----+---+-----------+------+
# All rows are unique when considering all columns!</code></pre>
        </div>

        <div class="highlight-box">
            <strong> Important:</strong> <code>dropDuplicates()</code> keeps the FIRST occurrence of each duplicate and removes subsequent duplicates. The order depends on data partitioning, so results may vary across runs unless data is sorted first.
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 52: Remove Duplicates -->
<div class="question-content" id="q52">
    <div class="question-header">
        <h1 class="question-title">How do you remove duplicates in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Deduplication Methods
        </div>
        <div class="definition-box">
            <p>Use <code>dropDuplicates()</code> for column-specific deduplication or <code>distinct()</code> for entire row deduplication.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            distinct() vs dropDuplicates()
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-star"></i> distinct()</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Scope:</strong> Entire row comparison</li>
                    <li><strong>Parameters:</strong> No parameters</li>
                    <li><strong>Use case:</strong> Remove exact duplicate rows</li>
                    <li><strong>Simple:</strong> Just call distinct()</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># Remove complete duplicates
df.distinct()

# Equivalent to:
df.dropDuplicates()</code></pre>
                </div>
            </div>

            <div class="comparison-card narrow">
                <h4><i class="fas fa-columns"></i> dropDuplicates()</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Scope:</strong> Can specify columns</li>
                    <li><strong>Parameters:</strong> Optional column list</li>
                    <li><strong>Use case:</strong> Flexible deduplication</li>
                    <li><strong>Flexible:</strong> Choose columns to check</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># All columns (same as distinct)
df.dropDuplicates()

# Specific columns
df.dropDuplicates(["col1", "col2"])</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between distinct() and dropDuplicates()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>distinct():</strong> Removes duplicate rows by comparing ALL columns. No parameters. Returns DataFrame with unique rows only.<br><br>
                    <strong>dropDuplicates():</strong> Can specify which columns to check for duplicates. More flexible.<br><br>
                    <strong>Equivalence:</strong> <code>df.distinct()</code> = <code>df.dropDuplicates()</code> (without parameters)<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># These are identical:
df.distinct()
df.dropDuplicates()
df.dropDuplicates([])  # Empty list means all columns

# dropDuplicates is more flexible:
df.dropDuplicates(["name"])           # Check name only
df.dropDuplicates(["name", "age"])    # Check name AND age</code></pre>
                    <strong>Performance:</strong> Identical - both trigger shuffle operation.<br>
                    <strong>Best Practice:</strong> Use <code>distinct()</code> when removing complete duplicates, <code>dropDuplicates()</code> when you need column-level control.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to drop duplicates based on specific columns?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>dropDuplicates()</code> with column list:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Single column
df.dropDuplicates(["email"])

# Multiple columns
df.dropDuplicates(["first_name", "last_name", "birth_date"])

# Example: Keep first occurrence of each user
users = df.dropDuplicates(["user_id"])

# Example: Unique combinations
unique_pairs = df.dropDuplicates(["customer_id", "product_id"])</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 53: Remove Duplicates Based on Columns -->
<div class="question-content" id="q53">
    <div class="question-header">
        <h1 class="question-title">How do you remove duplicates based on specific columns?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Column-Specific Deduplication
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-filter"></i> Removing Duplicates by Columns</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>data = [
    ("alice@email.com", "Alice", "Smith", 25),
    ("bob@email.com", "Bob", "Jones", 30),
    ("alice@email.com", "Alice", "Smith", 26),  # Duplicate email
    ("charlie@email.com", "Charlie", "Brown", 35)
]
df = spark.createDataFrame(data, ["email", "first_name", "last_name", "age"])

# Remove duplicates based on email only
df_unique_email = df.dropDuplicates(["email"])
df_unique_email.show()
# +----------------+----------+---------+---+
# |           email|first_name|last_name|age|
# +----------------+----------+---------+---+
# |  alice@email.com|     Alice|    Smith| 25|   First kept
# |    bob@email.com|       Bob|    Jones| 30|
# |charlie@email.com|   Charlie|    Brown| 35|
# +----------------+----------+---------+---+

# Remove duplicates based on first_name AND last_name
df_unique_name = df.dropDuplicates(["first_name", "last_name"])
df_unique_name.show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between distinct() and dropDuplicates(['col1', 'col2'])?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>distinct():</strong> Compares ALL columns. A row is duplicate only if ALL column values match.<br><br>
                    <strong>dropDuplicates(['col1', 'col2']):</strong> Compares ONLY specified columns. Rows with same col1 and col2 are considered duplicates, even if other columns differ.<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>data = [("A", 1, 100), ("A", 1, 200), ("B", 2, 100)]
df = spark.createDataFrame(data, ["name", "id", "value"])

# distinct() - checks ALL columns
df.distinct().show()
# +----+---+-----+
# |name| id|value|
# +----+---+-----+
# |   A|  1|  100|
# |   A|  1|  200|   Different value, so not duplicate
# |   B|  2|  100|
# +----+---+-----+

# dropDuplicates(['name', 'id']) - checks only name and id
df.dropDuplicates(['name', 'id']).show()
# +----+---+-----+
# |name| id|value|
# +----+---+-----+
# |   A|  1|  100|   First occurrence kept
# |   B|  2|  100|
# +----+---+-----+
# Second row removed because name='A' and id=1 already exists!</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 54: Find Null Values -->
<div class="question-content" id="q54">
    <div class="question-header">
        <h1 class="question-title">How to find null values in PySpark DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Finding Null Values
        </div>
        <div class="definition-box">
            <p><strong>Finding nulls</strong> involves checking which rows or columns contain null values using <code>isNull()</code>, <code>isNotNull()</code>, and aggregation functions.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> isNull()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> isNotNull()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Null Handling</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Missing Data</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Methods to Find Nulls
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-search"></i> Finding Null Values</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, isnan, when, count, sum as spark_sum

# Sample data with nulls
data = [
    ("Alice", 25, 50000),
    ("Bob", None, 60000),
    ("Charlie", 35, None),
    (None, 28, 55000),
    ("David", 30, 65000)
]
df = spark.createDataFrame(data, ["name", "age", "salary"])

df.show()
# +-------+----+------+
# |   name| age|salary|
# +-------+----+------+
# |  Alice|  25| 50000|
# |    Bob|null| 60000|
# |Charlie|  35|  null|
# |   null|  28| 55000|
# |  David|  30| 65000|
# +-------+----+------+

# Method 1: Filter rows with null in specific column
df.filter(col("age").isNull()).show()
# +---+----+------+
# |name|age|salary|
# +---+----+------+
# |Bob|null| 60000|
# +---+----+------+

df.filter(col("name").isNull()).show()
# +----+---+------+
# |name|age|salary|
# +----+---+------+
# |null| 28| 55000|
# +----+---+------+

# Method 2: Filter rows with null in ANY column
from functools import reduce
null_condition = reduce(lambda a, b: a | b, 
                       [col(c).isNull() for c in df.columns])
df.filter(null_condition).show()
# +-------+----+------+
# |   name| age|salary|
# +-------+----+------+
# |    Bob|null| 60000|
# |Charlie|  35|  null|
# |   null|  28| 55000|
# +-------+----+------+

# Method 3: Count nulls in each column
df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()
# +----+---+------+
# |name|age|salary|
# +----+---+------+
# |   1|  1|     1|
# +----+---+------+

# Method 4: Count nulls and non-nulls
print("Null counts per column:")
for col_name in df.columns:
    null_count = df.filter(col(col_name).isNull()).count()
    total_count = df.count()
    print(f"{col_name}: {null_count} nulls out of {total_count} ({null_count/total_count*100:.1f}%)")
# name: 1 nulls out of 5 (20.0%)
# age: 1 nulls out of 5 (20.0%)
# salary: 1 nulls out of 5 (20.0%)

# Method 5: Comprehensive null analysis
null_counts = df.select([
    count(when(col(c).isNull(), c)).alias(f"{c}_null"),
    count(when(col(c).isNotNull(), c)).alias(f"{c}_not_null")
    for c in df.columns
])
null_counts.show()

# Method 6: Get rows where specific column is NOT null
df.filter(col("age").isNotNull()).show()
# +-------+---+------+
# |   name|age|salary|
# +-------+---+------+
# |  Alice| 25| 50000|
# |Charlie| 35|  null|
# |   null| 28| 55000|
# |  David| 30| 65000|
# +-------+---+------+

# Method 7: Get completely clean rows (no nulls anywhere)
clean_df = df.dropna()  # Drops rows with ANY null
clean_df.show()
# +-----+---+------+
# | name|age|salary|
# +-----+---+------+
# |Alice| 25| 50000|
# |David| 30| 65000|
# +-----+---+------+

# Method 8: Create null indicator columns
df_with_flags = df.select(
    "*",
    col("age").isNull().alias("age_is_null"),
    col("salary").isNull().alias("salary_is_null")
)
df_with_flags.show()
# +-------+----+------+-----------+--------------+
# |   name| age|salary|age_is_null|salary_is_null|
# +-------+----+------+-----------+--------------+
# |  Alice|  25| 50000|      false|         false|
# |    Bob|null| 60000|       true|         false|
# |Charlie|  35|  null|      false|          true|
# +-------+----+------+-----------+--------------+

# Method 9: Summary statistics of null values
null_summary = df.select([
    (spark_sum(col(c).isNull().cast("int")) / count("*")).alias(f"{c}_null_pct")
    for c in df.columns
])
null_summary.show()
# +-------------+------------+---------------+
# |name_null_pct|age_null_pct|salary_null_pct|
# +-------------+------------+---------------+
# |          0.2|         0.2|            0.2|
# +-------------+------------+---------------+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to replace nulls using fillna()?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>fillna()</code> or <code>na.fill()</code> to replace null values:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Fill all null values with a single value
df.fillna(0)  # Replace all nulls with 0

# Fill specific columns
df.fillna({"age": 0, "salary": 50000})

# Fill with different values by column
df.fillna({
    "name": "Unknown",
    "age": 0,
    "salary": 0
})

# Fill with column mean (numeric columns)
from pyspark.sql.functions import mean
age_mean = df.select(mean("age")).first()[0]
df.fillna({"age": age_mean})</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to remove null rows using dropna()?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>dropna()</code> or <code>na.drop()</code> to remove rows with nulls:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Drop rows with ANY null value
df.dropna()

# Drop rows where ALL values are null
df.dropna(how='all')

# Drop rows with null in specific columns
df.dropna(subset=["age", "salary"])

# Drop rows with at least N non-null values
df.dropna(thresh=2)  # Keep rows with at least 2 non-null values</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 55: Handle Missing/Null Values -->
<div class="question-content" id="q55">
    <div class="question-header">
        <h1 class="question-title">How do you handle missing/null values?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Null Value Handling Strategies
        </div>
        <div class="definition-box">
            <p><strong>Handling missing values</strong> involves either removing rows/columns with nulls (<code>dropna()</code>) or replacing nulls with meaningful values (<code>fillna()</code>).</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> fillna()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> dropna()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Imputation</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Cleaning</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Handling Strategies
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-tools"></i> fillna() - Replace Nulls</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, mean, when

data = [
    ("Alice", 25, 50000, "Engineering"),
    ("Bob", None, 60000, "Marketing"),
    ("Charlie", 35, None, "Sales"),
    (None, 28, 55000, None)
]
df = spark.createDataFrame(data, ["name", "age", "salary", "department"])

# Strategy 1: Fill with constant values
df_filled = df.fillna({
    "name": "Unknown",
    "age": 0,
    "salary": 0,
    "department": "Not Specified"
})
df_filled.show()

# Strategy 2: Fill with column mean (for numeric columns)
age_mean = df.select(mean("age")).first()[0]
salary_mean = df.select(mean("salary")).first()[0]

df_mean_filled = df.fillna({
    "age": int(age_mean),
    "salary": int(salary_mean)
})

# Strategy 3: Forward fill (use previous non-null value)
from pyspark.sql.window import Window
from pyspark.sql.functions import last

window = Window.orderBy("name").rowsBetween(Window.unboundedPreceding, 0)
df_ffill = df.withColumn(
    "salary",
    last("salary", ignorenulls=True).over(window)
)

# Strategy 4: Conditional filling
df_conditional = df.withColumn(
    "salary",
    when(col("salary").isNull(), 50000).otherwise(col("salary"))
)

# Strategy 5: Fill with mode (most frequent value)
from pyspark.sql.functions import desc
mode_dept = df.groupBy("department").count() \
              .orderBy(desc("count")) \
              .first()[0]
df_mode = df.fillna({"department": mode_dept})</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-trash"></i> dropna() - Remove Nulls</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Strategy 1: Drop rows with ANY null
df_dropped_any = df.dropna()
df_dropped_any.show()
# +-----+---+------+-----------+
# | name|age|salary| department|
# +-----+---+------+-----------+
# |Alice| 25| 50000|Engineering|
# +-----+---+------+-----------+

# Strategy 2: Drop rows where ALL values are null
df_dropped_all = df.dropna(how='all')

# Strategy 3: Drop rows with nulls in specific columns
df_dropped_subset = df.dropna(subset=["age", "salary"])
df_dropped_subset.show()
# +-------+---+------+-----------+
# |   name|age|salary| department|
# +-------+---+------+-----------+
# |  Alice| 25| 50000|Engineering|
# |    Bob| 28| 55000|       null|
# +-------+---+------+-----------+

# Strategy 4: Drop if less than N non-null values
df_dropped_thresh = df.dropna(thresh=3)  # Keep rows with >= 3 non-nulls
df_dropped_thresh.show()

# Strategy 5: Drop columns with too many nulls
null_threshold = 0.5  # 50%
cols_to_keep = []
for col_name in df.columns:
    null_ratio = df.filter(col(col_name).isNull()).count() / df.count()
    if null_ratio < null_threshold:
        cols_to_keep.append(col_name)

df_cols_dropped = df.select(*cols_to_keep)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            When to Use Each Strategy
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-fill"></i> Use fillna() When:</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>Small percentage of nulls (&lt;10%)</li>
                    <li>Can't afford to lose data</li>
                    <li>Have meaningful default values</li>
                    <li>Machine learning requires no nulls</li>
                    <li>Domain knowledge suggests fill value</li>
                </ul>
            </div>

            <div class="comparison-card wide">
                <h4><i class="fas fa-trash"></i> Use dropna() When:</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>High percentage of nulls (&gt;50%)</li>
                    <li>Large dataset, can afford loss</li>
                    <li>No meaningful fill value</li>
                    <li>Null indicates invalid record</li>
                    <li>Critical columns must be complete</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What is the difference between fillna() and dropna()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>fillna():</strong> REPLACES null values with specified values. Keeps all rows, just fills nulls. Returns DataFrame with same or more data.<br><br>
                    <strong>dropna():</strong> REMOVES rows (or columns) containing null values. Reduces dataset size. Returns DataFrame with fewer rows.<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># fillna - no rows lost
df.fillna(0).count()  # Same count as original

# dropna - rows lost
df.dropna().count()   # Fewer rows than original

# Choose based on:
# - fillna(): When you need to keep all data
# - dropna(): When null rows are invalid/unusable</code></pre>
                    <strong>Best Practice:</strong> Analyze null patterns first, then decide strategy. Can combine both: fill some columns, drop rows missing critical columns.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 56: Handle Missing/Null Values in PySpark -->
<div class="question-content" id="q56">
    <div class="question-header">
        <h1 class="question-title">How do you handle missing or null values in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Complete Example
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-broom"></i> Comprehensive Null Handling</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, mean, when, count

data = [
    (1, "Alice", 25, 50000, "A"),
    (2, "Bob", None, 60000, "B"),
    (3, "Charlie", 35, None, "C"),
    (4, None, 28, 55000, None),
    (5, "Eve", 30, 65000, "A")
]
df = spark.createDataFrame(data, ["id", "name", "age", "salary", "grade"])

# Step 1: Analyze nulls
print("Null Analysis:")
null_counts = df.select([
    count(when(col(c).isNull(), c)).alias(c) for c in df.columns
])
null_counts.show()

# Step 2: Handle nulls strategically
df_clean = df \
    .fillna({"name": "Unknown"}) \
    .fillna({"grade": "N/A"}) \
    .dropna(subset=["age", "salary"])  # Drop rows missing critical numeric data

df_clean.show()
# +---+-------+---+------+-----+
# | id|   name|age|salary|grade|
# +---+-------+---+------+-----+
# |  1|  Alice| 25| 50000|    A|
# |  5|    Eve| 30| 65000|    A|
# +---+-------+---+------+-----+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between dropna() and fillna()?</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 55 for detailed answer. <strong>Summary:</strong> dropna() removes rows, fillna() replaces nulls with values.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you apply fillna() to only numeric columns?</strong></span>
                </div>
                <div class="cross-answer">
                    Yes! Filter columns by type and apply fillna():
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.types import NumericType

# Get numeric columns
numeric_cols = [field.name for field in df.schema.fields 
                if isinstance(field.dataType, NumericType)]

# Fill only numeric columns with 0
fill_dict = {col: 0 for col in numeric_cols}
df.fillna(fill_dict)

# Or explicitly:
df.fillna({"age": 0, "salary": 0})  # Only numeric columns

# String columns won't be affected:
df.fillna(0).show()  # String nulls remain null, only numbers filled</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 57: count() vs approx_count_distinct() -->
<div class="question-content" id="q57">
    <div class="question-header">
        <h1 class="question-title">What is the difference between count() and approx_count_distinct()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>count():</strong> Returns the total number of rows. Exact count.</p>
            <p style="margin-top: 1rem;"><strong>approx_count_distinct():</strong> Returns approximate count of distinct/unique values using HyperLogLog algorithm. Much faster for large datasets but approximate.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Aggregation</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Counting</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Distinct Values</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> HyperLogLog</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Comparison Table
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>count()</th>
                        <th>approx_count_distinct()</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Purpose</strong></td>
                        <td>Total row count</td>
                        <td>Approximate distinct count</td>
                    </tr>
                    <tr>
                        <td><strong>Accuracy</strong></td>
                        <td>100% exact</td>
                        <td>~97% accurate (configurable)</td>
                    </tr>
                    <tr>
                        <td><strong>Performance</strong></td>
                        <td>Fast (no shuffle for simple count)</td>
                        <td>Very fast (less memory, no exact tracking)</td>
                    </tr>
                    <tr>
                        <td><strong>Memory Usage</strong></td>
                        <td>Low (just counter)</td>
                        <td>Very low (HyperLogLog sketch)</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Total records</td>
                        <td>Unique values estimate</td>
                    </tr>
                    <tr>
                        <td><strong>Algorithm</strong></td>
                        <td>Simple counter</td>
                        <td>HyperLogLog probabilistic</td>
                    </tr>
                    <tr>
                        <td><strong>Shuffle Required</strong></td>
                        <td>No (for count())</td>
                        <td>Yes (for aggregation)</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-calculator"></i> Using count() and approx_count_distinct()</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import count, countDistinct, approx_count_distinct

# Sample data with duplicates
data = [
    ("Alice", "Engineering"),
    ("Bob", "Marketing"),
    ("Charlie", "Engineering"),
    ("David", "Sales"),
    ("Eve", "Engineering"),
    ("Frank", "Marketing"),
    ("Grace", "Engineering")
]
df = spark.createDataFrame(data, ["name", "department"])

# 1. count() - Total rows
total_count = df.count()
print(f"Total rows: {total_count}")  # 7

# 2. count() with groupBy - Count per group
dept_counts = df.groupBy("department").count()
dept_counts.show()
# +-----------+-----+
# | department|count|
# +-----------+-----+
# |Engineering|    4|
# |  Marketing|    2|
# |      Sales|    1|
# +-----------+-----+

# 3. countDistinct() - Exact distinct count
exact_distinct = df.select(countDistinct("department")).first()[0]
print(f"Exact distinct departments: {exact_distinct}")  # 3

# 4. approx_count_distinct() - Approximate distinct count
approx_distinct = df.select(approx_count_distinct("department")).first()[0]
print(f"Approximate distinct departments: {approx_distinct}")  # 3 (close to exact)

# Large dataset example - showing performance difference
large_data = [(i % 1000,) for i in range(10000000)]  # 10M rows, 1000 unique values
large_df = spark.createDataFrame(large_data, ["value"])

# Exact distinct (slower, more memory)
import time

start = time.time()
exact = large_df.select(countDistinct("value")).first()[0]
exact_time = time.time() - start
print(f"Exact distinct: {exact} in {exact_time:.2f}s")

# Approximate distinct (faster, less memory)
start = time.time()
approx = large_df.select(approx_count_distinct("value")).first()[0]
approx_time = time.time() - start
print(f"Approximate distinct: {approx} in {approx_time:.2f}s")

print(f"Speed improvement: {exact_time/approx_time:.1f}x faster")
print(f"Accuracy: {(approx/exact)*100:.2f}%")

# 5. approx_count_distinct with custom accuracy
# rsd = relative standard deviation (lower = more accurate but slower)
approx_high_accuracy = df.select(
    approx_count_distinct("department", rsd=0.01)  # 1% error margin
).first()[0]

approx_low_accuracy = df.select(
    approx_count_distinct("department", rsd=0.10)  # 10% error margin
).first()[0]

# 6. Multiple columns
df_multi = df.select(
    count("*").alias("total_rows"),
    countDistinct("name").alias("distinct_names"),
    countDistinct("department").alias("exact_distinct_dept"),
    approx_count_distinct("department").alias("approx_distinct_dept")
)
df_multi.show()
# +----------+--------------+-------------------+-------------------+
# |total_rows|distinct_names|exact_distinct_dept|approx_distinct_dept|
# +----------+--------------+-------------------+-------------------+
# |         7|             7|                  3|                  3|
# +----------+--------------+-------------------+-------------------+</code></pre>
        </div>

        <div class="highlight-box">
            <strong> Performance Tip:</strong> For datasets with millions of unique values, <code>approx_count_distinct()</code> can be 10-100x faster than <code>countDistinct()</code> with minimal accuracy loss (typically within 2-3%).
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When would you prefer using approx_count_distinct()?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>approx_count_distinct()</code> when:<br><br>
                    <strong>1. Large datasets:</strong> Millions of rows, performance matters more than exact count.<br>
                    <strong>2. Approximate is acceptable:</strong> Business case doesn't require exact count (e.g., analytics dashboards, monitoring).<br>
                    <strong>3. Memory constraints:</strong> Exact distinct requires significant memory for tracking all unique values.<br>
                    <strong>4. Real-time/streaming:</strong> Need fast approximate results.<br>
                    <strong>5. Cardinality estimation:</strong> Just need to know "how many unique" ballpark.<br><br>
                    <strong>Examples:</strong><br>
                     Unique visitor count: 10,234,567 vs 10,198,234 (difference doesn't matter)<br>
                     Distinct product views: Approximate is fine for reporting<br>
                     Cardinality checks: "Do we have millions or thousands of unique IDs?"<br><br>
                    <strong>Don't use when:</strong><br>
                     Financial calculations (exact counts required)<br>
                     Compliance/audit (must be exact)<br>
                     Small datasets (no performance benefit)<br>
                     Exact count is critical for business logic
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What is the HyperLogLog algorithm?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>HyperLogLog</strong> is a probabilistic algorithm for counting distinct elements. Instead of storing all unique values (high memory), it uses a small fixed-size "sketch" to estimate cardinality.<br><br>
                    <strong>How it works:</strong><br>
                    1. Hash each value<br>
                    2. Look at binary pattern (leading zeros)<br>
                    3. Use statistical properties to estimate total unique values<br><br>
                    <strong>Benefits:</strong><br>
                     Fixed memory: ~12KB per counter regardless of cardinality<br>
                     Very fast: No need to track every unique value<br>
                     Mergeable: Can combine estimates from different partitions<br><br>
                    <strong>Accuracy:</strong> Typically within 2% of exact count with default settings.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 58: orderBy() vs sort() -->
<div class="question-content" id="q58">
    <div class="question-header">
        <h1 class="question-title">What is orderBy() vs sort() in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>orderBy()</strong> and <strong>sort()</strong> are aliases - they are completely identical in functionality. Both sort DataFrame rows by specified columns.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Sorting</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> orderBy()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> sort()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Aliases</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Sorting Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-sort"></i> Using orderBy() and sort()</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, desc, asc

data = [
    ("Alice", 25, 50000),
    ("Bob", 30, 60000),
    ("Charlie", 35, 45000),
    ("David", 28, 55000)
]
df = spark.createDataFrame(data, ["name", "age", "salary"])

# Method 1: orderBy() - ascending by default
df.orderBy("age").show()
# +-------+---+------+
# |   name|age|salary|
# +-------+---+------+
# |  Alice| 25| 50000|
# |  David| 28| 55000|
# |    Bob| 30| 60000|
# |Charlie| 35| 45000|
# +-------+---+------+

# Method 2: sort() - EXACTLY THE SAME
df.sort("age").show()
# +-------+---+------+
# |   name|age|salary|
# +-------+---+------+
# |  Alice| 25| 50000|
# |  David| 28| 55000|
# |    Bob| 30| 60000|
# |Charlie| 35| 45000|
# +-------+---+------+

# Both are identical - use whichever you prefer!

# Descending order
df.orderBy(col("salary").desc()).show()
df.sort(col("salary").desc()).show()  # Same result

# Multiple columns
df.orderBy("age", "salary").show()
df.sort("age", "salary").show()  # Same result

# Explicit ascending/descending
df.orderBy(col("age").asc(), col("salary").desc()).show()
df.sort(col("age").asc(), col("salary").desc()).show()  # Same result

# Using asc() and desc() functions
df.orderBy(asc("age"), desc("salary")).show()
df.sort(asc("age"), desc("salary")).show()  # Same result

# Sort by column position (not recommended)
df.orderBy(df.columns[1]).show()  # Sort by 2nd column (age)

# Null handling
from pyspark.sql.functions import asc_nulls_first, asc_nulls_last, desc_nulls_first, desc_nulls_last

data_with_nulls = [("Alice", 25), ("Bob", None), ("Charlie", 30)]
df_nulls = spark.createDataFrame(data_with_nulls, ["name", "age"])

df_nulls.orderBy(asc_nulls_first("age")).show()
# +-------+----+
# |   name| age|
# +-------+----+
# |    Bob|null|   Nulls first
# |  Alice|  25|
# |Charlie|  30|
# +-------+----+

df_nulls.orderBy(asc_nulls_last("age")).show()
# +-------+----+
# |   name| age|
# +-------+----+
# |  Alice|  25|
# |Charlie|  30|
# |    Bob|null|   Nulls last
# +-------+----+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Are they equivalent? Which one gives better control over sorting direction?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Yes, they are 100% equivalent!</strong> <code>orderBy()</code> and <code>sort()</code> are aliases pointing to the same implementation. There is NO difference in functionality, performance, or control.<br><br>
                    <strong>Both have identical control:</strong><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># These pairs are identical:
df.orderBy(col("age").desc()) == df.sort(col("age").desc())
df.orderBy(asc("age")) == df.sort(asc("age"))
df.orderBy("col1", "col2") == df.sort("col1", "col2")</code></pre>
                    <strong>Naming convention:</strong><br>
                     <code>orderBy()</code>: More SQL-like (ORDER BY in SQL)<br>
                     <code>sort()</code>: More programming-like (common in other languages)<br><br>
                    <strong>Recommendation:</strong> Choose based on your team's preference. SQL users might prefer <code>orderBy()</code>, programmers might prefer <code>sort()</code>. Both work identically.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 59: orderBy() vs sort() Difference -->
<div class="question-content" id="q59">
    <div class="question-header">
        <h1 class="question-title">Difference between orderBy() and sort()</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 5 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Quick Answer
        </div>
        <div class="definition-box">
            <p><strong>There is NO difference!</strong> <code>orderBy()</code> and <code>sort()</code> are aliases. They call the same underlying implementation and have identical behavior, performance, and capabilities.</p>
        </div>

        <div class="highlight-box">
            <strong> Key Point:</strong> Use whichever method name you prefer. Both are equally valid and performant.
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Proof of Equivalence
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-equals"></i> Identical Results</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import desc

data = [(3, "C"), (1, "A"), (2, "B")]
df = spark.createDataFrame(data, ["id", "value"])

# These produce EXACTLY the same result
result1 = df.orderBy("id")
result2 = df.sort("id")

# Verify they're identical
print(result1.collect() == result2.collect())  # True

# Same execution plan
result1.explain()
result2.explain()
# Both show identical physical plans

# Same performance
result1.orderBy(desc("id")).collect()
result2.sort(desc("id")).collect()
# Identical execution time</code></pre>
        </div>

        <div class="note-box">
            <strong>Historical Note:</strong> Both methods exist for API compatibility - <code>sort()</code> for RDD-like naming, <code>orderBy()</code> for SQL-like naming. Choose based on team preference or coding style guidelines.
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 60: Cluster Managers -->
<div class="question-content" id="q60">
    <div class="question-header">
        <h1 class="question-title">What are the different cluster managers supported by Spark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Cluster Managers Overview
        </div>
        <div class="definition-box">
            <p><strong>Cluster managers</strong> are responsible for allocating resources and scheduling tasks across the cluster. Spark supports multiple cluster managers for different deployment scenarios.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Standalone</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> YARN</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Mesos</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Kubernetes</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Resource Management</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-server"></i>
            Supported Cluster Managers
        </div>

        <div style="display: grid; gap: 1.5rem;">
            <div class="highlight-box">
                <strong>1. Standalone Mode (Built-in)</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Description:</strong> Spark's native cluster manager. Simple setup, included with Spark.<br>
                    <strong>Pros:</strong> Easy setup, no external dependencies, good for development/testing<br>
                    <strong>Cons:</strong> Limited features, only runs Spark applications<br>
                    <strong>Use Case:</strong> Development, small clusters, Spark-only environments
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Start standalone cluster
./sbin/start-master.sh
./sbin/start-worker.sh spark://master-host:7077

# Submit job
spark-submit --master spark://master-host:7077 app.py</code></pre>
            </div>

            <div class="highlight-box">
                <strong>2. Apache YARN (Yet Another Resource Negotiator)</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Description:</strong> Hadoop's cluster manager. Most popular in enterprise Hadoop environments.<br>
                    <strong>Pros:</strong> Enterprise-ready, integrates with Hadoop ecosystem, resource sharing with other apps<br>
                    <strong>Cons:</strong> Complex setup, Hadoop dependency<br>
                    <strong>Use Case:</strong> Hadoop clusters, enterprise data platforms (Cloudera, Hortonworks)
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Submit to YARN cluster mode
spark-submit --master yarn --deploy-mode cluster app.py

# YARN client mode
spark-submit --master yarn --deploy-mode client app.py</code></pre>
            </div>

            <div class="highlight-box">
                <strong>3. Apache Mesos</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Description:</strong> General-purpose cluster manager. Can run various frameworks (Spark, Hadoop, etc.).<br>
                    <strong>Pros:</strong> Fine-grained resource sharing, dynamic allocation, multi-tenancy<br>
                    <strong>Cons:</strong> Complex setup, declining popularity<br>
                    <strong>Use Case:</strong> Multi-framework clusters, fine-grained resource control
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Submit to Mesos
spark-submit --master mesos://mesos-master:5050 app.py</code></pre>
            </div>

            <div class="highlight-box">
                <strong>4. Kubernetes (K8s)</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Description:</strong> Container orchestration platform. Modern, cloud-native approach.<br>
                    <strong>Pros:</strong> Cloud-native, containerized, excellent for microservices, growing adoption<br>
                    <strong>Cons:</strong> Requires K8s knowledge, newer (less mature than YARN)<br>
                    <strong>Use Case:</strong> Cloud deployments, containerized environments, modern infrastructure
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Submit to Kubernetes
spark-submit \
  --master k8s://https://kubernetes-api:443 \
  --deploy-mode cluster \
  --conf spark.kubernetes.container.image=spark:latest \
  app.py</code></pre>
            </div>

            <div class="note-box">
                <strong>5. Local Mode</strong>
                <p style="margin-top: 0.5rem; color: var(--text-secondary);">
                    <strong>Description:</strong> Run Spark on single machine. Not really a cluster manager.<br>
                    <strong>Use Case:</strong> Development, testing, learning<br>
                    <strong>Syntax:</strong> <code>--master local[*]</code> (use all cores) or <code>--master local[4]</code> (use 4 cores)
                </p>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Comparison Matrix
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Standalone</th>
                        <th>YARN</th>
                        <th>Mesos</th>
                        <th>Kubernetes</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Setup Complexity</strong></td>
                        <td>Easy</td>
                        <td>Medium</td>
                        <td>Complex</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td><strong>Resource Sharing</strong></td>
                        <td>Spark only</td>
                        <td>Multi-framework</td>
                        <td>Multi-framework</td>
                        <td>Multi-framework</td>
                    </tr>
                    <tr>
                        <td><strong>Dynamic Allocation</strong></td>
                        <td>Limited</td>
                        <td>Excellent</td>
                        <td>Excellent</td>
                        <td>Good</td>
                    </tr>
                    <tr>
                        <td><strong>Enterprise Adoption</strong></td>
                        <td>Low</td>
                        <td>Very High</td>
                        <td>Low</td>
                        <td>Growing</td>
                    </tr>
                    <tr>
                        <td><strong>Cloud Native</strong></td>
                        <td>No</td>
                        <td>Partial</td>
                        <td>No</td>
                        <td>Yes</td>
                    </tr>
                    <tr>
                        <td><strong>Containerization</strong></td>
                        <td>No</td>
                        <td>Limited</td>
                        <td>Yes</td>
                        <td>Native</td>
                    </tr>
                    <tr>
                        <td><strong>Best For</strong></td>
                        <td>Dev/Test</td>
                        <td>Hadoop clusters</td>
                        <td>Multi-framework</td>
                        <td>Cloud, containers</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Cluster Manager Architecture
        </div>
        <div class="image-container">
            <svg width="900" height="500" viewBox="0 0 900 500">
                <rect width="900" height="500" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="20" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Spark Cluster Managers Comparison</text>
                
                <!-- Standalone -->
                <rect x="50" y="60" width="180" height="380" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="140" y="90" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">Standalone</text>
                
                <rect x="70" y="110" width="140" height="60" fill="#334155" stroke="#10b981" stroke-width="1" rx="8"/>
                <text x="140" y="135" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Spark Master</text>
                <text x="140" y="155" font-size="11" fill="#cbd5e1" text-anchor="middle">Built-in</text>
                
                <rect x="70" y="190" width="140" height="100" fill="#334155" stroke="#10b981" stroke-width="1" rx="8"/>
                <text x="140" y="215" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Workers</text>
                <text x="140" y="235" font-size="10" fill="#cbd5e1" text-anchor="middle">Simple setup</text>
                <text x="140" y="255" font-size="10" fill="#cbd5e1" text-anchor="middle">Spark only</text>
                <text x="140" y="275" font-size="10" fill="#cbd5e1" text-anchor="middle">Good for dev</text>
                
                <rect x="70" y="310" width="140" height="100" fill="rgba(16, 185, 129, 0.15)" stroke="#10b981" stroke-width="1" rx="8"/>
                <text x="140" y="335" font-size="12" fill="#10b981" text-anchor="middle" font-weight="bold"> Pros</text>
                <text x="140" y="355" font-size="10" fill="#cbd5e1" text-anchor="middle">Easy setup</text>
                <text x="140" y="375" font-size="10" fill="#cbd5e1" text-anchor="middle">No dependencies</text>
                <text x="140" y="395" font-size="10" fill="#cbd5e1" text-anchor="middle">Fast start</text>
                
                <!-- YARN -->
                <rect x="260" y="60" width="180" height="380" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="350" y="90" font-size="16" fill="#3b82f6" text-anchor="middle" font-weight="bold">YARN</text>
                
                <rect x="280" y="110" width="140" height="60" fill="#334155" stroke="#3b82f6" stroke-width="1" rx="8"/>
                <text x="350" y="135" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Resource Manager</text>
                <text x="350" y="155" font-size="11" fill="#cbd5e1" text-anchor="middle">Hadoop</text>
                
                <rect x="280" y="190" width="140" height="100" fill="#334155" stroke="#3b82f6" stroke-width="1" rx="8"/>
                <text x="350" y="215" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Node Managers</text>
                <text x="350" y="235" font-size="10" fill="#cbd5e1" text-anchor="middle">Enterprise ready</text>
                <text x="350" y="255" font-size="10" fill="#cbd5e1" text-anchor="middle">Multi-framework</text>
                <text x="350" y="275" font-size="10" fill="#cbd5e1" text-anchor="middle">Popular choice</text>
                
                <rect x="280" y="310" width="140" height="100" fill="rgba(59, 130, 246, 0.15)" stroke="#3b82f6" stroke-width="1" rx="8"/>
                <text x="350" y="335" font-size="12" fill="#3b82f6" text-anchor="middle" font-weight="bold"> Pros</text>
                <text x="350" y="355" font-size="10" fill="#cbd5e1" text-anchor="middle">Mature</text>
                <text x="350" y="375" font-size="10" fill="#cbd5e1" text-anchor="middle">Resource sharing</text>
                <text x="350" y="395" font-size="10" fill="#cbd5e1" text-anchor="middle">Hadoop ecosystem</text>
                
                <!-- Mesos -->
                <rect x="470" y="60" width="180" height="380" fill="rgba(245, 158, 11, 0.1)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="560" y="90" font-size="16" fill="#f59e0b" text-anchor="middle" font-weight="bold">Mesos</text>
                
                <rect x="490" y="110" width="140" height="60" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="8"/>
                <text x="560" y="135" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Mesos Master</text>
                <text x="560" y="155" font-size="11" fill="#cbd5e1" text-anchor="middle">General purpose</text>
                
                <rect x="490" y="190" width="140" height="100" fill="#334155" stroke="#f59e0b" stroke-width="1" rx="8"/>
                <text x="560" y="215" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Mesos Agents</text>
                <text x="560" y="235" font-size="10" fill="#cbd5e1" text-anchor="middle">Fine-grained</text>
                <text x="560" y="255" font-size="10" fill="#cbd5e1" text-anchor="middle">Dynamic allocation</text>
                <text x="560" y="275" font-size="10" fill="#cbd5e1" text-anchor="middle">Multi-tenancy</text>
                
                <rect x="490" y="310" width="140" height="100" fill="rgba(245, 158, 11, 0.15)" stroke="#f59e0b" stroke-width="1" rx="8"/>
                <text x="560" y="335" font-size="12" fill="#f59e0b" text-anchor="middle" font-weight="bold"> Status</text>
                <text x="560" y="355" font-size="10" fill="#cbd5e1" text-anchor="middle">Declining use</text>
                <text x="560" y="375" font-size="10" fill="#cbd5e1" text-anchor="middle">Complex setup</text>
                <text x="560" y="395" font-size="10" fill="#cbd5e1" text-anchor="middle">Legacy systems</text>
                
                <!-- Kubernetes -->
                <rect x="680" y="60" width="180" height="380" fill="rgba(168, 85, 247, 0.1)" stroke="#a855f7" stroke-width="2" rx="12"/>
                <text x="770" y="90" font-size="16" fill="#a855f7" text-anchor="middle" font-weight="bold">Kubernetes</text>
                
                <rect x="700" y="110" width="140" height="60" fill="#334155" stroke="#a855f7" stroke-width="1" rx="8"/>
                <text x="770" y="135" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">K8s Master</text>
                <text x="770" y="155" font-size="11" fill="#cbd5e1" text-anchor="middle">Cloud-native</text>
                
                <rect x="700" y="190" width="140" height="100" fill="#334155" stroke="#a855f7" stroke-width="1" rx="8"/>
                <text x="770" y="215" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">K8s Nodes (Pods)</text>
                <text x="770" y="235" font-size="10" fill="#cbd5e1" text-anchor="middle">Containerized</text>
                <text x="770" y="255" font-size="10" fill="#cbd5e1" text-anchor="middle">Modern</text>
                <text x="770" y="275" font-size="10" fill="#cbd5e1" text-anchor="middle">Cloud-ready</text>
                
                <rect x="700" y="310" width="140" height="100" fill="rgba(168, 85, 247, 0.15)" stroke="#a855f7" stroke-width="1" rx="8"/>
                <text x="770" y="335" font-size="12" fill="#a855f7" text-anchor="middle" font-weight="bold"> Modern</text>
                <text x="770" y="355" font-size="10" fill="#cbd5e1" text-anchor="middle">Growing adoption</text>
                <text x="770" y="375" font-size="10" fill="#cbd5e1" text-anchor="middle">Container-native</text>
                <text x="770" y="395" font-size="10" fill="#cbd5e1" text-anchor="middle">Future-proof</text>
            </svg>
            <p class="image-caption">Four main cluster managers supported by Apache Spark</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which one is default in local mode?</strong></span>
                </div>
                <div class="cross-answer">
                    In <strong>local mode</strong>, there is no cluster manager - Spark runs everything in a single JVM on one machine. Use <code>--master local</code> or <code>--master local[*]</code>.<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Local mode with 1 thread
spark-submit --master local app.py

# Local mode with N threads
spark-submit --master local[4] app.py

# Local mode with all available cores
spark-submit --master local[*] app.py</code></pre>
                    Local mode is for development/testing only. For production clusters, use Standalone, YARN, or Kubernetes.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which cluster manager should I choose?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Choose based on your environment:</strong><br><br>
                     <strong>Existing Hadoop cluster?</strong>  Use YARN<br>
                     <strong>Cloud/containers (AWS, Azure, GCP)?</strong>  Use Kubernetes<br>
                     <strong>Learning Spark/small project?</strong>  Use Standalone or Local<br>
                     <strong>Legacy multi-framework setup?</strong>  Consider Mesos (but K8s is better)<br>
                     <strong>Starting fresh in 2024?</strong>  Kubernetes (most future-proof)<br><br>
                    <strong>Most common in enterprise:</strong> YARN (existing infrastructure) or Kubernetes (new deployments)
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 61: Default Deployment Mode -->
<div class="question-content" id="q61">
    <div class="question-header">
        <h1 class="question-title">What is the default deployment mode in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Deployment Modes
        </div>
        <div class="definition-box">
            <p><strong>Default deployment mode:</strong> <code>client mode</code> when using spark-submit without specifying <code>--deploy-mode</code>.</p>
            <p style="margin-top: 1rem;">There are two deployment modes: <strong>client mode</strong> (default) and <strong>cluster mode</strong>.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Client Mode</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Cluster Mode</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Driver Location</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Deployment</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            Client Mode vs Cluster Mode
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-desktop"></i> Client Mode (Default)</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Driver:</strong> Runs on client machine (where spark-submit is called)</li>
                    <li><strong>Interactive:</strong> Good for development, REPL, notebooks</li>
                    <li><strong>Output:</strong> Visible in client console</li>
                    <li><strong>Network:</strong> Client must stay connected</li>
                    <li><strong>Logs:</strong> Available on client machine</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># Default: client mode
spark-submit app.py

# Explicit client mode
spark-submit \
  --deploy-mode client \
  app.py</code></pre>
                </div>
            </div>

            <div class="comparison-card wide">
                <h4><i class="fas fa-server"></i> Cluster Mode</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Driver:</strong> Runs on worker node in cluster</li>
                    <li><strong>Production:</strong> Good for production jobs</li>
                    <li><strong>Output:</strong> In cluster logs, not client</li>
                    <li><strong>Network:</strong> Client can disconnect after submission</li>
                    <li><strong>Logs:</strong> On cluster nodes</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code># Cluster mode
spark-submit \
  --deploy-mode cluster \
  app.py

# Fire and forget!</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Visual Comparison
        </div>
        <div class="image-container">
            <svg width="900" height="400" viewBox="0 0 900 400">
                <rect width="900" height="400" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="20" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Client Mode vs Cluster Mode</text>
                
                <!-- Client Mode -->
                <rect x="50" y="60" width="380" height="300" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="240" y="90" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">Client Mode (Default)</text>
                
                <!-- Client Machine -->
                <rect x="80" y="110" width="140" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="150" y="135" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Client Machine</text>
                <text x="150" y="155" font-size="12" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Driver Here</text>
                <text x="150" y="175" font-size="11" fill="#cbd5e1" text-anchor="middle">spark-submit</text>
                
                <!-- Arrow to cluster -->
                <path d="M 220 150 L 270 150" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                <text x="245" y="145" font-size="11" fill="#10b981" text-anchor="middle">submits</text>
                
                <!-- Cluster -->
                <rect x="270" y="110" width="140" height="230" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="340" y="135" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Cluster</text>
                
                <rect x="285" y="150" width="110" height="50" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="340" y="172" font-size="12" fill="#f1f5f9" text-anchor="middle">Executor 1</text>
                <text x="340" y="188" font-size="10" fill="#cbd5e1" text-anchor="middle">Tasks</text>
                
                <rect x="285" y="210" width="110" height="50" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="340" y="232" font-size="12" fill="#f1f5f9" text-anchor="middle">Executor 2</text>
                <text x="340" y="248" font-size="10" fill="#cbd5e1" text-anchor="middle">Tasks</text>
                
                <rect x="285" y="270" width="110" height="50" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="340" y="292" font-size="12" fill="#f1f5f9" text-anchor="middle">Executor 3</text>
                <text x="340" y="308" font-size="10" fill="#cbd5e1" text-anchor="middle">Tasks</text>
                
                <!-- Cluster Mode -->
                <rect x="470" y="60" width="380" height="300" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="660" y="90" font-size="18" fill="#3b82f6" text-anchor="middle" font-weight="bold">Cluster Mode</text>
                
                <!-- Client Machine -->
                <rect x="500" y="110" width="140" height="80" fill="#334155" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="570" y="135" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Client Machine</text>
                <text x="570" y="155" font-size="12" fill="#cbd5e1" text-anchor="middle">spark-submit</text>
                <text x="570" y="175" font-size="11" fill="#cbd5e1" text-anchor="middle">(can disconnect)</text>
                
                <!-- Arrow to cluster -->
                <path d="M 640 150 L 685 150" stroke="#3b82f6" stroke-width="2" marker-end="url(#arrowblue)"/>
                <text x="662" y="145" font-size="11" fill="#3b82f6" text-anchor="middle">submits</text>
                
                <!-- Cluster with Driver -->
                <rect x="685" y="110" width="145" height="230" fill="#334155" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="757" y="135" font-size="14" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Cluster</text>
                
                <rect x="700" y="150" width="115" height="50" fill="rgba(14, 165, 233, 0.3)" stroke="#0ea5e9" stroke-width="2" rx="6"/>
                <text x="757" y="172" font-size="12" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Driver Here</text>
                <text x="757" y="188" font-size="10" fill="#cbd5e1" text-anchor="middle">(on worker)</text>
                
                <rect x="700" y="210" width="115" height="50" fill="rgba(59, 130, 246, 0.2)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="757" y="232" font-size="12" fill="#f1f5f9" text-anchor="middle">Executor 1</text>
                
                <rect x="700" y="270" width="115" height="50" fill="rgba(59, 130, 246, 0.2)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="757" y="292" font-size="12" fill="#f1f5f9" text-anchor="middle">Executor 2</text>
                
                <defs>
                    <marker id="arrowgreen" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                    </marker>
                    <marker id="arrowblue" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#3b82f6"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Client Mode: Driver on client machine | Cluster Mode: Driver in cluster</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            When to Use Each Mode
        </div>

        <div class="highlight-box">
            <strong>Use Client Mode (default) when:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Development and testing</li>
                <li>Interactive sessions (pyspark shell, Jupyter)</li>
                <li>Debugging (need to see logs immediately)</li>
                <li>Client machine has good network to cluster</li>
            </ul>
        </div>

        <div class="highlight-box">
            <strong>Use Cluster Mode when:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Production jobs</li>
                <li>Scheduled/automated jobs</li>
                <li>Client machine is unreliable or remote</li>
                <li>Don't need immediate console output</li>
            </ul>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 62: spark-submit -->
<div class="question-content" id="q62">
    <div class="question-header">
        <h1 class="question-title">How do you submit a PySpark job on a cluster using spark-submit?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            spark-submit Overview
        </div>
        <div class="definition-box">
            <p><strong>spark-submit</strong> is the command-line tool to submit Spark applications to a cluster. It handles application deployment, resource allocation, and execution across different cluster managers.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> spark-submit</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Deployment</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Configuration</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Resource Allocation</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Basic spark-submit Syntax
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-terminal"></i> Basic Usage</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Basic syntax
spark-submit [options] <application.py> [application-arguments]

# Simplest example - local mode
spark-submit my_app.py

# Specify master
spark-submit --master local[4] my_app.py
spark-submit --master spark://master-host:7077 my_app.py
spark-submit --master yarn my_app.py
spark-submit --master k8s://https://k8s-api:443 my_app.py</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-cogs"></i> Common Options</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Full example with common options
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 4g \
  --executor-memory 2g \
  --executor-cores 2 \
  --num-executors 10 \
  --conf spark.sql.shuffle.partitions=200 \
  --conf spark.default.parallelism=100 \
  --py-files dependencies.zip \
  --files config.json \
  --archives data.zip \
  my_app.py arg1 arg2

# Breakdown of options:
# --master yarn              : Cluster manager (yarn, spark://..., k8s://...)
# --deploy-mode cluster      : Where to run driver (client or cluster)
# --driver-memory 4g         : Memory for driver
# --executor-memory 2g       : Memory per executor
# --executor-cores 2         : Cores per executor
# --num-executors 10         : Number of executors
# --conf                     : Spark configuration properties
# --py-files                 : Additional Python files (zip or .py)
# --files                    : Additional files to distribute
# --archives                 : Archives to be extracted on each executor
# my_app.py                  : Main application file
# arg1 arg2                  : Application arguments</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-server"></i>
            Cluster-Specific Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-database"></i> YARN Cluster</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># YARN - Client mode (default)
spark-submit \
  --master yarn \
  --deploy-mode client \
  --driver-memory 2g \
  --executor-memory 4g \
  --executor-cores 2 \
  --num-executors 5 \
  --queue default \
  my_app.py

# YARN - Cluster mode (production)
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 2g \
  --executor-memory 4g \
  --executor-cores 2 \
  --num-executors 5 \
  --conf spark.yarn.submit.waitAppCompletion=false \
  my_app.py</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-dharmachakra"></i> Kubernetes Cluster</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Kubernetes
spark-submit \
  --master k8s://https://k8s-api-server:443 \
  --deploy-mode cluster \
  --name spark-pi \
  --conf spark.executor.instances=3 \
  --conf spark.kubernetes.container.image=spark:3.5.0 \
  --conf spark.kubernetes.namespace=spark-jobs \
  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
  local:///opt/spark/examples/src/main/python/pi.py</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-fire"></i> Standalone Cluster</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Standalone
spark-submit \
  --master spark://master-host:7077 \
  --deploy-mode client \
  --driver-memory 2g \
  --executor-memory 4g \
  --total-executor-cores 8 \
  my_app.py</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Important spark-submit Options
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Option</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>--master</strong></td>
                        <td>Cluster manager URL</td>
                        <td>yarn, local[4], spark://host:7077</td>
                    </tr>
                    <tr>
                        <td><strong>--deploy-mode</strong></td>
                        <td>Where to run driver</td>
                        <td>client (default), cluster</td>
                    </tr>
                    <tr>
                        <td><strong>--driver-memory</strong></td>
                        <td>Driver memory</td>
                        <td>2g, 4g, 512m</td>
                    </tr>
                    <tr>
                        <td><strong>--executor-memory</strong></td>
                        <td>Memory per executor</td>
                        <td>2g, 4g, 8g</td>
                    </tr>
                    <tr>
                        <td><strong>--executor-cores</strong></td>
                        <td>Cores per executor</td>
                        <td>2, 4, 8</td>
                    </tr>
                    <tr>
                        <td><strong>--num-executors</strong></td>
                        <td>Number of executors (YARN)</td>
                        <td>5, 10, 20</td>
                    </tr>
                    <tr>
                        <td><strong>--total-executor-cores</strong></td>
                        <td>Total cores (Standalone, Mesos)</td>
                        <td>16, 32</td>
                    </tr>
                    <tr>
                        <td><strong>--py-files</strong></td>
                        <td>Additional Python files</td>
                        <td>libs.zip, utils.py</td>
                    </tr>
                    <tr>
                        <td><strong>--files</strong></td>
                        <td>Additional files</td>
                        <td>config.json, data.csv</td>
                    </tr>
                    <tr>
                        <td><strong>--conf</strong></td>
                        <td>Spark configuration</td>
                        <td>spark.sql.shuffle.partitions=200</td>
                    </tr>
                    <tr>
                        <td><strong>--name</strong></td>
                        <td>Application name</td>
                        <td>"My Spark Job"</td>
                    </tr>
                    <tr>
                        <td><strong>--queue</strong></td>
                        <td>YARN queue</td>
                        <td>default, production</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Best Practices
        </div>

        <div class="highlight-box">
            <strong>1. Resource Allocation:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Rule of thumb: executor-cores * num-executors  total cluster cores
# Leave some cores for system processes
# Example: 50-core cluster
--num-executors 10 --executor-cores 4  # Uses 40 cores, leaves 10 for OS</code></pre>
        </div>

        <div class="highlight-box">
            <strong>2. Memory Configuration:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Executor memory formula:
# executor-memory = (node_memory / executors_per_node) * 0.9
# Leave 10% for OS and overhead

# Example: 32GB node, 4 executors per node
--executor-memory 7g  # (32GB / 4) * 0.9  7GB</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Application Dependencies:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Package Python dependencies
zip -r mylibs.zip mypackage/

spark-submit \
  --py-files mylibs.zip,utils.py \
  --files config.json \
  my_app.py</code></pre>
        </div>

        <div class="warning-box">
            <strong> Common Mistakes:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Over-allocating resources (causes cluster instability)</li>
                <li>Too many executor cores (>5 cores per executor reduces performance)</li>
                <li>Forgetting --py-files for dependencies</li>
                <li>Using client mode for production jobs</li>
            </ul>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 63: Role of Driver and Executors -->
<div class="question-content" id="q63">
    <div class="question-header">
        <h1 class="question-title">Explain the role of driver and executors</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Spark Architecture Components
        </div>
        <div class="definition-box">
            <p><strong>Driver:</strong> The main control process that runs the main() function, creates SparkContext, converts user code into tasks, and schedules tasks on executors.</p>
            <p style="margin-top: 1rem;"><strong>Executors:</strong> Worker processes that run on cluster nodes, execute tasks assigned by the driver, and store data for cached RDDs/DataFrames.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Driver</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Executors</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> SparkContext</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Task Scheduling</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Distributed Computing</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Driver and Executors Architecture
        </div>
        <div class="image-container">
            <svg width="900" height="550" viewBox="0 0 900 550">
                <rect width="900" height="550" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Spark Driver-Executor Architecture</text>
                
                <!-- Driver -->
                <rect x="350" y="60" width="200" height="180" fill="rgba(14, 165, 233, 0.2)" stroke="#0ea5e9" stroke-width="3" rx="12"/>
                <text x="450" y="90" font-size="18" fill="#0ea5e9" text-anchor="middle" font-weight="bold">DRIVER</text>
                <text x="450" y="110" font-size="12" fill="#cbd5e1" text-anchor="middle">(Master Process)</text>
                
                <rect x="370" y="120" width="160" height="40" fill="#334155" stroke="#0ea5e9" stroke-width="1" rx="6"/>
                <text x="450" y="138" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">SparkContext</text>
                <text x="450" y="152" font-size="11" fill="#cbd5e1" text-anchor="middle">Entry point</text>
                
                <rect x="370" y="170" width="160" height="55" fill="#334155" stroke="#0ea5e9" stroke-width="1" rx="6"/>
                <text x="450" y="185" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Responsibilities:</text>
                <text x="450" y="200" font-size="10" fill="#cbd5e1" text-anchor="middle"> Creates DAG</text>
                <text x="450" y="213" font-size="10" fill="#cbd5e1" text-anchor="middle"> Schedules tasks</text>
                
                <!-- Cluster Manager -->
                <rect x="625" y="60" width="200" height="80" fill="rgba(245, 158, 11, 0.2)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="725" y="90" font-size="16" fill="#f59e0b" text-anchor="middle" font-weight="bold">Cluster Manager</text>
                <text x="725" y="110" font-size="11" fill="#cbd5e1" text-anchor="middle">YARN / K8s / Standalone</text>
                <text x="725" y="125" font-size="10" fill="#cbd5e1" text-anchor="middle">Allocates resources</text>
                
                <!-- Arrow from Driver to Cluster Manager -->
                <path d="M 550 100 L 625 100" stroke="#f59e0b" stroke-width="2" marker-end="url(#arroworange)"/>
                <text x="587" y="95" font-size="11" fill="#f59e0b" text-anchor="middle">requests</text>
                <text x="587" y="108" font-size="11" fill="#f59e0b" text-anchor="middle">resources</text>
                
                <!-- Executors -->
                <text x="450" y="280" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">EXECUTORS (Worker Nodes)</text>
                
                <!-- Executor 1 -->
                <rect x="75" y="310" width="220" height="200" fill="rgba(16, 185, 129, 0.15)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="185" y="340" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">Executor 1</text>
                
                <rect x="95" y="355" width="180" height="45" fill="#334155" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="185" y="372" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 1</text>
                <text x="185" y="388" font-size="10" fill="#cbd5e1" text-anchor="middle">Processing partition 1</text>
                
                <rect x="95" y="410" width="180" height="45" fill="#334155" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="185" y="427" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 2</text>
                <text x="185" y="443" font-size="10" fill="#cbd5e1" text-anchor="middle">Processing partition 2</text>
                
                <rect x="95" y="465" width="180" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="185" y="485" font-size="11" fill="#cbd5e1" text-anchor="middle">Cache storage</text>
                
                <!-- Executor 2 -->
                <rect x="340" y="310" width="220" height="200" fill="rgba(16, 185, 129, 0.15)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="450" y="340" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">Executor 2</text>
                
                <rect x="360" y="355" width="180" height="45" fill="#334155" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="450" y="372" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 3</text>
                <text x="450" y="388" font-size="10" fill="#cbd5e1" text-anchor="middle">Processing partition 3</text>
                
                <rect x="360" y="410" width="180" height="45" fill="#334155" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="450" y="427" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 4</text>
                <text x="450" y="443" font-size="10" fill="#cbd5e1" text-anchor="middle">Processing partition 4</text>
                
                <rect x="360" y="465" width="180" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="450" y="485" font-size="11" fill="#cbd5e1" text-anchor="middle">Cache storage</text>
                
                <!-- Executor 3 -->
                <rect x="605" y="310" width="220" height="200" fill="rgba(16, 185, 129, 0.15)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="715" y="340" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">Executor 3</text>
                
                <rect x="625" y="355" width="180" height="45" fill="#334155" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="715" y="372" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 5</text>
                <text x="715" y="388" font-size="10" fill="#cbd5e1" text-anchor="middle">Processing partition 5</text>
                
                <rect x="625" y="410" width="180" height="45" fill="#334155" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="715" y="427" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 6</text>
                <text x="715" y="443" font-size="10" fill="#cbd5e1" text-anchor="middle">Processing partition 6</text>
                
                <rect x="625" y="465" width="180" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="715" y="485" font-size="11" fill="#cbd5e1" text-anchor="middle">Cache storage</text>
                
                <!-- Arrows from Driver to Executors -->
                <path d="M 400 240 L 185 310" stroke="#0ea5e9" stroke-width="2" marker-end="url(#arrowblue)" stroke-dasharray="5,5"/>
                <path d="M 450 240 L 450 310" stroke="#0ea5e9" stroke-width="2" marker-end="url(#arrowblue)" stroke-dasharray="5,5"/>
                <path d="M 500 240 L 715 310" stroke="#0ea5e9" stroke-width="2" marker-end="url(#arrowblue)" stroke-dasharray="5,5"/>
                
                <text x="450" y="275" font-size="12" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Sends tasks & monitors</text>
                
                <defs>
                    <marker id="arrowblue" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#0ea5e9"/>
                    </marker>
                    <marker id="arroworange" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#f59e0b"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Driver coordinates; Executors execute tasks on data partitions</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-tasks"></i>
            Detailed Roles
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-brain"></i> Driver Responsibilities</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Run main():</strong> Executes user's main program</li>
                    <li><strong>Create SparkContext:</strong> Entry point to Spark</li>
                    <li><strong>Build DAG:</strong> Converts transformations to execution plan</li>
                    <li><strong>Schedule tasks:</strong> Assigns tasks to executors</li>
                    <li><strong>Monitor execution:</strong> Tracks task progress</li>
                    <li><strong>Collect results:</strong> Gathers data from executors</li>
                    <li><strong>Cache metadata:</strong> Tracks cached data locations</li>
                </ul>
            </div>

            <div class="comparison-card narrow">
                <h4><i class="fas fa-running"></i> Executor Responsibilities</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Execute tasks:</strong> Run code sent by driver</li>
                    <li><strong>Store data:</strong> Keep RDD/DataFrame partitions in memory</li>
                    <li><strong>Cache data:</strong> Store cached partitions</li>
                    <li><strong>Send results:</strong> Return results to driver</li>
                    <li><strong>Report status:</strong> Heartbeat to driver</li>
                    <li><strong>Read/Write data:</strong> Access storage (HDFS, S3, etc.)</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Driver and Executor in Code
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-laptop-code"></i> Understanding Driver and Executors</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql import SparkSession

# This runs on DRIVER
spark = SparkSession.builder.appName("Driver-Executor Demo").getOrCreate()

# This runs on DRIVER
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = spark.sparkContext.parallelize(data, 3)  # 3 partitions

# Transformation: Plan created on DRIVER, executed on EXECUTORS
# Each executor processes different partitions
squared = rdd.map(lambda x: x * x)  # Runs on executors

# This lambda function runs on EXECUTORS (distributed)
def square_partition(partition):
    print(f"Processing on executor")  # Prints on executor logs
    return [x * x for x in partition]

result_rdd = rdd.mapPartitions(square_partition)

# Action: Triggers execution on EXECUTORS, results sent to DRIVER
result = result_rdd.collect()  # Runs on executors, collected by driver
print(f"Results on driver: {result}")  # Prints on driver console

# Driver code vs Executor code
def process_on_executor(x):
    """This function runs on EXECUTORS"""
    return x * 2

# This runs on DRIVER
print("Driver: Creating RDD")

# This runs on EXECUTORS (distributed across partitions)
doubled = rdd.map(process_on_executor)

# This runs on DRIVER (after collecting results from executors)
print(f"Driver: Final count = {doubled.count()}")

# Example showing data flow
df = spark.range(100).repartition(3)  # 3 partitions on 3 executors

# Transformation on executors
df_transformed = df.selectExpr("id * 2 as doubled")

# Action: executors compute, driver collects
first_10 = df_transformed.take(10)  # Executors compute, driver gets first 10
print(f"Driver received: {first_10}")</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens if the driver fails?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>If the driver fails, the entire application fails!</strong> The driver is the single point of failure.<br><br>
                    <strong>Consequences:</strong><br>
                    1. <strong>All executors become orphaned:</strong> They have no coordinator<br>
                    2. <strong>Application terminates:</strong> Complete failure, not recoverable<br>
                    3. <strong>All work lost:</strong> Must restart from beginning<br>
                    4. <strong>Resources released:</strong> Cluster manager deallocates executors<br><br>
                    <strong>Why it's critical:</strong><br>
                     Driver holds SparkContext (the entry point)<br>
                     Driver has the DAG and execution plan<br>
                     Driver knows which executor has which data partition<br>
                     Driver schedules all tasks<br><br>
                    <strong>Mitigation strategies:</strong><br>
                    1. <strong>Use cluster mode:</strong> Driver runs on worker node (more reliable)<br>
                    2. <strong>Driver high availability:</strong> Some cluster managers support driver restart<br>
                    3. <strong>Checkpointing:</strong> Save state periodically (for Streaming)<br>
                    4. <strong>Monitoring:</strong> Alert when driver fails, restart quickly<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Enable driver supervision (YARN)
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.yarn.maxAppAttempts=3 \  # Retry up to 3 times
  app.py

# For Streaming: checkpoint for recovery
streamingContext.checkpoint("hdfs://checkpoint")
</code></pre>
                    <strong>Note:</strong> Unlike executor failures (which Spark can recover from), driver failure requires application restart.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 64: Driver and Executors Explained -->
<div class="question-content" id="q64">
    <div class="question-header">
        <h1 class="question-title">What are driver and executors in Spark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Quick Summary
        </div>
        <div class="definition-box">
            <p><strong>Driver:</strong> The master/coordinator process that converts your code into jobs and schedules tasks.</p>
            <p style="margin-top: 1rem;"><strong>Executors:</strong> Worker processes that execute the tasks and store data.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Quick Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Driver</th>
                        <th>Executors</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Count</strong></td>
                        <td>1 per application</td>
                        <td>Multiple (configurable)</td>
                    </tr>
                    <tr>
                        <td><strong>Location</strong></td>
                        <td>Client or cluster</td>
                        <td>Worker nodes</td>
                    </tr>
                    <tr>
                        <td><strong>Role</strong></td>
                        <td>Coordinator/Scheduler</td>
                        <td>Worker/Executor</td>
                    </tr>
                    <tr>
                        <td><strong>Failure Impact</strong></td>
                        <td>Application dies</td>
                        <td>Task retried on another executor</td>
                    </tr>
                    <tr>
                        <td><strong>Memory</strong></td>
                        <td>For driver operations</td>
                        <td>For data processing & caching</td>
                    </tr>
                    <tr>
                        <td><strong>Code Execution</strong></td>
                        <td>main(), transformations planning</td>
                        <td>Actual data processing</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens if your driver fails?</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 63 for detailed answer. <strong>Summary:</strong> Application fails completely - driver is single point of failure. All executors become orphaned, work is lost, application must restart from beginning.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 65: DAG in Spark -->
<div class="question-content" id="q65">
    <div class="question-header">
        <h1 class="question-title">What is a DAG in Spark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>DAG (Directed Acyclic Graph)</strong> is a graph of the sequence of operations (transformations and actions) that will be executed. It represents the logical execution plan.</p>
            <p style="margin-top: 1rem;"><strong>Directed:</strong> Operations have a direction (A  B)<br>
            <strong>Acyclic:</strong> No cycles/loops - cannot go back to previous operation<br>
            <strong>Graph:</strong> Nodes (RDDs/DataFrames) connected by edges (transformations)</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> DAG</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Execution Plan</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Stages</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Tasks</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Optimization</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            DAG Visualization
        </div>
        <div class="image-container">
            <svg width="900" height="600" viewBox="0 0 900 600">
                <rect width="900" height="600" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Spark DAG Example</text>
                
                <!-- Code Example on Left -->
                <rect x="30" y="60" width="280" height="200" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="170" y="85" font-size="14" fill="#3b82f6" text-anchor="middle" font-weight="bold">User Code</text>
                <text x="50" y="110" font-size="11" fill="#cbd5e1" font-family="monospace">df = spark.read.csv("data.csv")</text>
                <text x="50" y="130" font-size="11" fill="#cbd5e1" font-family="monospace">df2 = df.filter(col("age") > 25)</text>
                <text x="50" y="150" font-size="11" fill="#cbd5e1" font-family="monospace">df3 = df2.select("name", "age")</text>
                <text x="50" y="170" font-size="11" fill="#cbd5e1" font-family="monospace">df4 = df3.groupBy("age").count()</text>
                <text x="50" y="190" font-size="11" fill="#cbd5e1" font-family="monospace">df5 = df4.orderBy("count")</text>
                <text x="50" y="210" font-size="11" fill="#10b981" font-family="monospace" font-weight="bold">df5.show()  # ACTION</text>
                <text x="50" y="240" font-size="11" fill="#f59e0b" text-anchor="start" font-weight="bold"> Creates DAG Below </text>
                
                <!-- DAG Representation -->
                <text x="450" y="295" font-size="18" fill="#0ea5e9" text-anchor="middle" font-weight="bold">DAG Generated by Spark</text>
                
                <!-- Stage 1 -->
                <rect x="50" y="320" width="800" height="250" fill="rgba(16, 185, 129, 0.05)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="70" y="345" font-size="16" fill="#10b981" text-anchor="start" font-weight="bold">Stage 1 (Narrow Transformations)</text>
                
                <!-- RDD Chain -->
                <rect x="80" y="370" width="120" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="140" y="395" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Read CSV</text>
                <text x="140" y="413" font-size="10" fill="#cbd5e1" text-anchor="middle">RDD[Row]</text>
                
                <path d="M 200 400 L 240 400" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                <text x="220" y="395" font-size="10" fill="#10b981" text-anchor="middle">filter</text>
                
                <rect x="240" y="370" width="120" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="300" y="395" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Filter age>25</text>
                <text x="300" y="413" font-size="10" fill="#cbd5e1" text-anchor="middle">RDD[Row]</text>
                
                <path d="M 360 400 L 400 400" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                <text x="380" y="395" font-size="10" fill="#10b981" text-anchor="middle">select</text>
                
                <rect x="400" y="370" width="120" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="460" y="395" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Select cols</text>
                <text x="460" y="413" font-size="10" fill="#cbd5e1" text-anchor="middle">RDD[Row]</text>
                
                <!-- Shuffle Boundary -->
                <rect x="540" y="320" width="300" height="250" fill="rgba(239, 68, 68, 0.05)" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" rx="12"/>
                <text x="555" y="345" font-size="16" fill="#ef4444" text-anchor="start" font-weight="bold">Stage 2 (Wide - Shuffle)</text>
                
                <path d="M 520 400 L 560 400" stroke="#ef4444" stroke-width="3" marker-end="url(#arrowred)" stroke-dasharray="5,5"/>
                <text x="540" y="390" font-size="11" fill="#ef4444" text-anchor="middle" font-weight="bold">SHUFFLE</text>
                <text x="540" y="403" font-size="10" fill="#ef4444" text-anchor="middle">groupBy</text>
                
                <rect x="560" y="370" width="120" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="620" y="395" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">GroupBy age</text>
                <text x="620" y="413" font-size="10" fill="#cbd5e1" text-anchor="middle">Count</text>
                
                <path d="M 680 400 L 710 400" stroke="#ef4444" stroke-width="2" marker-end="url(#arrowred)"/>
                <text x="695" y="395" font-size="10" fill="#ef4444" text-anchor="middle">sort</text>
                
                <rect x="710" y="370" width="110" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="765" y="395" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">OrderBy</text>
                <text x="765" y="413" font-size="10" fill="#cbd5e1" text-anchor="middle">Sorted</text>
                
                <!-- Tasks breakdown -->
                <text x="140" y="460" font-size="14" fill="#10b981" text-anchor="middle" font-weight="bold">Tasks (Parallel)</text>
                
                <rect x="90" y="475" width="100" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="140" y="495" font-size="11" fill="#cbd5e1" text-anchor="middle">Task 1 (P1)</text>
                
                <rect x="90" y="520" width="100" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="140" y="540" font-size="11" fill="#cbd5e1" text-anchor="middle">Task 2 (P2)</text>
                
                <rect x="250" y="475" width="100" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="300" y="495" font-size="11" fill="#cbd5e1" text-anchor="middle">Task 3 (P3)</text>
                
                <rect x="250" y="520" width="100" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="300" y="540" font-size="11" fill="#cbd5e1" text-anchor="middle">Task 4 (P4)</text>
                
                <text x="620" y="460" font-size="14" fill="#ef4444" text-anchor="middle" font-weight="bold">Tasks (Parallel)</text>
                
                <rect x="570" y="475" width="100" height="35" fill="rgba(239, 68, 68, 0.2)" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="620" y="495" font-size="11" fill="#cbd5e1" text-anchor="middle">Task 1 (P1)</text>
                
                <rect x="570" y="520" width="100" height="35" fill="rgba(239, 68, 68, 0.2)" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="620" y="540" font-size="11" fill="#cbd5e1" text-anchor="middle">Task 2 (P2)</text>
                
                <rect x="720" y="475" width="100" height="35" fill="rgba(239, 68, 68, 0.2)" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="770" y="495" font-size="11" fill="#cbd5e1" text-anchor="middle">Task 3 (P3)</text>
                
                <defs>
                    <marker id="arrowgreen" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                    </marker>
                    <marker id="arrowred" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#ef4444"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">DAG shows execution plan: Stages separated by shuffle operations, broken into parallel tasks</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-layer-group"></i>
            DAG Hierarchy
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-sitemap"></i> DAG  Stages  Tasks
            </div>
            <div style="margin-top: 1rem;">
                <strong style="color: var(--primary-color);">1. DAG (Job Level)</strong>
                <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                    Complete execution plan for an action. One action = one DAG = one Job.
                </p>
                
                <strong style="color: var(--success-color); margin-top: 1rem; display: block;">2. Stages (Within a Job)</strong>
                <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                    Subdivisions of DAG separated by shuffle operations. Narrow transformations = same stage. Wide transformation = new stage.
                </p>
                
                <strong style="color: var(--warning-color); margin-top: 1rem; display: block;">3. Tasks (Within a Stage)</strong>
                <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                    One task per partition. Tasks within a stage run in parallel on executors.
                </p>
            </div>

            <div class="example-box" style="margin-top: 1rem;">
                <pre><code># Example hierarchy
Job: df.groupBy("age").count().show()
   Stage 1: Read CSV  Filter  Select (narrow, 4 tasks for 4 partitions)
       Task 1: Process Partition 1
       Task 2: Process Partition 2
       Task 3: Process Partition 3
       Task 4: Process Partition 4
   Stage 2: GroupBy  Count  OrderBy (wide, 3 tasks for 3 output partitions)
       Task 1: Aggregate group 1
       Task 2: Aggregate group 2
       Task 3: Aggregate group 3</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How is it generated?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>DAG is generated when you call an action.</strong> Here's the process:<br><br>
                    <strong>1. During transformations (lazy):</strong><br>
                     Spark records transformations in a lineage<br>
                     Nothing is executed yet<br>
                     Just building a plan<br><br>
                    <strong>2. When action is called:</strong><br>
                     DAGScheduler analyzes the lineage<br>
                     Creates logical execution plan<br>
                     Identifies stages (separated by shuffles)<br>
                     Catalyst optimizer optimizes the plan<br>
                     Creates physical execution plan<br><br>
                    <strong>3. Execution:</strong><br>
                     DAG is divided into stages<br>
                     Stages are divided into tasks<br>
                     Tasks are sent to executors<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Transformations: DAG not generated yet (lazy)
df1 = df.filter(col("age") > 25)  # Recorded, not executed
df2 = df1.select("name")           # Recorded, not executed

# Action: DAG generated and executed now!
df2.show()  #  DAG created here and executed</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How does it relate to the concept of stages and tasks?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>DAG  Stages  Tasks</strong> (Top to bottom hierarchy)<br><br>
                    <strong>DAG contains Stages:</strong><br>
                     DAG is broken into stages at shuffle boundaries<br>
                     Each stage contains a sequence of narrow transformations<br>
                     Wide transformation (shuffle) = new stage<br><br>
                    <strong>Stages contain Tasks:</strong><br>
                     Each stage is divided into tasks (1 task per partition)<br>
                     Tasks are the smallest unit of work<br>
                     Tasks run in parallel on executors<br><br>
                    <strong>Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Code
df = spark.read.csv("data.csv")  # 4 partitions
df2 = df.filter(col("age") > 25)
df3 = df2.groupBy("dept").count()
df3.show()

# DAG (1 Job)
#  Stage 0: Read + Filter (narrow)
#     Task 0 (partition 0)
#     Task 1 (partition 1)
#     Task 2 (partition 2)
#     Task 3 (partition 3)
#  Stage 1: GroupBy + Count (wide - shuffle)
#      Task 0 (output partition 0)
#      Task 1 (output partition 1)
#      Task 2 (output partition 2)</code></pre>
                    <strong>Key relationships:</strong><br>
                     1 Action = 1 Job = 1 DAG<br>
                     1 Wide transformation = 1 new Stage<br>
                     1 Partition = 1 Task per stage<br>
                     Tasks execute in parallel on executors
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 66: Spark Execution Flow -->
<div class="question-content" id="q66">
    <div class="question-header">
        <h1 class="question-title">Explain Spark's execution flow</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Spark Execution Flow Overview
        </div>
        <div class="definition-box">
            <p><strong>Spark execution flow</strong> describes the complete journey from writing code to getting results. It involves lazy evaluation, DAG creation, optimization, and distributed execution.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Execution Flow</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Lazy Evaluation</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> DAG Scheduler</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Task Execution</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Complete Execution Flow Diagram
        </div>
        <div class="image-container">
            <svg width="900" height="700" viewBox="0 0 900 700">
                <rect width="900" height="700" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Spark Complete Execution Flow</text>
                
                <!-- Step 1: User Code -->
                <rect x="100" y="60" width="700" height="80" fill="rgba(59, 130, 246, 0.2)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="120" y="85" font-size="16" fill="#3b82f6" text-anchor="start" font-weight="bold">1. User Code (Transformations)</text>
                <text x="120" y="105" font-size="12" fill="#cbd5e1" font-family="monospace">df.filter(...).select(...).groupBy(...)</text>
                <text x="120" y="125" font-size="11" fill="#f59e0b" font-weight="bold"> Lazy Evaluation: Nothing executed yet!</text>
                
                <!-- Arrow -->
                <path d="M 450 140 L 450 170" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown)"/>
                
                <!-- Step 2: Action Triggered -->
                <rect x="100" y="170" width="700" height="60" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="120" y="195" font-size="16" fill="#10b981" text-anchor="start" font-weight="bold">2. Action Triggered</text>
                <text x="120" y="215" font-size="12" fill="#cbd5e1" font-family="monospace">df.show() / df.count() / df.collect()</text>
                
                <!-- Arrow -->
                <path d="M 450 230 L 450 260" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown)"/>
                
                <!-- Step 3: DAG Creation -->
                <rect x="100" y="260" width="700" height="80" fill="rgba(168, 85, 247, 0.2)" stroke="#a855f7" stroke-width="2" rx="12"/>
                <text x="120" y="285" font-size="16" fill="#a855f7" text-anchor="start" font-weight="bold">3. DAG Creation (DAGScheduler)</text>
                <text x="120" y="305" font-size="12" fill="#cbd5e1"> Analyzes lineage of transformations</text>
                <text x="120" y="323" font-size="12" fill="#cbd5e1"> Creates Directed Acyclic Graph (DAG)</text>
                
                <!-- Arrow -->
                <path d="M 450 340 L 450 370" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown)"/>
                
                <!-- Step 4: Optimization -->
                <rect x="100" y="370" width="700" height="80" fill="rgba(245, 158, 11, 0.2)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="120" y="395" font-size="16" fill="#f59e0b" text-anchor="start" font-weight="bold">4. Catalyst Optimizer</text>
                <text x="120" y="415" font-size="12" fill="#cbd5e1"> Logical plan optimization (predicate pushdown, etc.)</text>
                <text x="120" y="433" font-size="12" fill="#cbd5e1"> Physical plan selection (best execution strategy)</text>
                
                <!-- Arrow -->
                <path d="M 450 450 L 450 480" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown)"/>
                
                <!-- Step 5: Stage Division -->
                <rect x="100" y="480" width="700" height="80" fill="rgba(239, 68, 68, 0.2)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="120" y="505" font-size="16" fill="#ef4444" text-anchor="start" font-weight="bold">5. Divide into Stages</text>
                <text x="120" y="525" font-size="12" fill="#cbd5e1"> Split DAG at shuffle boundaries (wide transformations)</text>
                <text x="120" y="543" font-size="12" fill="#cbd5e1"> Each stage contains narrow transformations</text>
                
                <!-- Arrow -->
                <path d="M 450 560 L 450 590" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown)"/>
                
                <!-- Step 6: Task Creation -->
                <rect x="100" y="590" width="340" height="80" fill="rgba(14, 165, 233, 0.2)" stroke="#0ea5e9" stroke-width="2" rx="12"/>
                <text x="120" y="615" font-size="16" fill="#0ea5e9" text-anchor="start" font-weight="bold">6. Create Tasks</text>
                <text x="120" y="635" font-size="12" fill="#cbd5e1"> 1 task per partition</text>
                <text x="120" y="653" font-size="12" fill="#cbd5e1"> Tasks are smallest unit</text>
                
                <!-- Step 7: Execution -->
                <rect x="460" y="590" width="340" height="80" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="480" y="615" font-size="16" fill="#10b981" text-anchor="start" font-weight="bold">7. Execute on Executors</text>
                <text x="480" y="635" font-size="12" fill="#cbd5e1"> Tasks sent to executors</text>
                <text x="480" y="653" font-size="12" fill="#cbd5e1"> Parallel execution</text>
                
                <defs>
                    <marker id="arrowdown" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#10b981"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Complete flow from code to execution</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-list-ol"></i>
            Detailed Step-by-Step Flow
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-code"></i> Step 1: Write Code with Transformations
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>df = spark.read.csv("data.csv")
df2 = df.filter(col("age") > 25)     # Transformation (lazy)
df3 = df2.select("name", "age")      # Transformation (lazy)
df4 = df3.groupBy("age").count()     # Transformation (lazy)

# Nothing executed yet! Just building lineage.</code></pre>
                <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                    Transformations are lazy - they only record what needs to be done.
                </p>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-play"></i> Step 2: Action Triggers Execution
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>df4.show()  #  ACTION! This triggers everything</code></pre>
                <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                    Action tells Spark: "Now execute all the transformations and give me results"
                </p>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-project-diagram"></i> Step 3: DAGScheduler Creates DAG
            </div>
            <div style="margin-top: 1rem;">
                <p style="color: var(--text-secondary);">
                     Analyzes the lineage of transformations<br>
                     Creates a Directed Acyclic Graph (DAG)<br>
                     Identifies dependencies between operations<br>
                     Determines execution order
                </p>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-magic"></i> Step 4: Catalyst Optimizer Optimizes
            </div>
            <div style="margin-top: 1rem;">
                <p style="color: var(--text-secondary);">
                    <strong>Logical Optimization:</strong><br>
                     Predicate pushdown (filter early)<br>
                     Column pruning (select only needed columns)<br>
                     Constant folding<br>
                     Boolean expression simplification<br><br>
                    <strong>Physical Optimization:</strong><br>
                     Choose join strategy (broadcast vs shuffle)<br>
                     Select best algorithm for operations
                </p>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-layer-group"></i> Step 5: Divide into Stages
            </div>
            <div style="margin-top: 1rem;">
                <p style="color: var(--text-secondary);">
                     DAG is split at shuffle boundaries<br>
                     Wide transformations (groupBy, join) create new stages<br>
                     Narrow transformations (filter, select) stay in same stage<br>
                     Stages execute sequentially (stage 1  stage 2  stage 3)
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>Stage 0: Read CSV  Filter  Select (narrow)
          SHUFFLE (groupBy)
Stage 1: GroupBy  Count (wide)</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-tasks"></i> Step 6: Create Tasks
            </div>
            <div style="margin-top: 1rem;">
                <p style="color: var(--text-secondary);">
                     Each stage is divided into tasks<br>
                     1 task per partition<br>
                     If DataFrame has 10 partitions  10 tasks per stage<br>
                     TaskScheduler manages task execution
                </p>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-server"></i> Step 7: Execute on Executors
            </div>
            <div style="margin-top: 1rem;">
                <p style="color: var(--text-secondary);">
                     Driver sends tasks to executors<br>
                     Executors run tasks in parallel<br>
                     Each task processes one partition<br>
                     Results sent back to driver (for collect/show)<br>
                     Or written to storage (for save operations)
                </p>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What is a DAG in Spark?</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 65 for detailed answer. <strong>Summary:</strong> DAG (Directed Acyclic Graph) is the execution plan showing the sequence of operations. It's created when an action is called, showing how transformations are connected.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens after you call an action?</strong></span>
                </div>
                <div class="cross-answer">
                    After calling an action:<br><br>
                    <strong>1. DAG Creation:</strong> DAGScheduler creates execution plan from lineage<br>
                    <strong>2. Optimization:</strong> Catalyst Optimizer optimizes the plan<br>
                    <strong>3. Stage Division:</strong> DAG split into stages at shuffle boundaries<br>
                    <strong>4. Task Creation:</strong> Stages divided into tasks (1 per partition)<br>
                    <strong>5. Scheduling:</strong> TaskScheduler assigns tasks to executors<br>
                    <strong>6. Execution:</strong> Executors run tasks in parallel<br>
                    <strong>7. Result Collection:</strong> Results gathered by driver or written to storage<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Before action: nothing happens
df2 = df.filter(col("age") > 25)  # Just recorded

# Action: triggers entire pipeline
result = df2.collect()  # NOW everything executes!</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 67: Caching and Persistence -->
<div class="question-content" id="q67">
    <div class="question-header">
        <h1 class="question-title">What is caching and persistence in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Caching/Persistence</strong> stores intermediate results in memory (or disk) so they can be reused without recomputation. Essential for iterative algorithms or when using the same DataFrame multiple times.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> cache()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> persist()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Storage Levels</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance Optimization</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            When to Use Caching
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-stopwatch"></i> Without Caching (Slow)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>df = spark.read.csv("large_file.csv")  # 10GB file
df_filtered = df.filter(col("age") > 25)

# First action: reads entire file, applies filter
count1 = df_filtered.count()  # Takes 30 seconds

# Second action: reads entire file AGAIN, applies filter AGAIN
count2 = df_filtered.filter(col("salary") > 50000).count()  # Takes 30 seconds

# Third action: reads entire file AGAIN!
df_filtered.show()  # Takes 30 seconds

# Total time: 90 seconds (file read 3 times!)</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-rocket"></i> With Caching (Fast)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>df = spark.read.csv("large_file.csv")
df_filtered = df.filter(col("age") > 25)

# Cache the filtered result
df_filtered.cache()  # or df_filtered.persist()

# First action: reads file, applies filter, stores in memory
count1 = df_filtered.count()  # Takes 30 seconds + caching time

# Second action: uses cached data!
count2 = df_filtered.filter(col("salary") > 50000).count()  # Takes 2 seconds

# Third action: uses cached data!
df_filtered.show()  # Takes 2 seconds

# Total time: ~35 seconds (file read once, subsequent operations fast!)

# Don't forget to unpersist when done
df_filtered.unpersist()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            cache() vs persist()
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-database"></i> Using cache() and persist()</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.storagelevel import StorageLevel

df = spark.read.csv("data.csv")
df_processed = df.filter(col("age") > 25).select("name", "age")

# Method 1: cache() - default storage level (MEMORY_AND_DISK)
df_processed.cache()

# Method 2: persist() with default storage level (same as cache)
df_processed.persist()

# Method 3: persist() with specific storage level
df_processed.persist(StorageLevel.MEMORY_ONLY)
df_processed.persist(StorageLevel.DISK_ONLY)
df_processed.persist(StorageLevel.MEMORY_AND_DISK)
df_processed.persist(StorageLevel.MEMORY_ONLY_2)  # Replicated twice

# Use the cached DataFrame
df_processed.count()  # Triggers caching
df_processed.show()   # Uses cached data

# Check if cached
print(df_processed.is_cached)  # True

# Remove from cache when done
df_processed.unpersist()
print(df_processed.is_cached)  # False</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            When to Use Caching
        </div>

        <div class="highlight-box">
            <strong> Use caching when:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>DataFrame is used multiple times</li>
                <li>Iterative algorithms (machine learning)</li>
                <li>Interactive analysis (exploring data)</li>
                <li>Expensive computations that don't need to be repeated</li>
                <li>After wide transformations (shuffle) that you'll reuse</li>
            </ul>
        </div>

        <div class="warning-box">
            <strong> Don't use caching when:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>DataFrame is used only once</li>
                <li>Not enough memory available</li>
                <li>Very large datasets that won't fit in memory</li>
                <li>Simple, fast operations</li>
            </ul>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What is the difference between .cache() and .persist()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>cache():</strong> Shorthand for <code>persist(StorageLevel.MEMORY_AND_DISK)</code>. Uses default storage level.<br><br>
                    <strong>persist():</strong> Allows you to specify storage level explicitly. More flexible.<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># These are equivalent:
df.cache()
df.persist()
df.persist(StorageLevel.MEMORY_AND_DISK)

# persist() gives you control:
df.persist(StorageLevel.MEMORY_ONLY)      # Memory only
df.persist(StorageLevel.DISK_ONLY)        # Disk only
df.persist(StorageLevel.MEMORY_ONLY_2)    # Memory, replicated 2x</code></pre>
                    <strong>Key Difference:</strong><br>
                     <code>cache()</code> = Simple, uses default storage<br>
                     <code>persist()</code> = Flexible, choose your storage level<br><br>
                    <strong>Recommendation:</strong> Use <code>cache()</code> for most cases. Use <code>persist()</code> when you need specific storage control.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 68: Default Storage Level -->
<div class="question-content" id="q68">
    <div class="question-header">
        <h1 class="question-title">What is the default storage level of .cache()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Default Storage Level
        </div>
        <div class="definition-box">
            <p><strong>Default storage level for cache():</strong> <code>MEMORY_AND_DISK</code></p>
            <p style="margin-top: 1rem;">This means: Try to store in memory first. If memory is full, spill to disk.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> MEMORY_AND_DISK</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Storage Levels</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Deserialized</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            All Available Storage Levels
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Storage Level</th>
                        <th>Memory</th>
                        <th>Disk</th>
                        <th>Serialized</th>
                        <th>Replicated</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>MEMORY_ONLY</strong></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>1x</td>
                        <td>Fast, but data lost if memory full</td>
                    </tr>
                    <tr>
                        <td><strong>MEMORY_AND_DISK</strong> </td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>1x</td>
                        <td>DEFAULT - safe, fast when in memory</td>
                    </tr>
                    <tr>
                        <td><strong>MEMORY_ONLY_SER</strong></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>1x</td>
                        <td>Less memory, more CPU</td>
                    </tr>
                    <tr>
                        <td><strong>MEMORY_AND_DISK_SER</strong></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>1x</td>
                        <td>Compact storage</td>
                    </tr>
                    <tr>
                        <td><strong>DISK_ONLY</strong></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>1x</td>
                        <td>When memory is scarce</td>
                    </tr>
                    <tr>
                        <td><strong>MEMORY_ONLY_2</strong></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>2x</td>
                        <td>Fault tolerance</td>
                    </tr>
                    <tr>
                        <td><strong>MEMORY_AND_DISK_2</strong></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>2x</td>
                        <td>Safe + fault tolerant</td>
                    </tr>
                    <tr>
                        <td><strong>OFF_HEAP</strong></td>
                        <td>Off-heap</td>
                        <td></td>
                        <td></td>
                        <td>1x</td>
                        <td>Advanced: avoid GC</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p style="margin-top: 1rem; color: var(--text-secondary);">
             = Default level for <code>cache()</code>
        </p>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Using Different Storage Levels
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-layer-group"></i> Storage Level Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.storagelevel import StorageLevel

df = spark.read.csv("data.csv")

# 1. MEMORY_ONLY - fastest but risky
df.persist(StorageLevel.MEMORY_ONLY)
# If memory full: partitions evicted, recomputed on next use

# 2. MEMORY_AND_DISK - default, safest
df.persist(StorageLevel.MEMORY_AND_DISK)
df.cache()  # Same as above
# If memory full: spills to disk

# 3. MEMORY_ONLY_SER - compact, slower access
df.persist(StorageLevel.MEMORY_ONLY_SER)
# Serialized: uses less memory but requires deserialization

# 4. DISK_ONLY - when memory is scarce
df.persist(StorageLevel.DISK_ONLY)
# All data on disk: slower but doesn't use memory

# 5. MEMORY_ONLY_2 - replicated for fault tolerance
df.persist(StorageLevel.MEMORY_ONLY_2)
# Stored on 2 nodes: if one fails, data still available

# Check storage level
print(df.storageLevel)  # Shows current storage level

# Unpersist to free resources
df.unpersist()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What are other storage levels available?</strong></span>
                </div>
                <div class="cross-answer">
                    See the table above for all 8 storage levels. Most commonly used:<br><br>
                     <strong>MEMORY_AND_DISK</strong> (default): Safe, fast<br>
                     <strong>MEMORY_ONLY</strong>: Fastest, but can lose data<br>
                     <strong>DISK_ONLY</strong>: Slowest, uses no memory<br>
                     <strong>MEMORY_ONLY_SER</strong>: Compact, more CPU<br>
                     <strong>_2 variants</strong>: Replicated for fault tolerance
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How does it affect memory and disk usage?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Memory Impact:</strong><br>
                     <strong>Deserialized</strong> (MEMORY_ONLY, MEMORY_AND_DISK): Uses more memory, faster access<br>
                     <strong>Serialized</strong> (_SER variants): Uses less memory (compressed), slower access<br>
                     <strong>Replication</strong> (_2 variants): Uses 2x memory/disk<br><br>
                    <strong>Disk Impact:</strong><br>
                     <strong>MEMORY_ONLY</strong>: No disk usage<br>
                     <strong>MEMORY_AND_DISK</strong>: Disk used only when memory full<br>
                     <strong>DISK_ONLY</strong>: All data on disk<br><br>
                    <strong>Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 10 GB DataFrame
df.persist(StorageLevel.MEMORY_ONLY)
# Memory: 10 GB, Disk: 0 GB

df.persist(StorageLevel.MEMORY_AND_DISK)
# Memory: up to available, Disk: overflow

df.persist(StorageLevel.MEMORY_ONLY_SER)
# Memory: ~3-5 GB (compressed), Disk: 0 GB

df.persist(StorageLevel.DISK_ONLY)
# Memory: 0 GB, Disk: 10 GB

df.persist(StorageLevel.MEMORY_ONLY_2)
# Memory: 20 GB (2 copies), Disk: 0 GB</code></pre>
                    <strong>Best Practice:</strong> Start with default (MEMORY_AND_DISK). If memory is constrained, use MEMORY_AND_DISK_SER. If data is critical, use _2 variants.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 69: MEMORY_ONLY vs DISK_ONLY -->
<div class="question-content" id="q69">
    <div class="question-header">
        <h1 class="question-title">What is the difference between persist(StorageLevel.MEMORY_ONLY) and persist(StorageLevel.DISK_ONLY)?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            MEMORY_ONLY vs DISK_ONLY
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-memory"></i> MEMORY_ONLY</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Storage:</strong> RAM only</li>
                    <li><strong>Speed:</strong> Fastest (in-memory)</li>
                    <li><strong>Capacity:</strong> Limited by RAM</li>
                    <li><strong>Overflow:</strong> Partitions evicted (recomputed later)</li>
                    <li><strong>Fault Tolerance:</strong> Recompute if lost</li>
                    <li><strong>Use:</strong> Iterative algorithms, small datasets</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code>df.persist(
    StorageLevel.MEMORY_ONLY
)

# Fast access
# Risk: data loss if memory full</code></pre>
                </div>
            </div>

            <div class="comparison-card narrow">
                <h4><i class="fas fa-hdd"></i> DISK_ONLY</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li><strong>Storage:</strong> Disk only</li>
                    <li><strong>Speed:</strong> Slower (disk I/O)</li>
                    <li><strong>Capacity:</strong> Large (disk space)</li>
                    <li><strong>Overflow:</strong> No overflow issue</li>
                    <li><strong>Fault Tolerance:</strong> Data persists on disk</li>
                    <li><strong>Use:</strong> Large datasets, memory constrained</li>
                </ul>
                <div class="example-box" style="margin-top: 1rem;">
                    <pre><code>df.persist(
    StorageLevel.DISK_ONLY
)

# Slower access
# Safe: all data stored</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Detailed Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>MEMORY_ONLY</th>
                        <th>DISK_ONLY</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Storage Location</strong></td>
                        <td>Executor JVM heap (RAM)</td>
                        <td>Executor local disk</td>
                    </tr>
                    <tr>
                        <td><strong>Access Speed</strong></td>
                        <td>Very fast (~ns access)</td>
                        <td>Slow (~ms access)</td>
                    </tr>
                    <tr>
                        <td><strong>Capacity</strong></td>
                        <td>Limited (~GBs per executor)</td>
                        <td>Large (~TBs available)</td>
                    </tr>
                    <tr>
                        <td><strong>If Full</strong></td>
                        <td>Partitions evicted (LRU)</td>
                        <td>Can store everything</td>
                    </tr>
                    <tr>
                        <td><strong>Evicted Data</strong></td>
                        <td>Recomputed on next use</td>
                        <td>N/A (no eviction)</td>
                    </tr>
                    <tr>
                        <td><strong>Serialization</strong></td>
                        <td>Deserialized (Java objects)</td>
                        <td>Serialized (bytes)</td>
                    </tr>
                    <tr>
                        <td><strong>CPU Overhead</strong></td>
                        <td>Low (direct access)</td>
                        <td>Medium (deserialization needed)</td>
                    </tr>
                    <tr>
                        <td><strong>Best For</strong></td>
                        <td>Small/Medium datasets, iterative</td>
                        <td>Large datasets, memory scarce</td>
                    </tr>
                    <tr>
                        <td><strong>Performance</strong></td>
                        <td>Excellent (10-100x faster)</td>
                        <td>Moderate (better than recompute)</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Example
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-stopwatch"></i> Performance Comparison</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>import time

df = spark.range(10000000).toDF("id")
df_transformed = df.selectExpr("id * 2 as double_id")

# Test 1: MEMORY_ONLY
df_memory = df_transformed.persist(StorageLevel.MEMORY_ONLY)
start = time.time()
df_memory.count()  # First action: compute and cache
first_time = time.time() - start

start = time.time()
df_memory.count()  # Second action: use cache
second_time = time.time() - start
print(f"MEMORY_ONLY - First: {first_time:.2f}s, Second: {second_time:.2f}s")
df_memory.unpersist()

# Test 2: DISK_ONLY
df_disk = df_transformed.persist(StorageLevel.DISK_ONLY)
start = time.time()
df_disk.count()  # First action: compute and write to disk
first_time = time.time() - start

start = time.time()
df_disk.count()  # Second action: read from disk
second_time = time.time() - start
print(f"DISK_ONLY - First: {first_time:.2f}s, Second: {second_time:.2f}s")
df_disk.unpersist()

# Typical results:
# MEMORY_ONLY - First: 2.5s, Second: 0.1s   Very fast second access
# DISK_ONLY - First: 3.0s, Second: 1.5s    Slower second access</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How do you check storage level of an RDD?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>.storageLevel</code> or <code>.getStorageLevel()</code>:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># For DataFrame
df.persist(StorageLevel.MEMORY_ONLY)
df.count()  # Trigger caching
print(df.storageLevel)
# Output: StorageLevel(True, False, False, False, 1)
#         (useDisk, useMemory, useOffHeap, deserialized, replication)

# For RDD
rdd = spark.sparkContext.parallelize([1, 2, 3])
rdd.persist(StorageLevel.MEMORY_AND_DISK)
print(rdd.getStorageLevel())
# Output: Memory Serialized 1x Replicated

# Check if cached
print(df.is_cached)  # True or False

# Get detailed storage info from Spark UI
# Go to: http://localhost:4040  Storage tab</code></pre>
                    <strong>Understanding the output:</strong><br>
                     <code>StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication)</code><br>
                     <code>True, False, False, False, 1</code> = MEMORY_ONLY<br>
                     <code>True, True, False, False, 1</code> = MEMORY_AND_DISK<br>
                     <code>True, False, False, False, 2</code> = MEMORY_ONLY_2
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 70: Wide and Narrow Transformations -->
<div class="question-content" id="q70">
    <div class="question-header">
        <h1 class="question-title">What are wide and narrow transformations?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Narrow Transformations:</strong> Each input partition contributes to only one output partition. No data shuffle across partitions. Fast and efficient.</p>
            <p style="margin-top: 1rem;"><strong>Wide Transformations:</strong> Each input partition contributes to multiple output partitions. Requires shuffle (data exchange across network). Expensive operation.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Narrow</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Wide</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Shuffle</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Partitions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Visual Comparison
        </div>
        <div class="image-container">
            <svg width="900" height="500" viewBox="0 0 900 500">
                <rect width="900" height="500" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Narrow vs Wide Transformations</text>
                
                <!-- Narrow Transformation -->
                <rect x="50" y="60" width="380" height="400" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="240" y="90" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">Narrow Transformation</text>
                <text x="240" y="110" font-size="12" fill="#cbd5e1" text-anchor="middle">1-to-1 or 1-to-Few (No Shuffle)</text>
                
                <!-- Input Partitions -->
                <text x="100" y="145" font-size="14" fill="#cbd5e1" text-anchor="start" font-weight="bold">Input RDD</text>
                <rect x="90" y="160" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="140" y="190" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 1</text>
                
                <rect x="90" y="220" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="140" y="250" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 2</text>
                
                <rect x="90" y="280" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="140" y="310" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 3</text>
                
                <rect x="90" y="340" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="140" y="370" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 4</text>
                
                <!-- Arrows (1-to-1) -->
                <path d="M 190 185 L 230 185" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                <path d="M 190 245 L 230 245" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                <path d="M 190 305 L 230 305" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                <path d="M 190 365 L 230 365" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen)"/>
                
                <!-- Output Partitions -->
                <text x="290" y="145" font-size="14" fill="#cbd5e1" text-anchor="start" font-weight="bold">Output RDD</text>
                <rect x="230" y="160" width="100" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="6"/>
                <text x="280" y="190" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 1</text>
                
                <rect x="230" y="220" width="100" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="6"/>
                <text x="280" y="250" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 2</text>
                
                <rect x="230" y="280" width="100" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="6"/>
                <text x="280" y="310" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 3</text>
                
                <rect x="230" y="340" width="100" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="6"/>
                <text x="280" y="370" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 4</text>
                
                <text x="240" y="430" font-size="13" fill="#10b981" text-anchor="middle" font-weight="bold">Examples: map(), filter(), select()</text>
                
                <!-- Wide Transformation -->
                <rect x="470" y="60" width="380" height="400" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="660" y="90" font-size="18" fill="#ef4444" text-anchor="middle" font-weight="bold">Wide Transformation</text>
                <text x="660" y="110" font-size="12" fill="#cbd5e1" text-anchor="middle">Many-to-Many (Shuffle Required)</text>
                
                <!-- Input Partitions -->
                <text x="520" y="145" font-size="14" fill="#cbd5e1" text-anchor="start" font-weight="bold">Input RDD</text>
                <rect x="510" y="160" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="560" y="190" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 1</text>
                
                <rect x="510" y="220" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="560" y="250" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 2</text>
                
                <rect x="510" y="280" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="560" y="310" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 3</text>
                
                <rect x="510" y="340" width="100" height="50" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="6"/>
                <text x="560" y="370" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 4</text>
                
                <!-- Shuffle Arrows (Many-to-Many) -->
                <path d="M 610 185 L 650 185" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 185 L 650 245" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 245 L 650 185" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 245 L 650 245" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 245 L 650 305" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 305 L 650 245" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 305 L 650 305" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 365 L 650 305" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                <path d="M 610 365 L 650 365" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowred)"/>
                
                <text x="630" y="200" font-size="11" fill="#f59e0b" text-anchor="middle" font-weight="bold">SHUFFLE</text>
                
                <!-- Output Partitions -->
                <text x="710" y="145" font-size="14" fill="#cbd5e1" text-anchor="start" font-weight="bold">Output RDD</text>
                <rect x="650" y="160" width="100" height="50" fill="rgba(239, 68, 68, 0.3)" stroke="#ef4444" stroke-width="2" rx="6"/>
                <text x="700" y="190" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 1</text>
                
                <rect x="650" y="220" width="100" height="50" fill="rgba(239, 68, 68, 0.3)" stroke="#ef4444" stroke-width="2" rx="6"/>
                <text x="700" y="250" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 2</text>
                
                <rect x="650" y="280" width="100" height="50" fill="rgba(239, 68, 68, 0.3)" stroke="#ef4444" stroke-width="2" rx="6"/>
                <text x="700" y="310" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 3</text>
                
                <rect x="650" y="340" width="100" height="50" fill="rgba(239, 68, 68, 0.3)" stroke="#ef4444" stroke-width="2" rx="6"/>
                <text x="700" y="370" font-size="12" fill="#f1f5f9" text-anchor="middle">Partition 4</text>
                
                <text x="660" y="430" font-size="13" fill="#ef4444" text-anchor="middle" font-weight="bold">Examples: groupBy(), join(), reduceByKey()</text>
                
                <defs>
                    <marker id="arrowgreen" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                    </marker>
                    <marker id="arrowred" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#ef4444"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Narrow: 1-to-1 mapping (fast) | Wide: Many-to-many with shuffle (expensive)</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Detailed Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Narrow Transformation</th>
                        <th>Wide Transformation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Data Movement</strong></td>
                        <td>No shuffle, data stays in partition</td>
                        <td>Shuffle required, data moves across network</td>
                    </tr>
                    <tr>
                        <td><strong>Dependency</strong></td>
                        <td>1 input partition  1 output partition</td>
                        <td>Multiple input partitions  1 output partition</td>
                    </tr>
                    <tr>
                        <td><strong>Performance</strong></td>
                        <td>Fast (no network I/O)</td>
                        <td>Slow (network + disk I/O)</td>
                    </tr>
                    <tr>
                        <td><strong>Stage Boundary</strong></td>
                        <td>No new stage</td>
                        <td>Creates new stage</td>
                    </tr>
                    <tr>
                        <td><strong>Pipelining</strong></td>
                        <td>Can be pipelined together</td>
                        <td>Cannot be pipelined with previous ops</td>
                    </tr>
                    <tr>
                        <td><strong>Failure Recovery</strong></td>
                        <td>Fast (recompute only failed partition)</td>
                        <td>Slower (may need to recompute multiple partitions)</td>
                    </tr>
                    <tr>
                        <td><strong>Examples</strong></td>
                        <td>map(), filter(), select(), union()</td>
                        <td>groupBy(), join(), repartition(), distinct()</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-check-circle"></i> Narrow Transformation Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

df = spark.read.csv("data.csv")

# 1. map() - narrow (1-to-1)
rdd = spark.sparkContext.parallelize([1, 2, 3, 4])
mapped = rdd.map(lambda x: x * 2)  # Each partition processes independently

# 2. filter() - narrow (1-to-0 or 1-to-1)
filtered = df.filter(col("age") > 25)  # Each partition filters independently

# 3. select() - narrow (column projection)
selected = df.select("name", "age")  # Each partition selects columns independently

# 4. union() - narrow (just combines partitions)
df2 = spark.read.csv("data2.csv")
unioned = df.union(df2)  # No shuffle, just appends partitions

# 5. mapPartitions() - narrow
def process_partition(iterator):
    return [x * 2 for x in iterator]
result = rdd.mapPartitions(process_partition)

# All these operations:
# - Process data within each partition
# - No data exchange between partitions
# - Fast and efficient
# - Can be pipelined together in single stage</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-exclamation-triangle"></i> Wide Transformation Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># 1. groupBy() - wide (requires shuffle)
grouped = df.groupBy("department").count()
# Data with same key must be in same partition  SHUFFLE

# 2. join() - wide (requires shuffle for both DataFrames)
df1 = spark.read.csv("employees.csv")
df2 = spark.read.csv("departments.csv")
joined = df1.join(df2, "dept_id")
# Both DataFrames reshuffled by join key

# 3. repartition() - wide (explicitly redistributes data)
repartitioned = df.repartition(10)  # Redistributes to 10 partitions

# 4. distinct() - wide (needs to check all partitions)
unique = df.distinct()  # Must shuffle to find duplicates across partitions

# 5. reduceByKey() - wide (aggregation by key)
pairs = spark.sparkContext.parallelize([("a", 1), ("b", 2), ("a", 3)])
reduced = pairs.reduceByKey(lambda x, y: x + y)
# All values for key "a" must be in same partition

# 6. sortBy() / orderBy() - wide (global sorting)
sorted_df = df.orderBy("age")  # Needs all data to determine global order

# All these operations:
# - Move data across network (shuffle)
# - Create stage boundaries
# - Expensive in terms of performance
# - Cannot be pipelined with previous operations</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Give one example of each</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Narrow Transformation:</strong> <code>filter()</code> - Each partition filters its own data independently, no data exchange needed.
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.filter(col("age") > 25)  # Narrow - no shuffle</code></pre>
                    <strong>Wide Transformation:</strong> <code>groupBy()</code> - Data with same key must be brought together from different partitions, requires shuffle.
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.groupBy("department").count()  # Wide - shuffle required</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Why are wide transformations expensive?</strong></span>
                </div>
                <div class="cross-answer">
                    Wide transformations are expensive because they require **shuffle** - redistributing data across partitions. This involves:<br><br>
                    <strong>1. Disk I/O:</strong> Write intermediate data to disk on each executor<br>
                    <strong>2. Network I/O:</strong> Transfer data across network between executors<br>
                    <strong>3. Serialization/Deserialization:</strong> Convert objects to bytes and back<br>
                    <strong>4. Memory Pressure:</strong> Buffering data during shuffle<br>
                    <strong>5. Stage Barrier:</strong> Can't proceed to next stage until shuffle completes<br><br>
                    <strong>Example Performance Impact:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Narrow - fast (1 second)
df.filter(col("age") > 25).select("name").count()

# Wide - slow (30 seconds for same data)
df.groupBy("age").count().orderBy("count").show()
# Reasons: 2 shuffles (groupBy + orderBy), disk + network I/O</code></pre>
                    <strong>Cost Breakdown:</strong><br>
                     Disk write: 50% of shuffle time<br>
                     Network transfer: 30% of shuffle time<br>
                     Deserialization: 20% of shuffle time<br><br>
                    <strong>Mitigation:</strong> Minimize shuffles by filtering early, using broadcast joins, and caching intermediate results.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 71: Narrow vs Wide Difference -->
<div class="question-content" id="q71">
    <div class="question-header">
        <h1 class="question-title">What is the difference between narrow and wide transformations?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Quick Summary
        </div>
        <div class="definition-box">
            <p><strong>Narrow:</strong> Each input partition contributes to at most one output partition. No shuffle. Examples: map(), filter(), select().</p>
            <p style="margin-top: 1rem;"><strong>Wide:</strong> Each input partition contributes to multiple output partitions. Requires shuffle. Examples: groupBy(), join(), distinct().</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-balance-scale"></i>
            Key Differences
        </div>

        <div class="comparison-grid">
            <div class="comparison-card narrow">
                <h4><i class="fas fa-arrow-right"></i> Narrow</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>No data movement between partitions</li>
                    <li>Operations done in parallel independently</li>
                    <li>Fast execution</li>
                    <li>Single stage</li>
                    <li>Simple recovery on failure</li>
                </ul>
            </div>

            <div class="comparison-card narrow">
                <h4><i class="fas fa-exchange-alt"></i> Wide</h4>
                <ul style="color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                    <li>Data shuffled across partitions</li>
                    <li>Operations require coordination</li>
                    <li>Slow execution (network I/O)</li>
                    <li>Creates new stage boundary</li>
                    <li>Complex recovery (re-shuffle)</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which one triggers a shuffle?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Wide transformations</strong> trigger shuffles. Narrow transformations do NOT trigger shuffles.<br><br>
                    <strong>Shuffle-triggering operations (Wide):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># All these trigger shuffles:
df.groupBy("column").count()        # Wide
df.distinct()                        # Wide
df1.join(df2, "key")                # Wide
df.repartition(10)                   # Wide
df.orderBy("column")                 # Wide
rdd.reduceByKey(lambda x,y: x+y)    # Wide

# These do NOT trigger shuffles:
df.filter(col("age") > 25)          # Narrow
df.select("name", "age")            # Narrow
df.map(lambda x: x * 2)             # Narrow</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Example of a wide transformation?</strong></span>
                </div>
                <div class="cross-answer">
                    <code>groupBy()</code> is a classic wide transformation:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df = spark.read.csv("employees.csv")
# Wide transformation - requires shuffle
result = df.groupBy("department").agg(avg("salary"))

# Why it's wide:
# - Employees from same dept are in different partitions
# - Must bring all dept="Sales" records together
# - Requires data movement across network (shuffle)</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 72: Shuffling in Spark -->
<div class="question-content" id="q72">
    <div class="question-header">
        <h1 class="question-title">What is shuffling in Spark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 15 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Shuffling</strong> is the process of redistributing data across partitions. It involves writing data to disk, transferring it across the network, and reading it back. This is one of the most expensive operations in Spark.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Shuffle</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Redistribution</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Network I/O</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Stage Boundary</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Shuffle Process Visualization
        </div>
        <div class="image-container">
            <svg width="900" height="600" viewBox="0 0 900 600">
                <rect width="900" height="600" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Shuffle Process in Spark</text>
                
                <!-- Stage 1: Map Side -->
                <rect x="50" y="60" width="350" height="220" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="225" y="90" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">Stage 1: Map Side (Write)</text>
                
                <!-- Executors with data -->
                <rect x="80" y="110" width="130" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="145" y="135" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 1</text>
                <text x="145" y="155" font-size="11" fill="#cbd5e1" text-anchor="middle">{A:1, B:2, C:3}</text>
                
                <rect x="80" y="180" width="130" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="145" y="205" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 2</text>
                <text x="145" y="225" font-size="11" fill="#cbd5e1" text-anchor="middle">{A:4, B:5, C:6}</text>
                
                <!-- Shuffle Write -->
                <rect x="240" y="110" width="130" height="130" fill="rgba(245, 158, 11, 0.2)" stroke="#f59e0b" stroke-width="2" rx="8"/>
                <text x="305" y="135" font-size="13" fill="#f59e0b" text-anchor="middle" font-weight="bold">Shuffle Write</text>
                <text x="305" y="155" font-size="10" fill="#cbd5e1" text-anchor="middle">1. Hash by key</text>
                <text x="305" y="175" font-size="10" fill="#cbd5e1" text-anchor="middle">2. Sort locally</text>
                <text x="305" y="195" font-size="10" fill="#cbd5e1" text-anchor="middle">3. Write to disk</text>
                <text x="305" y="215" font-size="10" fill="#cbd5e1" text-anchor="middle">4. Create shuffle files</text>
                
                <!-- Network Transfer -->
                <rect x="50" y="300" width="800" height="80" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="450" y="330" font-size="16" fill="#ef4444" text-anchor="middle" font-weight="bold">Network Transfer (Shuffle)</text>
                <text x="450" y="355" font-size="12" fill="#cbd5e1" text-anchor="middle">Data moved across network from map tasks to reduce tasks</text>
                
                <!-- Stage 2: Reduce Side -->
                <rect x="500" y="400" width="350" height="180" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="675" y="430" font-size="16" fill="#3b82f6" text-anchor="middle" font-weight="bold">Stage 2: Reduce Side (Read)</text>
                
                <!-- Shuffle Read -->
                <rect x="530" y="450" width="130" height="110" fill="rgba(168, 85, 247, 0.2)" stroke="#a855f7" stroke-width="2" rx="8"/>
                <text x="595" y="475" font-size="13" fill="#a855f7" text-anchor="middle" font-weight="bold">Shuffle Read</text>
                <text x="595" y="495" font-size="10" fill="#cbd5e1" text-anchor="middle">1. Fetch from network</text>
                <text x="595" y="515" font-size="10" fill="#cbd5e1" text-anchor="middle">2. Deserialize</text>
                <text x="595" y="535" font-size="10" fill="#cbd5e1" text-anchor="middle">3. Merge sort</text>
                
                <!-- Final Executors -->
                <rect x="690" y="450" width="130" height="50" fill="#334155" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="755" y="472" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 3</text>
                <text x="755" y="490" font-size="11" fill="#cbd5e1" text-anchor="middle">{A: [1,4]}  5</text>
                
                <rect x="690" y="510" width="130" height="50" fill="#334155" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="755" y="532" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 4</text>
                <text x="755" y="550" font-size="11" fill="#cbd5e1" text-anchor="middle">{B: [2,5], C: [3,6]}</text>
                
                <!-- Arrows -->
                <path d="M 210 170 L 240 170" stroke="#f59e0b" stroke-width="2" marker-end="url(#arroworange)"/>
                <path d="M 305 240 L 305 300" stroke="#ef4444" stroke-width="3" marker-end="url(#arrowred)" stroke-dasharray="5,5"/>
                <text x="320" y="275" font-size="12" fill="#ef4444" text-anchor="start" font-weight="bold">NETWORK</text>
                <path d="M 595 380 L 595 450" stroke="#a855f7" stroke-width="2" marker-end="url(#arrowpurple)"/>
                <path d="M 660 500 L 690 500" stroke="#3b82f6" stroke-width="2" marker-end="url(#arrowblue)"/>
                
                <defs>
                    <marker id="arroworange" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#f59e0b"/>
                    </marker>
                    <marker id="arrowred" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#ef4444"/>
                    </marker>
                    <marker id="arrowpurple" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#a855f7"/>
                    </marker>
                    <marker id="arrowblue" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#3b82f6"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Shuffle: Write to disk  Transfer over network  Read and merge</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Shuffle Example
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-exchange-alt"></i> Understanding Shuffle</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, sum as spark_sum

# Create DataFrame with 4 partitions
data = [
    ("Alice", "Sales", 5000),
    ("Bob", "Engineering", 7000),
    ("Charlie", "Sales", 6000),
    ("David", "Engineering", 8000),
    ("Eve", "Sales", 5500),
    ("Frank", "Engineering", 7500)
]
df = spark.createDataFrame(data, ["name", "dept", "salary"]).repartition(4)

print(f"Initial partitions: {df.rdd.getNumPartitions()}")  # 4

# groupBy triggers SHUFFLE
result = df.groupBy("dept").agg(spark_sum("salary").alias("total_salary"))

# What happens during shuffle:
# 1. Map Phase (Stage 1):
#    - Each partition processes its data
#    - Records hashed by "dept" key
#    - Written to local disk (shuffle files)
#    Partition 1: Sales records  shuffle file A
#    Partition 2: Engineering records  shuffle file B
#    Partition 3: Sales records  shuffle file A
#    Partition 4: Engineering records  shuffle file B

# 2. Network Transfer:
#    - All Sales records fetched to one reducer
#    - All Engineering records fetched to another reducer

# 3. Reduce Phase (Stage 2):
#    - Reducer 1: Reads all Sales records, sums salary
#    - Reducer 2: Reads all Engineering records, sums salary

result.show()
# +------------+------------+
# |        dept|total_salary|
# +------------+------------+
# |       Sales|       16500|
# |Engineering|       22500|
# +------------+------------+

# Check execution plan to see shuffle
result.explain()
# == Physical Plan ==
# *(2) HashAggregate(...)               Stage 2 (Reduce)
# +- Exchange hashpartitioning(...)     SHUFFLE!
#    +- *(1) HashAggregate(...)         Stage 1 (Map)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How can you reduce shuffle operations?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Strategies to reduce shuffles:</strong><br><br>
                    <strong>1. Filter early (reduce data volume):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Bad: Shuffle all data then filter
df.groupBy("dept").count().filter(col("count") > 10)

# Good: Filter before shuffle
df.filter(col("active") == True).groupBy("dept").count()</code></pre>
                    <strong>2. Use broadcast joins for small tables:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import broadcast

# Bad: Shuffle both tables
large_df.join(small_df, "key")

# Good: Broadcast small table (no shuffle for small table)
large_df.join(broadcast(small_df), "key")</code></pre>
                    <strong>3. Partition data appropriately:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Partition by column you'll group by
df.repartition("dept").write.parquet("data")
# Later groupBy("dept") will be faster</code></pre>
                    <strong>4. Use reduceByKey instead of groupByKey:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Bad: Shuffle all values
rdd.groupByKey().mapValues(sum)

# Good: Reduce locally first, then shuffle
rdd.reduceByKey(lambda x,y: x+y)</code></pre>
                    <strong>5. Cache intermediate results:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># If reusing shuffled data
shuffled = df.groupBy("dept").count()
shuffled.cache()
shuffled.filter(col("count") > 10).show()
shuffled.orderBy("count").show()  # Reuses cached data</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Why is it expensive?</strong></span>
                </div>
                <div class="cross-answer">
                    Shuffle is expensive due to multiple costly operations:<br><br>
                    <strong>1. Disk I/O (50% of cost):</strong><br>
                     Write intermediate results to disk<br>
                     Read shuffle files from disk<br>
                     Disk is 100-1000x slower than memory<br><br>
                    <strong>2. Network I/O (30% of cost):</strong><br>
                     Transfer data across network<br>
                     Network bandwidth limited<br>
                     Latency adds up with large data<br><br>
                    <strong>3. Serialization/Deserialization (15% of cost):</strong><br>
                     Convert objects to bytes for transfer<br>
                     Convert bytes back to objects<br>
                     CPU intensive<br><br>
                    <strong>4. Memory Pressure (5% of cost):</strong><br>
                     Buffer data during shuffle<br>
                     Can cause GC pauses<br>
                     May spill to disk if memory full<br><br>
                    <strong>Performance Impact Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 100 GB dataset
# No shuffle (filter): 10 seconds
df.filter(col("age") > 25).count()

# With shuffle (groupBy): 5 minutes (30x slower!)
df.groupBy("dept").count().show()</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How can you minimize shuffling?</strong></span>
                </div>
                <div class="cross-answer">
                    See "How can you reduce shuffle operations?" above - same strategies apply.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What causes shuffling?</strong></span>
                </div>
                <div class="cross-answer">
                    Shuffling is caused by <strong>wide transformations</strong> that require data redistribution. See "Which operations trigger shuffles?" below.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which operations trigger shuffles?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Operations that trigger shuffles:</strong><br><br>
                    <strong>1. Aggregations:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.groupBy("col").count()
df.groupBy("col").agg(avg("value"))
rdd.reduceByKey(lambda x,y: x+y)
rdd.aggregateByKey(...)</code></pre>
                    <strong>2. Joins:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.join(df2, "key")  # Both tables shuffled
rdd1.join(rdd2)
df1.crossJoin(df2)  # Cartesian product</code></pre>
                    <strong>3. Sorting:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.orderBy("col")
df.sort("col")
rdd.sortBy(lambda x: x[0])</code></pre>
                    <strong>4. Repartitioning:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.repartition(10)
df.repartition("col")
rdd.partitionBy(10)</code></pre>
                    <strong>5. Distinct/Deduplication:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.distinct()
df.dropDuplicates()
rdd.distinct()</code></pre>
                    <strong>6. Set Operations:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.intersect(df2)
df1.subtract(df2)
rdd1.intersection(rdd2)</code></pre>
                    <strong>Operations that DON'T shuffle:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.filter(), df.select(), df.map()
df.union()  # Just concatenates partitions
df.coalesce()  # Reduces partitions without full shuffle</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 73: Excessive Shuffling -->
<div class="question-content" id="q73">
    <div class="question-header">
        <h1 class="question-title">What causes excessive shuffling?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Common Causes
        </div>
        <div class="definition-box">
            <p><strong>Excessive shuffling</strong> occurs when multiple shuffle operations happen unnecessarily, often due to poor query design, wrong partition numbers, or inefficient operation ordering.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-exclamation-triangle"></i>
            Causes of Excessive Shuffling
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-times-circle"></i> 1. Multiple Wide Transformations in Sequence
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Bad: Multiple shuffles
df.groupBy("dept").count() \
  .join(other_df, "dept") \
  .orderBy("count")
# 3 shuffles: groupBy, join, orderBy

# Better: Minimize operations
df.groupBy("dept").count() \
  .join(broadcast(other_df), "dept")  # Broadcast join (no shuffle)
  .orderBy("count")  # Only 2 shuffles now</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-times-circle"></i> 2. Wrong Number of Partitions
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Too many partitions (default 200)
spark.conf.set("spark.sql.shuffle.partitions", "200")
df.groupBy("dept").count()  # Creates 200 partitions for small data

# Too few partitions
spark.conf.set("spark.sql.shuffle.partitions", "2")
df.groupBy("dept").count()  # Only 2 tasks, underutilizes cluster

# Optimal: Match cluster resources
# Rule: 2-4 partitions per core
# 20 cores  40-80 partitions
spark.conf.set("spark.sql.shuffle.partitions", "60")</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-times-circle"></i> 3. Not Filtering Early
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Bad: Shuffle entire dataset, then filter
df.groupBy("dept").count() \
  .filter(col("count") > 100)
# Shuffles all data

# Good: Filter before shuffle
df.filter(col("active") == True) \
  .groupBy("dept").count() \
  .filter(col("count") > 100)
# Shuffles only active records (much less data)</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-times-circle"></i> 4. Using groupByKey Instead of reduceByKey
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Bad: groupByKey shuffles all values
rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4)])
rdd.groupByKey().mapValues(sum).collect()
# Shuffles: ("a", [1,3]), ("b", [2,4])

# Good: reduceByKey combines locally first
rdd.reduceByKey(lambda x,y: x+y).collect()
# Reduces locally: ("a", 4), ("b", 6)
# Then shuffles reduced results (less data)</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-times-circle"></i> 5. Joining Large Tables Without Optimization
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Bad: Both large tables shuffled
large_df1.join(large_df2, "key")
# Shuffles both tables

# Good: If one is small, broadcast it
large_df1.join(broadcast(small_df), "key")
# Only large_df1 shuffled (if needed)

# Better: Pre-partition by join key
df1.repartition("key").write.parquet("df1_partitioned")
df2.repartition("key").write.parquet("df2_partitioned")
# Future joins on same key: no shuffle needed!</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-times-circle"></i> 6. Unnecessary Repartitioning
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Bad: Multiple repartitions
df.repartition(10) \
  .groupBy("dept").count() \
  .repartition(5)
# 3 shuffles!

# Good: Repartition once if needed
df.repartition(10, "dept") \
  .groupBy("dept").count()
# 1 shuffle (repartition by key used in groupBy)</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-chart-line"></i>
            How to Detect Excessive Shuffling
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-search"></i> Detection Methods</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># 1. Check execution plan
df.groupBy("dept").count().explain()
# Look for "Exchange" operations (shuffles)

# 2. Use Spark UI
# Go to: http://localhost:4040
# Check: Stages tab  Look for "Shuffle Read" and "Shuffle Write"

# 3. Monitor shuffle metrics
# In Spark UI, look for:
# - Shuffle Read Size (should be reasonable)
# - Shuffle Write Size (should be reasonable)
# - Number of shuffle partitions

# 4. Check stage count
# Each shuffle creates a new stage
# Too many stages = too many shuffles

# Example output:
# Stage 0: Read data (no shuffle)
# Stage 1: groupBy (shuffle 1)
# Stage 2: join (shuffle 2)
# Stage 3: orderBy (shuffle 3)
#  3 shuffles! Optimization needed</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 74: Avoid Full Data Shuffles -->
<div class="question-content" id="q74">
    <div class="question-header">
        <h1 class="question-title">How to avoid full data shuffles?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Strategies to Avoid Shuffles
        </div>
        <div class="definition-box">
            <p><strong>Avoiding full data shuffles</strong> means reducing the amount of data that needs to be redistributed across the network, or eliminating shuffles entirely through smart partitioning and operation choices.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Optimization Techniques
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 1. Use Broadcast Joins for Small Tables
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>from pyspark.sql.functions import broadcast

# Small lookup table (< 10MB)
lookup = spark.read.csv("small_lookup.csv")

# Large fact table
facts = spark.read.parquet("large_facts.parquet")

# Broadcast join - no shuffle for lookup table
result = facts.join(broadcast(lookup), "key")

# Spark automatically broadcasts if table < spark.sql.autoBroadcastJoinThreshold (10MB)
# But explicit is better for clarity</code></pre>
                <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                    <strong>Why it works:</strong> Small table sent to all executors, large table not shuffled.
                </p>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 2. Pre-partition Data by Common Join/Group Keys
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Write data partitioned by key you'll use frequently
df.repartition("customer_id") \
  .write.partitionBy("customer_id") \
  .parquet("customers_partitioned")

# Later operations on same key won't shuffle
df_partitioned = spark.read.parquet("customers_partitioned")
df_partitioned.groupBy("customer_id").agg(...)  # No shuffle!

# Joins on pre-partitioned data
df1_partitioned.join(df2_partitioned, "customer_id")  # Minimal shuffle</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 3. Filter Before Wide Transformations
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Bad: Shuffle 1TB, then filter to 10GB
df.groupBy("dept").count().filter(col("count") > 1000)

# Good: Filter to 10GB, then shuffle only 10GB
df.filter(col("date") >= "2024-01-01") \
  .groupBy("dept").count() \
  .filter(col("count") > 1000)

# Reduction: 100x less data shuffled!</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 4. Use reduceByKey Instead of groupByKey
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4)])

# Bad: groupByKey - shuffles all values
rdd.groupByKey().mapValues(sum)
# Shuffle: ("a", [1, 3]), ("b", [2, 4])
# Network: 4 values transferred

# Good: reduceByKey - combines locally first
rdd.reduceByKey(lambda x, y: x + y)
# Local reduce: ("a", 4 in partition 1), ("a", 0 in partition 2)
# Shuffle: ("a", 4), ("b", 6)
# Network: 2 values transferred (50% reduction!)</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 5. Optimize Partition Count
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Check current setting
print(spark.conf.get("spark.sql.shuffle.partitions"))  # Default: 200

# For small data (< 1GB): reduce partitions
spark.conf.set("spark.sql.shuffle.partitions", "20")

# For large data (> 100GB): increase partitions
spark.conf.set("spark.sql.shuffle.partitions", "400")

# Rule of thumb: 2-4 partitions per core
# 50 cores  100-200 partitions

# Enable adaptive query execution (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
# Automatically adjusts partitions based on data size</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 6. Use coalesce() Instead of repartition() When Reducing
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Bad: repartition for reducing (full shuffle)
df.repartition(2)  # Shuffles all data

# Good: coalesce for reducing (minimal shuffle)
df.coalesce(2)  # Combines partitions without full shuffle

# Use repartition when:
# - Increasing partitions
# - Need even distribution

# Use coalesce when:
# - Decreasing partitions
# - Uneven distribution acceptable</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 7. Cache Intermediate Shuffled Results
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># If reusing shuffled data multiple times
shuffled = df.groupBy("dept").agg(sum("salary"))
shuffled.cache()  # Cache after expensive shuffle

# Multiple uses without re-shuffling
shuffled.filter(col("sum(salary)") > 100000).show()
shuffled.orderBy("dept").show()
shuffled.join(other_df, "dept").show()

# Remember to unpersist when done
shuffled.unpersist()</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 8. Use Window Functions Instead of Self-Joins
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, lag

# Bad: Self-join (2 shuffles)
df1 = df.groupBy("id").agg(max("value").alias("max_val"))
df.join(df1, "id")

# Good: Window function (1 shuffle)
window = Window.partitionBy("id").orderBy(col("value").desc())
df.withColumn("max_val", first("value").over(window))</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-chart-bar"></i>
            Performance Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Technique</th>
                        <th>Shuffle Reduction</th>
                        <th>Typical Speedup</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Broadcast join (small table)</td>
                        <td>50-100%</td>
                        <td>2-5x</td>
                    </tr>
                    <tr>
                        <td>Pre-partitioned data</td>
                        <td>80-100%</td>
                        <td>3-10x</td>
                    </tr>
                    <tr>
                        <td>Filter before shuffle</td>
                        <td>50-90%</td>
                        <td>2-5x</td>
                    </tr>
                    <tr>
                        <td>reduceByKey vs groupByKey</td>
                        <td>30-50%</td>
                        <td>1.5-2x</td>
                    </tr>
                    <tr>
                        <td>Optimize partition count</td>
                        <td>20-40%</td>
                        <td>1.2-2x</td>
                    </tr>
                    <tr>
                        <td>Cache shuffled results</td>
                        <td>100% (for reuse)</td>
                        <td>5-50x</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 75: Data Partitioning in Spark -->
<div class="question-content" id="q75">
    <div class="question-header">
        <h1 class="question-title">How does Spark handle data partitioning?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Data Partitioning Overview
        </div>
        <div class="definition-box">
            <p><strong>Data partitioning</strong> is how Spark divides data into smaller chunks (partitions) that can be processed in parallel across multiple executors. Each partition is processed by a single task on a single executor.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Partitions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Parallel Processing</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Distribution</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Partitioning Visualization
        </div>
        <div class="image-container">
            <svg width="900" height="500" viewBox="0 0 900 500">
                <rect width="900" height="500" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Data Partitioning in Spark</text>
                
                <!-- Original Data -->
                <rect x="50" y="70" width="200" height="350" fill="rgba(59, 130, 246, 0.2)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="150" y="100" font-size="16" fill="#3b82f6" text-anchor="middle" font-weight="bold">Original Dataset</text>
                <text x="150" y="120" font-size="12" fill="#cbd5e1" text-anchor="middle">(1 million rows)</text>
                
                <rect x="70" y="140" width="160" height="260" fill="#334155" stroke="#3b82f6" stroke-width="1" rx="8"/>
                <text x="150" y="170" font-size="11" fill="#cbd5e1" text-anchor="middle">Row 1</text>
                <text x="150" y="190" font-size="11" fill="#cbd5e1" text-anchor="middle">Row 2</text>
                <text x="150" y="210" font-size="11" fill="#cbd5e1" text-anchor="middle">Row 3</text>
                <text x="150" y="230" font-size="11" fill="#cbd5e1" text-anchor="middle">...</text>
                <text x="150" y="270" font-size="11" fill="#cbd5e1" text-anchor="middle">...</text>
                <text x="150" y="310" font-size="11" fill="#cbd5e1" text-anchor="middle">...</text>
                <text x="150" y="350" font-size="11" fill="#cbd5e1" text-anchor="middle">Row 999,999</text>
                <text x="150" y="380" font-size="11" fill="#cbd5e1" text-anchor="middle">Row 1,000,000</text>
                
                <!-- Arrow -->
                <path d="M 250 250 L 320 250" stroke="#10b981" stroke-width="3" marker-end="url(#arrowgreen2)"/>
                <text x="285" y="240" font-size="13" fill="#10b981" text-anchor="middle" font-weight="bold">Partition</text>
                
                <!-- Partitioned Data -->
                <rect x="320" y="70" width="530" height="350" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="585" y="100" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">Partitioned Data (4 partitions)</text>
                
                <!-- Partition 1 -->
                <rect x="340" y="120" width="110" height="80" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="395" y="145" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 0</text>
                <text x="395" y="165" font-size="11" fill="#cbd5e1" text-anchor="middle">Rows 1-250K</text>
                <text x="395" y="185" font-size="10" fill="#cbd5e1" text-anchor="middle"> Executor 1</text>
                
                <!-- Partition 2 -->
                <rect x="470" y="120" width="110" height="80" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="525" y="145" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                <text x="525" y="165" font-size="11" fill="#cbd5e1" text-anchor="middle">Rows 250K-500K</text>
                <text x="525" y="185" font-size="10" fill="#cbd5e1" text-anchor="middle"> Executor 2</text>
                
                <!-- Partition 3 -->
                <rect x="600" y="120" width="110" height="80" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="655" y="145" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                <text x="655" y="165" font-size="11" fill="#cbd5e1" text-anchor="middle">Rows 500K-750K</text>
                <text x="655" y="185" font-size="10" fill="#cbd5e1" text-anchor="middle"> Executor 3</text>
                
                <!-- Partition 4 -->
                <rect x="730" y="120" width="110" height="80" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="785" y="145" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 3</text>
                <text x="785" y="165" font-size="11" fill="#cbd5e1" text-anchor="middle">Rows 750K-1M</text>
                <text x="785" y="185" font-size="10" fill="#cbd5e1" text-anchor="middle"> Executor 4</text>
                
                <!-- Processing -->
                <text x="585" y="240" font-size="15" fill="#10b981" text-anchor="middle" font-weight="bold">Parallel Processing</text>
                
                <!-- Tasks -->
                <rect x="340" y="260" width="110" height="60" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="395" y="285" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 1</text>
                <text x="395" y="305" font-size="10" fill="#cbd5e1" text-anchor="middle">Processes P0</text>
                
                <rect x="470" y="260" width="110" height="60" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="525" y="285" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 2</text>
                <text x="525" y="305" font-size="10" fill="#cbd5e1" text-anchor="middle">Processes P1</text>
                
                <rect x="600" y="260" width="110" height="60" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="655" y="285" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 3</text>
                <text x="655" y="305" font-size="10" fill="#cbd5e1" text-anchor="middle">Processes P2</text>
                
                <rect x="730" y="260" width="110" height="60" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="785" y="285" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Task 4</text>
                <text x="785" y="305" font-size="10" fill="#cbd5e1" text-anchor="middle">Processes P3</text>
                
                <!-- Key Point -->
                <rect x="340" y="350" width="500" height="50" fill="rgba(245, 158, 11, 0.2)" stroke="#f59e0b" stroke-width="2" rx="8"/>
                <text x="590" y="375" font-size="13" fill="#f59e0b" text-anchor="middle" font-weight="bold">Key: 1 Partition = 1 Task = Processed by 1 Core</text>
                <text x="590" y="390" font-size="11" fill="#cbd5e1" text-anchor="middle">4 partitions = 4 tasks running in parallel</text>
                
                <defs>
                    <marker id="arrowgreen2" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Data split into partitions for parallel processing across executors</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-cogs"></i>
            How Spark Partitions Data
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-file-import"></i> 1. Reading Data (Initial Partitioning)
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Reading from file
df = spark.read.csv("data.csv")
# Spark automatically partitions based on:
# - File size
# - Number of files
# - Block size (HDFS: 128MB blocks)

# Default partitions for reading
# - spark.default.parallelism (for RDDs)
# - spark.sql.files.maxPartitionBytes (for DataFrames, default 128MB)

# Control partitions explicitly
df = spark.read.option("maxPartitionsBytes", "64MB").csv("data.csv")

# From Parquet (preserves partitioning)
df = spark.read.parquet("data.parquet")  # Uses existing partitions</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-random"></i> 2. Hash Partitioning (Default for Shuffles)
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># During groupBy, join, etc.
df.groupBy("user_id").count()

# Spark uses hash partitioning:
# partition_id = hash(key) % num_partitions

# Example with 4 partitions:
# user_id=123  hash(123) % 4 = partition 1
# user_id=456  hash(456) % 4 = partition 0
# user_id=789  hash(789) % 4 = partition 1

# All rows with same key go to same partition</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-sort"></i> 3. Range Partitioning (For Sorting)
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># For orderBy operations
df.orderBy("age")

# Spark uses range partitioning:
# Partition 0: ages 0-25
# Partition 1: ages 26-50
# Partition 2: ages 51-75
# Partition 3: ages 76-100

# Data sorted within and across partitions</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-sitemap"></i> 4. Custom Partitioning
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Partition by specific column
df.repartition("department")  # Hash partition by department

# Partition by multiple columns
df.repartition("year", "month")

# Specify exact number of partitions
df.repartition(10)  # 10 random partitions

# Partition and sort
df.repartition(10, "user_id")  # 10 partitions by user_id</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to control the number of partitions?</strong></span>
                </div>
                <div class="cross-answer">
                    Multiple ways to control partitions:<br><br>
                    <strong>1. When reading data:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Option 1: Set max partition bytes
df = spark.read.option("maxPartitionBytes", "64MB").csv("data.csv")

# Option 2: Repartition after reading
df = spark.read.csv("data.csv").repartition(50)</code></pre>
                    <strong>2. For shuffles (groupBy, join):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Set shuffle partitions globally
spark.conf.set("spark.sql.shuffle.partitions", "100")  # Default: 200

# Or per DataFrame
df.repartition(50).groupBy("dept").count()</code></pre>
                    <strong>3. Explicitly with repartition():</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Increase partitions
df.repartition(100)

# Decrease partitions
df.coalesce(10)  # More efficient for reduction</code></pre>
                    <strong>4. Enable adaptive query execution (Spark 3.0+):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>spark.conf.set("spark.sql.adaptive.enabled", "true")
# Automatically adjusts partitions based on data size</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the use of repartition() and coalesce()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>repartition():</strong> Redistributes data evenly across specified number of partitions. Performs full shuffle.<br><br>
                    <strong>coalesce():</strong> Reduces partitions without full shuffle. More efficient for decreasing partitions.<br><br>
                    <strong>Detailed comparison:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># repartition() - Full shuffle, even distribution
df.repartition(10)  # Shuffle all data to 10 partitions
# Use when: Increasing partitions or need even distribution

# coalesce() - Minimal shuffle, may be uneven
df.coalesce(10)  # Combine existing partitions to 10
# Use when: Decreasing partitions, optimization matters

# Example:
# 100 partitions  10 partitions
df.repartition(10)  # Shuffles all data
df.coalesce(10)     # Just combines partitions (faster)</code></pre>
                    See Question 81 for more details.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 76: Partitions in PySpark -->
<div class="question-content" id="q76">
    <div class="question-header">
        <h1 class="question-title">Explain partitions in PySpark</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            What are Partitions?
        </div>
        <div class="definition-box">
            <p><strong>Partitions</strong> are logical divisions of data in Spark. Each partition is a chunk of data that can be processed independently in parallel. The number of partitions determines the level of parallelism.</p>
            <p style="margin-top: 1rem;"><strong>Key Formula:</strong> Number of Tasks = Number of Partitions (per stage)</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Key Concepts
        </div>

        <div class="highlight-box">
            <strong>1. Partition Properties:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Each partition processed by one task on one executor core</li>
                <li>Partitions enable parallel processing</li>
                <li>Immutable (transformations create new partitions)</li>
                <li>Size should be 100MB-1GB for optimal performance</li>
                <li>Too many partitions = overhead, too few = underutilization</li>
            </ul>
        </div>

        <div class="highlight-box">
            <strong>2. Default Partition Numbers:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># For RDDs
spark.sparkContext.defaultParallelism
# Usually: number of cores in cluster

# For DataFrames (reading files)
# Based on: file size / spark.sql.files.maxPartitionBytes (128MB)

# For shuffles (groupBy, join, etc.)
spark.conf.get("spark.sql.shuffle.partitions")  # Default: 200</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Optimal Partition Count:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Rule of Thumb: 2-4 partitions per CPU core
# 
# Example calculations:
# - 10 executors  4 cores = 40 cores
# - Optimal partitions: 80-160
# - Set: spark.conf.set("spark.sql.shuffle.partitions", "120")

# Partition size guideline:
# - Minimum: 100 MB per partition
# - Maximum: 1 GB per partition
# - Sweet spot: 128-512 MB

# Example for 100GB data:
# 100GB / 200MB per partition = 500 partitions
spark.conf.set("spark.sql.shuffle.partitions", "500")</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Working with Partitions
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-cube"></i> Partition Operations</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Create DataFrame
df = spark.range(1000000)

# Check number of partitions
print(f"Partitions: {df.rdd.getNumPartitions()}")

# Check data distribution across partitions
df.rdd.glom().map(len).collect()
# Output: [250000, 250000, 250000, 250000] - evenly distributed

# Repartition data
df_repart = df.repartition(10)
print(f"After repartition: {df_repart.rdd.getNumPartitions()}")  # 10

# Repartition by column (hash partitioning)
df_hash = df.repartition(10, "id")
# All rows with same id go to same partition

# Coalesce (reduce partitions efficiently)
df_coal = df.coalesce(2)
print(f"After coalesce: {df_coal.rdd.getNumPartitions()}")  # 2

# Check partition distribution
def show_partition_info(df):
    partition_sizes = df.rdd.glom().map(len).collect()
    print(f"Partitions: {len(partition_sizes)}")
    print(f"Sizes: {partition_sizes}")
    print(f"Min: {min(partition_sizes)}, Max: {max(partition_sizes)}")
    print(f"Total rows: {sum(partition_sizes)}")

show_partition_info(df)

# Example output:
# Partitions: 4
# Sizes: [250000, 250000, 250000, 250000]
# Min: 250000, Max: 250000
# Total rows: 1000000</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-chart-bar"></i> Partition Skew Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Create skewed data
from pyspark.sql.functions import col, when

data = [(i % 10, f"value_{i}") for i in range(10000)]
df = spark.createDataFrame(data, ["key", "value"])

# Group by key (creates skew - key 0-9)
grouped = df.groupBy("key").count()

# Check partition distribution
grouped.rdd.glom().map(len).collect()
# Might see: [3, 2, 1, 1, 2, 1]  - unbalanced!

# Add salt to reduce skew
df_salted = df.withColumn("salt", (col("key") % 100))
grouped_salted = df_salted.groupBy("key", "salt").count()
# Better distribution across partitions

# Remove salt in final result
final = grouped_salted.groupBy("key").agg(sum("count").alias("total"))
final.show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How do you check the number of partitions in an RDD/DataFrame?</strong></span>
                </div>
                <div class="cross-answer">
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># For DataFrame
df.rdd.getNumPartitions()

# For RDD
rdd.getNumPartitions()

# Alternative for DataFrame
df.rdd.getNumPartitions()

# Example
df = spark.range(1000)
print(f"Number of partitions: {df.rdd.getNumPartitions()}")

# Check partition sizes
partition_sizes = df.rdd.glom().map(len).collect()
print(f"Partition sizes: {partition_sizes}")</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What happens if partitions are not balanced?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Partition skew causes performance problems:</strong><br><br>
                    <strong>1. Straggler tasks:</strong> One partition takes much longer than others<br>
                     Task 1: 100 seconds (large partition)<br>
                     Task 2: 10 seconds<br>
                     Task 3: 10 seconds<br>
                     Total time: 100 seconds (waiting for slowest task)<br><br>
                    <strong>2. Resource underutilization:</strong> Some executors idle while waiting<br>
                     99 tasks complete in 10 seconds<br>
                     1 task takes 100 seconds<br>
                     99 cores sit idle for 90 seconds<br><br>
                    <strong>3. Memory issues:</strong> Large partitions may cause OOM<br>
                     Partition too big  doesn't fit in executor memory<br>
                     Results in: OOM errors or spill to disk (slow)<br><br>
                    <strong>Solutions:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 1. Increase partition count
df.repartition(200)

# 2. Salt skewed keys
df.withColumn("salt", (col("key") % 10))

# 3. Use broadcast join for small tables
large_df.join(broadcast(small_df), "key")

# 4. Filter before shuffle
df.filter(col("valid") == True).groupBy("key").count()</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When do you use repartition() vs coalesce()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Use repartition() when:</strong><br>
                     Increasing number of partitions<br>
                     Need even distribution across partitions<br>
                     Partitioning by specific column<br>
                     Data skew needs correction<br><br>
                    <strong>Use coalesce() when:</strong><br>
                     Decreasing number of partitions<br>
                     Performance is critical (avoids full shuffle)<br>
                     Before writing to reduce number of output files<br>
                     Uneven distribution acceptable<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># repartition - full shuffle, even distribution
df.repartition(10)  # 100  10 partitions (shuffles all data)

# coalesce - minimal shuffle, may be uneven
df.coalesce(10)  # 100  10 partitions (combines partitions)

# Example: Writing to file
df.coalesce(1).write.csv("output.csv")  # Single output file

# When data is skewed
df.repartition(200)  # Fix skew with more partitions</code></pre>
                    See Question 81 for detailed comparison.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 77: Partitioning Importance -->
<div class="question-content" id="q77">
    <div class="question-header">
        <h1 class="question-title">What is partitioning and why is it important in Spark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Why Partitioning Matters
        </div>
        <div class="definition-box">
            <p><strong>Partitioning</strong> is the foundation of Spark's parallel processing. It determines how data is distributed across the cluster and directly impacts performance, parallelism, and resource utilization.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-star"></i>
            Importance of Partitioning
        </div>

        <div class="highlight-box">
            <strong>1. Enables Parallelism</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Each partition processed independently in parallel. More partitions = more parallelism (up to available cores).
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 1 partition = 1 task = uses 1 core
df.repartition(1).count()  # Uses only 1 core (slow)

# 100 partitions = 100 tasks = can use 100 cores
df.repartition(100).count()  # Uses all available cores (fast)</code></pre>
        </div>

        <div class="highlight-box">
            <strong>2. Controls Memory Usage</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Each partition must fit in executor memory. Right partition size prevents OOM errors.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Bad: 10GB data in 10 partitions = 1GB per partition
# If executor has 512MB memory  OOM!
df.repartition(10)

# Good: 10GB data in 100 partitions = 100MB per partition
# Fits comfortably in 512MB executor memory
df.repartition(100)</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Minimizes Shuffles</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Pre-partitioning by key reduces shuffle during groupBy/join operations.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Without pre-partitioning: Full shuffle
df.groupBy("user_id").count()  # Shuffles all data

# With pre-partitioning: Minimal/no shuffle
df.repartition("user_id") \
  .groupBy("user_id").count()  # Data already partitioned by user_id!</code></pre>
        </div>

        <div class="highlight-box">
            <strong>4. Affects I/O Performance</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Number of partitions = number of output files when writing.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Bad: 1000 small files (overhead)
df.repartition(1000).write.parquet("output")

# Bad: 1 huge file (not parallel)
df.coalesce(1).write.parquet("output")

# Good: Reasonable number of files
df.repartition(50).write.parquet("output")  # 50 files, ~200MB each</code></pre>
        </div>

        <div class="highlight-box">
            <strong>5. Load Balancing</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Even partitions = even workload distribution across executors.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Skewed partitions:
# Partition 1: 1GB (takes 100 seconds)
# Partition 2: 10MB (takes 1 second)
# Partition 3: 10MB (takes 1 second)
# Total: 100 seconds (bottlenecked by largest partition)

# Balanced partitions:
# Partition 1: 350MB (takes 35 seconds)
# Partition 2: 350MB (takes 35 seconds)
# Partition 3: 350MB (takes 35 seconds)
# Total: 35 seconds (all finish at same time)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How does partitioning affect performance?</strong></span>
                </div>
                <div class="cross-answer">
                    Partitioning has major performance impacts:<br><br>
                    <strong>1. Too Few Partitions:</strong><br>
                     Underutilizes cluster (idle cores)<br>
                     Large partitions  OOM errors<br>
                     Limited parallelism<br>
                     Example: 10 partitions on 100-core cluster = 90 idle cores<br><br>
                    <strong>2. Too Many Partitions:</strong><br>
                     Task scheduling overhead<br>
                     Small tasks finish quickly (scheduling overhead dominates)<br>
                     More shuffle overhead<br>
                     Example: 10,000 partitions for 1GB data = wasteful<br><br>
                    <strong>3. Optimal Partitions:</strong><br>
                     2-4 partitions per core<br>
                     100MB-1GB per partition<br>
                     Balanced workload<br><br>
                    <strong>Performance Impact Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 100GB data, 50-core cluster

# Bad: 10 partitions (too few)
spark.conf.set("spark.sql.shuffle.partitions", "10")
df.groupBy("key").count()  # 500 seconds

# Bad: 10000 partitions (too many)
spark.conf.set("spark.sql.shuffle.partitions", "10000")
df.groupBy("key").count()  # 300 seconds (overhead)

# Good: 100 partitions (optimal)
spark.conf.set("spark.sql.shuffle.partitions", "100")
df.groupBy("key").count()  # 50 seconds (10x faster!)</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How can you inspect partition count in a DataFrame?</strong></span>
                </div>
                <div class="cross-answer">
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Method 1: getNumPartitions()
num_partitions = df.rdd.getNumPartitions()
print(f"Partitions: {num_partitions}")

# Method 2: Check partition sizes
partition_sizes = df.rdd.glom().map(len).collect()
print(f"Sizes: {partition_sizes}")
print(f"Total partitions: {len(partition_sizes)}")

# Method 3: Detailed partition info
def inspect_partitions(df):
    sizes = df.rdd.glom().map(len).collect()
    print(f"Total partitions: {len(sizes)}")
    print(f"Partition sizes: {sizes}")
    print(f"Min: {min(sizes)}, Max: {max(sizes)}, Avg: {sum(sizes)/len(sizes):.0f}")
    print(f"Total rows: {sum(sizes)}")
    
    # Check for skew
    avg = sum(sizes) / len(sizes)
    skewed = [i for i, size in enumerate(sizes) if size > avg * 2]
    if skewed:
        print(f" Skewed partitions: {skewed}")

inspect_partitions(df)

# Method 4: In Spark UI
# Go to: http://localhost:4040
# SQL tab  Click query  See "Number of Partitions"</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 78: What is Partitioning -->
<div class="question-content" id="q78">
    <div class="question-header">
        <h1 class="question-title">What is partitioning?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Quick Definition
        </div>
        <div class="definition-box">
            <p><strong>Partitioning</strong> is the process of dividing a large dataset into smaller, manageable chunks (partitions) that can be distributed across cluster nodes and processed in parallel.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-th-large"></i>
            Key Points
        </div>

        <div class="highlight-box">
            <strong>Partition = Unit of Parallelism</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                 Each partition is processed by one task<br>
                 Each task runs on one executor core<br>
                 More partitions = more parallel tasks<br>
                 Partitions are the smallest unit of work in Spark
            </p>
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> Basic Partitioning Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Create data
df = spark.range(1000)

# Check partitions
print(f"Partitions: {df.rdd.getNumPartitions()}")

# Repartition
df_4 = df.repartition(4)
print(f"After repartition: {df_4.rdd.getNumPartitions()}")

# Partition by column
df_user = df.repartition("id")  # Hash partition by id

# See partition distribution
sizes = df_4.rdd.glom().map(len).collect()
print(f"Distribution: {sizes}")</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How can you check the number of partitions?</strong></span>
                </div>
                <div class="cross-answer">
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.rdd.getNumPartitions()</code></pre>
                    See Question 79 for detailed methods.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between repartition() and coalesce()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>repartition():</strong> Full shuffle, even distribution, can increase/decrease partitions.<br>
                    <strong>coalesce():</strong> Minimal shuffle, may be uneven, only decreases partitions.<br><br>
                    See Question 81 for detailed comparison.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 79: Check Number of Partitions -->
<div class="question-content" id="q79">
    <div class="question-header">
        <h1 class="question-title">How can you check the number of partitions in an RDD or DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Methods to Check Partitions
        </div>
        <div class="definition-box">
            <p>Several methods exist to inspect partition count and distribution in Spark RDDs and DataFrames.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Checking Partition Count
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-search"></i> Methods to Check Partitions</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Method 1: getNumPartitions() - Most Common
df = spark.range(1000)
num_partitions = df.rdd.getNumPartitions()
print(f"Number of partitions: {num_partitions}")

# For RDD
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd_partitions = rdd.getNumPartitions()
print(f"RDD partitions: {rdd_partitions}")

# Method 2: Check partition sizes (distribution)
partition_sizes = df.rdd.glom().map(len).collect()
print(f"Partition sizes: {partition_sizes}")
print(f"Total partitions: {len(partition_sizes)}")

# Method 3: Detailed partition analysis
def analyze_partitions(df):
    """Comprehensive partition analysis"""
    sizes = df.rdd.glom().map(len).collect()
    
    print("="*50)
    print("PARTITION ANALYSIS")
    print("="*50)
    print(f"Total partitions: {len(sizes)}")
    print(f"Partition sizes: {sizes}")
    print(f"Total rows: {sum(sizes)}")
    print(f"Min size: {min(sizes)}")
    print(f"Max size: {max(sizes)}")
    print(f"Average size: {sum(sizes)/len(sizes):.2f}")
    
    # Calculate skew
    avg = sum(sizes) / len(sizes)
    skew_threshold = 2  # 2x average is considered skewed
    skewed_partitions = [(i, size) for i, size in enumerate(sizes) if size > avg * skew_threshold]
    
    if skewed_partitions:
        print(f"\n SKEWED PARTITIONS (>{skew_threshold}x average):")
        for part_id, size in skewed_partitions:
            print(f"  Partition {part_id}: {size} rows ({size/avg:.1f}x average)")
    else:
        print("\n Partitions are well-balanced")
    
    # Empty partitions
    empty = [i for i, size in enumerate(sizes) if size == 0]
    if empty:
        print(f"\n Empty partitions: {empty}")
    
    print("="*50)

# Example usage
df = spark.range(10000).repartition(8)
analyze_partitions(df)

# Output example:
# ==================================================
# PARTITION ANALYSIS
# ==================================================
# Total partitions: 8
# Partition sizes: [1250, 1250, 1250, 1250, 1250, 1250, 1250, 1250]
# Total rows: 10000
# Min size: 1250
# Max size: 1250
# Average size: 1250.00
# 
#  Partitions are well-balanced
# ==================================================

# Method 4: Using mapPartitionsWithIndex
def partition_info(idx, iterator):
    count = sum(1 for _ in iterator)
    return iter([(idx, count)])

partition_counts = df.rdd.mapPartitionsWithIndex(partition_info).collect()
print(f"Partition info: {partition_counts}")
# Output: [(0, 1250), (1, 1250), (2, 1250), ...]

# Method 5: Visualize partition distribution
def visualize_partitions(df, title="Partition Distribution"):
    sizes = df.rdd.glom().map(len).collect()
    print(f"\n{title}")
    print("-" * 60)
    max_size = max(sizes)
    for i, size in enumerate(sizes):
        bar_length = int((size / max_size) * 40)
        bar = "" * bar_length
        print(f"P{i:2d} [{size:6d}] {bar}")
    print("-" * 60)

visualize_partitions(df)

# Output example:
# Partition Distribution
# ------------------------------------------------------------
# P 0 [  1250] 
# P 1 [  1250] 
# P 2 [  1250] 
# P 3 [  1250] 
# P 4 [  1250] 
# P 5 [  1250] 
# P 6 [  1250] 
# P 7 [  1250] 
# ------------------------------------------------------------

# Method 6: Check in Spark UI
# Navigate to: http://localhost:4040
# Go to: SQL/DataFrame tab  Click on query
# Look for: "Number of Partitions" in the DAG visualization</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-exclamation-triangle"></i> Detecting Skewed Partitions</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Create skewed data
skewed_data = [(i % 3, f"value_{i}") for i in range(1000)]
# key 0: 334 records
# key 1: 333 records  
# key 2: 333 records

df_skewed = spark.createDataFrame(skewed_data, ["key", "value"])
df_grouped = df_skewed.repartition(10, "key")

# Analyze skew
def detect_skew(df, threshold=2.0):
    """Detect partition skew"""
    sizes = df.rdd.glom().map(len).collect()
    avg = sum(sizes) / len(sizes)
    
    skewed = []
    for i, size in enumerate(sizes):
        ratio = size / avg if avg > 0 else 0
        if ratio > threshold:
            skewed.append({
                'partition': i,
                'size': size,
                'ratio': ratio,
                'excess': size - avg
            })
    
    if skewed:
        print(f" SKEW DETECTED (threshold: {threshold}x average)")
        print(f"Average partition size: {avg:.0f}")
        for s in skewed:
            print(f"  Partition {s['partition']}: {s['size']} rows "
                  f"({s['ratio']:.1f}x avg, +{s['excess']:.0f} excess)")
        return True
    else:
        print(" No significant skew detected")
        return False

detect_skew(df_grouped)

# Example output:
#  SKEW DETECTED (threshold: 2.0x average)
# Average partition size: 100
# Partition 3: 334 rows (3.3x avg, +234 excess)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to increase or decrease the number of partitions?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Increase Partitions (use repartition):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Method 1: Explicit repartition
df_more = df.repartition(20)  # Increase to 20 partitions

# Method 2: Repartition by column
df_hash = df.repartition(20, "user_id")  # 20 partitions by user_id

# Method 3: Set shuffle partitions globally
spark.conf.set("spark.sql.shuffle.partitions", "300")
# Affects all shuffle operations (groupBy, join, etc.)</code></pre>
                    <strong>Decrease Partitions:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Method 1: coalesce (efficient, no full shuffle)
df_less = df.coalesce(5)  # Reduce to 5 partitions

# Method 2: repartition (full shuffle, even distribution)
df_less2 = df.repartition(5)  # Reduce to 5 with redistribution

# When to use which:
# - coalesce: Faster, for reducing partitions, uneven OK
# - repartition: Slower, for even distribution needed</code></pre>
                    <strong>Best Practices:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Before writing (reduce output files)
df.coalesce(10).write.parquet("output")

# Before shuffle operations (balance load)
df.repartition(100).groupBy("dept").count()

# After filtering (remove empty partitions)
df.filter(col("active") == True).coalesce(50)

# For even distribution
df.repartition(50)  # Even split across 50 partitions</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 81: repartition() vs coalesce() -->
<div class="question-content" id="q81">
    <div class="question-header">
        <h1 class="question-title">What's the difference between repartition() and coalesce()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>repartition():</strong> Performs a full shuffle to redistribute data evenly across specified number of partitions. Can increase or decrease partitions.</p>
            <p style="margin-top: 1rem;"><strong>coalesce():</strong> Reduces partitions by combining existing partitions without a full shuffle. More efficient but only for decreasing partitions, may result in uneven distribution.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> repartition()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> coalesce()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Shuffle</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Visual Comparison
        </div>
        <div class="image-container">
            <svg width="900" height="550" viewBox="0 0 900 550">
                <rect width="900" height="550" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">repartition() vs coalesce()</text>
                
                <!-- repartition() -->
                <rect x="50" y="60" width="380" height="450" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="240" y="90" font-size="18" fill="#ef4444" text-anchor="middle" font-weight="bold">repartition() - Full Shuffle</text>
                
                <!-- Original 6 partitions -->
                <text x="100" y="125" font-size="14" fill="#cbd5e1" text-anchor="start">Original: 6 partitions</text>
                <rect x="80" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="115" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P0: 100</text>
                
                <rect x="160" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="195" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P1: 150</text>
                
                <rect x="240" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="275" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P2: 200</text>
                
                <rect x="320" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="355" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P3: 50</text>
                
                <rect x="80" y="190" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="115" y="215" font-size="11" fill="#f1f5f9" text-anchor="middle">P4: 120</text>
                
                <rect x="160" y="190" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="195" y="215" font-size="11" fill="#f1f5f9" text-anchor="middle">P5: 80</text>
                
                <!-- Shuffle arrows -->
                <text x="240" y="265" font-size="14" fill="#ef4444" text-anchor="middle" font-weight="bold"> FULL SHUFFLE </text>
                <text x="240" y="285" font-size="11" fill="#cbd5e1" text-anchor="middle">All data redistributed</text>
                
                <!-- Result: 3 partitions (even) -->
                <text x="100" y="320" font-size="14" fill="#cbd5e1" text-anchor="start">Result: 3 partitions</text>
                <rect x="90" y="335" width="100" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="6"/>
                <text x="140" y="360" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">P0: 233</text>
                <text x="140" y="375" font-size="10" fill="#10b981" text-anchor="middle">Even</text>
                
                <rect x="210" y="335" width="100" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="6"/>
                <text x="260" y="360" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">P1: 233</text>
                <text x="260" y="375" font-size="10" fill="#10b981" text-anchor="middle">Even</text>
                
                <rect x="90" y="395" width="100" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="6"/>
                <text x="140" y="420" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">P2: 234</text>
                <text x="140" y="435" font-size="10" fill="#10b981" text-anchor="middle">Even</text>
                
                <rect x="70" y="465" width="310" height="35" fill="rgba(239, 68, 68, 0.2)" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="225" y="487" font-size="11" fill="#cbd5e1" text-anchor="middle"> Expensive (network I/O)</text>
                
                <!-- coalesce() -->
                <rect x="470" y="60" width="380" height="450" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="660" y="90" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">coalesce() - Minimal Shuffle</text>
                
                <!-- Original 6 partitions -->
                <text x="520" y="125" font-size="14" fill="#cbd5e1" text-anchor="start">Original: 6 partitions</text>
                <rect x="500" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="535" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P0: 100</text>
                
                <rect x="580" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="615" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P1: 150</text>
                
                <rect x="660" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="695" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P2: 200</text>
                
                <rect x="740" y="140" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="775" y="165" font-size="11" fill="#f1f5f9" text-anchor="middle">P3: 50</text>
                
                <rect x="500" y="190" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="535" y="215" font-size="11" fill="#f1f5f9" text-anchor="middle">P4: 120</text>
                
                <rect x="580" y="190" width="70" height="40" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="1" rx="6"/>
                <text x="615" y="215" font-size="11" fill="#f1f5f9" text-anchor="middle">P5: 80</text>
                
                <!-- Combine arrows -->
                <path d="M 535 180 L 535 305" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen3)"/>
                <path d="M 615 180 L 585 305" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen3)"/>
                <path d="M 695 180 L 660 305" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen3)"/>
                <path d="M 775 180 L 660 305" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen3)" stroke-dasharray="3,3"/>
                <text x="660" y="265" font-size="14" fill="#10b981" text-anchor="middle" font-weight="bold"> COMBINE </text>
                <text x="660" y="285" font-size="11" fill="#cbd5e1" text-anchor="middle">Just merge partitions</text>
                
                <!-- Result: 3 partitions (uneven) -->
                <text x="520" y="320" font-size="14" fill="#cbd5e1" text-anchor="start">Result: 3 partitions</text>
                <rect x="510" y="335" width="100" height="50" fill="rgba(245, 158, 11, 0.3)" stroke="#f59e0b" stroke-width="2" rx="6"/>
                <text x="560" y="360" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">P0: 220</text>
                <text x="560" y="375" font-size="10" fill="#f59e0b" text-anchor="middle">0+4</text>
                
                <rect x="630" y="335" width="100" height="50" fill="rgba(245, 158, 11, 0.3)" stroke="#f59e0b" stroke-width="2" rx="6"/>
                <text x="680" y="360" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">P1: 230</text>
                <text x="680" y="375" font-size="10" fill="#f59e0b" text-anchor="middle">1+5</text>
                
                <rect x="510" y="395" width="100" height="50" fill="rgba(245, 158, 11, 0.3)" stroke="#f59e0b" stroke-width="2" rx="6"/>
                <text x="560" y="420" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">P2: 250</text>
                <text x="560" y="435" font-size="10" fill="#f59e0b" text-anchor="middle">2+3</text>
                
                <rect x="490" y="465" width="310" height="35" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="645" y="487" font-size="11" fill="#cbd5e1" text-anchor="middle"> Fast (no full shuffle)</text>
                
                <defs>
                    <marker id="arrowgreen3" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#10b981"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">repartition: Full shuffle, even distribution | coalesce: Minimal shuffle, may be uneven</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Detailed Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>repartition()</th>
                        <th>coalesce()</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Shuffle</strong></td>
                        <td>Full shuffle (expensive)</td>
                        <td>Minimal/no shuffle (efficient)</td>
                    </tr>
                    <tr>
                        <td><strong>Direction</strong></td>
                        <td>Increase or decrease</td>
                        <td>Only decrease</td>
                    </tr>
                    <tr>
                        <td><strong>Distribution</strong></td>
                        <td>Even distribution guaranteed</td>
                        <td>May be uneven</td>
                    </tr>
                    <tr>
                        <td><strong>Performance</strong></td>
                        <td>Slower (network I/O)</td>
                        <td>Faster (local merging)</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Increase partitions, fix skew</td>
                        <td>Decrease partitions, reduce files</td>
                    </tr>
                    <tr>
                        <td><strong>Algorithm</strong></td>
                        <td>Hash partitioning</td>
                        <td>Combines adjacent partitions</td>
                    </tr>
                    <tr>
                        <td><strong>Network</strong></td>
                        <td>High network usage</td>
                        <td>Low/no network usage</td>
                    </tr>
                    <tr>
                        <td><strong>When to Use</strong></td>
                        <td>Need even distribution</td>
                        <td>Need efficiency</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-balance-scale"></i> Comparison Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>import time

# Create DataFrame with 100 partitions
df = spark.range(10000000).repartition(100)
print(f"Original partitions: {df.rdd.getNumPartitions()}")  # 100

# Scenario 1: Reduce from 100 to 10 partitions
# Using repartition (slow)
start = time.time()
df_repart = df.repartition(10)
df_repart.write.mode("overwrite").parquet("/tmp/repart")
repart_time = time.time() - start
print(f"repartition time: {repart_time:.2f}s")

# Using coalesce (fast)
start = time.time()
df_coal = df.coalesce(10)
df_coal.write.mode("overwrite").parquet("/tmp/coal")
coal_time = time.time() - start
print(f"coalesce time: {coal_time:.2f}s")

# Typical results:
# repartition time: 15.23s (full shuffle)
# coalesce time: 3.45s (4x faster!)

# Check distribution
def check_distribution(df):
    sizes = df.rdd.glom().map(len).collect()
    print(f"Partition sizes: {sizes}")
    print(f"Min: {min(sizes)}, Max: {max(sizes)}, Avg: {sum(sizes)/len(sizes):.0f}")

print("\nrepartition distribution:")
check_distribution(df_repart)
# Partition sizes: [1000000, 1000000, 1000000, ...]
# Min: 1000000, Max: 1000000, Avg: 1000000
# Perfect balance!

print("\ncoalesce distribution:")
check_distribution(df_coal)
# Partition sizes: [980000, 1020000, 990000, ...]
# Min: 980000, Max: 1020000, Avg: 1000000
# Slightly uneven but acceptable

# Scenario 2: Increase partitions (must use repartition)
df_small = spark.range(1000).coalesce(2)
print(f"Small DF partitions: {df_small.rdd.getNumPartitions()}")  # 2

# coalesce can't increase
df_cant_increase = df_small.coalesce(10)
print(f"After coalesce(10): {df_cant_increase.rdd.getNumPartitions()}")  # Still 2!

# repartition CAN increase
df_increased = df_small.repartition(10)
print(f"After repartition(10): {df_increased.rdd.getNumPartitions()}")  # 10

# Scenario 3: Partition by column
df_users = spark.range(1000).selectExpr("id as user_id", "id % 10 as group")

# repartition by column (hash partitioning)
df_hash = df_users.repartition(10, "group")
# All rows with same 'group' go to same partition

# coalesce doesn't support column-based partitioning
# df_users.coalesce(10, "group")  # ERROR!

# Scenario 4: Before writing files
df_large = spark.range(100000000).repartition(1000)

# Bad: 1000 output files (too many)
# df_large.write.parquet("output")

# Good: Use coalesce to reduce files
df_large.coalesce(50).write.parquet("output")  # 50 files (~reasonable)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When would you use coalesce() instead of repartition()?</strong></span>
                </div>
                <div class="cross-answer">
                    Use <code>coalesce()</code> when:<br><br>
                    <strong>1. Reducing partitions (decreasing):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.coalesce(10)  # 100  10 partitions (fast)</code></pre>
                    <strong>2. Before writing to reduce output files:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.coalesce(1).write.csv("single_file.csv")  # Single output file</code></pre>
                    <strong>3. After filtering (remove empty partitions):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.filter(col("active") == True).coalesce(50)  # Compact remaining data</code></pre>
                    <strong>4. Performance is critical and slight imbalance acceptable:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.coalesce(20)  # Fast reduction, ok if not perfectly balanced</code></pre>
                    <strong>5. At end of pipeline to reduce task overhead:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.filter(...).select(...).coalesce(10).write.parquet("output")</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When to use each?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Use repartition() when:</strong><br>
                     Increasing partitions: 10  100<br>
                     Need even distribution (fix skew)<br>
                     Partitioning by specific column<br>
                     Preparing for downstream shuffles<br><br>
                    <strong>Use coalesce() when:</strong><br>
                     Decreasing partitions: 100  10<br>
                     Writing output files<br>
                     Performance matters more than perfect balance<br>
                     After filtering to compact data<br><br>
                    <strong>Decision Tree:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>Increasing partitions?
   Yes  Use repartition()
   No (decreasing)  
       Need perfect distribution?
          Yes  Use repartition()
          No  Use coalesce() (faster)</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 83: repartition(1) -->
<div class="question-content" id="q83">
    <div class="question-header">
        <h1 class="question-title">What happens when you use .repartition(1)?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            What Happens
        </div>
        <div class="definition-box">
            <p><strong>repartition(1)</strong> consolidates all data into a single partition. This means all data is shuffled and sent to one executor, processed by one task, resulting in complete serialization of processing.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Single Partition</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> No Parallelism</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Bottleneck</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Single File Output</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-exclamation-triangle"></i>
            Effects of repartition(1)
        </div>

        <div class="warning-box">
            <strong> Critical Impact:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li><strong>Single task:</strong> Only 1 task processes all data (no parallelism)</li>
                <li><strong>One executor:</strong> All data goes to one executor (bottleneck)</li>
                <li><strong>Full shuffle:</strong> All partitions shuffle to single partition (network overhead)</li>
                <li><strong>Memory risk:</strong> All data must fit in single executor's memory (OOM risk)</li>
                <li><strong>Single file:</strong> Writes single output file (useful for small results)</li>
            </ul>
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> Example of repartition(1)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Original: 100 partitions across 20 executors
df = spark.range(10000000).repartition(100)
print(f"Original partitions: {df.rdd.getNumPartitions()}")  # 100

# Apply repartition(1)
df_single = df.repartition(1)
print(f"After repartition(1): {df_single.rdd.getNumPartitions()}")  # 1

# What happens internally:
# 1. All 100 partitions shuffle to 1 partition
# 2. All data sent to one executor (massive network transfer)
# 3. One task processes all 10M rows (no parallelism)
# 4. Other 19 executors sit idle (waste of resources)

# Performance impact
import time

# With parallelism (100 partitions)
start = time.time()
df.count()
parallel_time = time.time() - start
print(f"100 partitions: {parallel_time:.2f}s")  # ~2 seconds

# Without parallelism (1 partition)
start = time.time()
df_single.count()
single_time = time.time() - start
print(f"1 partition: {single_time:.2f}s")  # ~30 seconds (15x slower!)

# Common use case: Writing single output file
df_small = spark.range(1000)
df_small.repartition(1).write.csv("output.csv")
# Results in: single output file (output.csv/part-00000...)

# vs multiple files:
df_small.write.csv("output_multi.csv")
# Results in: multiple files (part-00000, part-00001, ...)</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-chart-bar"></i> Visual Impact</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Before repartition(1): 4 partitions on 4 executors
# Executor 1: [Task 1] Processing P0 (250K rows) 
# Executor 2: [Task 2] Processing P1 (250K rows) 
# Executor 3: [Task 3] Processing P2 (250K rows) 
# Executor 4: [Task 4] Processing P3 (250K rows) 
# Total time: 10 seconds (parallel)

# After repartition(1): 1 partition on 1 executor
# Executor 1: [Task 1] Processing ALL (1M rows) 
# Executor 2:  IDLE
# Executor 3:  IDLE
# Executor 4:  IDLE
# Total time: 40 seconds (serial, 4x slower!)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Why is this not recommended for large datasets?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>repartition(1) is dangerous for large datasets because:</strong><br><br>
                    <strong>1. Memory Overflow (OOM):</strong><br>
                    All data must fit in one executor's memory. Large datasets won't fit.
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Example: 100GB dataset, executor has 10GB memory
df_large.repartition(1)  # OOM Error!
# Cannot fit 100GB in 10GB memory</code></pre>
                    <strong>2. No Parallelism:</strong><br>
                    Only 1 task processes all data. Completely negates Spark's distributed computing.
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 1000-core cluster, but only 1 core working
# Other 999 cores idle  massive waste!</code></pre>
                    <strong>3. Extreme Shuffle:</strong><br>
                    All partitions shuffle to one partition  huge network transfer.
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 100 partitions across 100 nodes
# repartition(1)  all data transferred to 1 node
# Network bottleneck!</code></pre>
                    <strong>4. Performance Disaster:</strong><br>
                    Processing time increases linearly with data size (no parallelism).
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Example:
# 1GB data with 100 partitions: 10 seconds
# 1GB data with 1 partition: 200 seconds (20x slower!)
# 100GB data with 1 partition: 20,000 seconds (5.5 hours!)</code></pre>
                    <strong>5. Single Point of Failure:</strong><br>
                    If that one executor fails, entire job fails. No fault tolerance.
                    <br><br>
                    <strong>When is repartition(1) acceptable?</strong><br>
                     Very small datasets (&lt; 1GB)<br>
                     Final result is small (after aggregation)<br>
                     Need single output file<br>
                     Example: Writing final summary report
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Good: Small result after aggregation
df.groupBy("country").count() \  # Results in ~200 rows
  .coalesce(1) \                  # Use coalesce, not repartition
  .write.csv("country_summary.csv")

# Bad: Large dataset
df_100gb.repartition(1).write.parquet("output")  # DON'T DO THIS!</code></pre>
                    <strong>Alternative: Use coalesce(1) instead</strong><br>
                    More efficient than repartition(1) for reducing to single partition:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.coalesce(1).write.csv("output.csv")  # Better than repartition(1)</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 84: spark.sql.shuffle.partitions Parameter -->
<div class="question-content" id="q84">
    <div class="question-header">
        <h1 class="question-title">What's the role of spark.sql.shuffle.partitions parameter?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>spark.sql.shuffle.partitions</strong> controls the number of partitions created after shuffle operations (like groupBy, join, aggregations). Default is 200.</p>
            <p style="margin-top: 1rem;">This is one of the most important tuning parameters in Spark SQL/DataFrames.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Configuration</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Shuffle</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Partitions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance Tuning</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-cogs"></i>
            How It Works
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-info-circle"></i> What It Controls
            </div>
            <div style="margin-top: 1rem;">
                <pre><code># Affects these operations (wide transformations):
# - groupBy()
# - join()
# - agg()
# - distinct()
# - repartition()
# - orderBy()

# Check current value
print(spark.conf.get("spark.sql.shuffle.partitions"))  # Default: 200

# Example: groupBy creates shuffle
df = spark.range(1000000)
grouped = df.groupBy("id").count()

# After groupBy, result has 200 partitions (default)
print(grouped.rdd.getNumPartitions())  # 200

# Change the parameter
spark.conf.set("spark.sql.shuffle.partitions", "50")
grouped2 = df.groupBy("id").count()
print(grouped2.rdd.getNumPartitions())  # 50</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-chart-line"></i> Why It Matters
            </div>
            <div style="margin-top: 1rem;">
                <strong>Problem with default 200:</strong><br>
                 Small data: 200 partitions is too many (overhead)<br>
                 Large data: 200 partitions is too few (underutilization)<br><br>
                <pre><code># Example 1: Small data (1GB) with default 200
# Each partition: 1GB / 200 = 5MB
# Tasks finish in milliseconds  scheduling overhead dominates
# Result: Slow performance

# Example 2: Large data (1TB) with default 200
# Each partition: 1TB / 200 = 5GB
# Partitions too large  OOM errors, slow tasks
# Result: Memory issues</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Tuning Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-sliders-h"></i> Optimization Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>import time

# Dataset: 10GB
df = spark.read.parquet("10gb_data.parquet")

# Scenario 1: Default (200 partitions) - suboptimal
spark.conf.set("spark.sql.shuffle.partitions", "200")
start = time.time()
df.groupBy("category").agg(sum("amount")).show()
default_time = time.time() - start
print(f"Default (200): {default_time:.2f}s")  # 45 seconds

# Scenario 2: Too few partitions (20) - underutilizes cluster
spark.conf.set("spark.sql.shuffle.partitions", "20")
start = time.time()
df.groupBy("category").agg(sum("amount")).show()
few_time = time.time() - start
print(f"Too few (20): {few_time:.2f}s")  # 60 seconds (slower!)

# Scenario 3: Optimal partitions (100) - balanced
# Rule: 2-4 partitions per core, 100-500MB per partition
# 50 cores  100-200 partitions
# 10GB / 100 = 100MB per partition 
spark.conf.set("spark.sql.shuffle.partitions", "100")
start = time.time()
df.groupBy("category").agg(sum("amount")).show()
optimal_time = time.time() - start
print(f"Optimal (100): {optimal_time:.2f}s")  # 25 seconds (2x faster!)

# Scenario 4: Too many partitions (2000) - overhead
spark.conf.set("spark.sql.shuffle.partitions", "2000")
start = time.time()
df.groupBy("category").agg(sum("amount")).show()
many_time = time.time() - start
print(f"Too many (2000): {many_time:.2f}s")  # 50 seconds (overhead!)

# Guideline for different data sizes:
data_size_gb = 10
num_cores = 50

if data_size_gb < 1:
    # Small data
    partitions = max(num_cores, 20)
elif data_size_gb < 10:
    # Medium data
    partitions = num_cores * 2
elif data_size_gb < 100:
    # Large data
    partitions = num_cores * 4
else:
    # Very large data
    partitions = num_cores * 8

spark.conf.set("spark.sql.shuffle.partitions", str(partitions))
print(f"Set partitions to: {partitions}")

# For different operations
# Join of large tables (100GB each)
spark.conf.set("spark.sql.shuffle.partitions", "400")
df1.join(df2, "key")

# Aggregation of medium data (10GB)
spark.conf.set("spark.sql.shuffle.partitions", "100")
df.groupBy("col").count()

# Quick query on small data (100MB)
spark.conf.set("spark.sql.shuffle.partitions", "20")
df_small.groupBy("col").count()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-calculator"></i> Calculation Formula</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Formula: Number of shuffle partitions
# 
# Option 1: Based on cores
# partitions = num_cores * (2 to 4)
#
# Option 2: Based on data size
# partitions = data_size_gb / target_partition_size_gb
# target_partition_size: 0.1 to 0.5 GB (100-500MB)
#
# Take the larger of the two

def calculate_shuffle_partitions(data_size_gb, num_cores):
    """Calculate optimal shuffle partitions"""
    
    # Target: 128-256 MB per partition
    target_size_gb = 0.2  # 200MB
    
    # Based on data size
    size_based = int(data_size_gb / target_size_gb)
    
    # Based on cores (2-4x)
    core_based = num_cores * 3
    
    # Take the larger, but cap at reasonable limits
    partitions = max(size_based, core_based)
    partitions = max(partitions, 10)   # Minimum 10
    partitions = min(partitions, 2000)  # Maximum 2000
    
    return partitions

# Examples:
print(calculate_shuffle_partitions(1, 50))     # Small: 150
print(calculate_shuffle_partitions(10, 50))    # Medium: 150
print(calculate_shuffle_partitions(100, 50))   # Large: 500
print(calculate_shuffle_partitions(1000, 50))  # Very large: 2000 (capped)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Best Practices
        </div>

        <div class="highlight-box">
            <strong>1. Start with calculation, then tune based on monitoring:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Initial calculation
partitions = (data_size_gb / 0.2)  # 200MB per partition
spark.conf.set("spark.sql.shuffle.partitions", str(partitions))

# Monitor in Spark UI and adjust if needed</code></pre>
        </div>

        <div class="highlight-box">
            <strong>2. Use Adaptive Query Execution (Spark 3.0+):</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# Spark automatically adjusts partition count dynamically</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Set per job, not globally:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Different jobs have different needs
spark.conf.set("spark.sql.shuffle.partitions", "50")
df_small.groupBy("col").count()

spark.conf.set("spark.sql.shuffle.partitions", "500")
df_large.groupBy("col").count()</code></pre>
        </div>

        <div class="warning-box">
            <strong> Common Mistakes:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Using default 200 for all workloads</li>
                <li>Setting too high (>2000)  scheduling overhead</li>
                <li>Setting too low (&lt;cores)  underutilization</li>
                <li>Not adjusting for different data sizes</li>
            </ul>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 85: Broadcast Variables -->
<div class="question-content" id="q85">
    <div class="question-header">
        <h1 class="question-title">What is broadcast variable?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Broadcast variable</strong> is a read-only variable cached on each executor, allowing efficient sharing of large data (like lookup tables) across all tasks without shipping it with every task.</p>
            <p style="margin-top: 1rem;">Instead of sending data with each task, broadcast variables send data once to each executor, significantly reducing network overhead.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Broadcast</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Shared Data</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Optimization</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Network Efficiency</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Broadcast Variable Visualization
        </div>
        <div class="image-container">
            <svg width="900" height="600" viewBox="0 0 900 600">
                <rect width="900" height="600" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Broadcast Variables: Without vs With</text>
                
                <!-- WITHOUT BROADCAST -->
                <rect x="50" y="60" width="380" height="250" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="240" y="90" font-size="16" fill="#ef4444" text-anchor="middle" font-weight="bold">WITHOUT Broadcast (Inefficient)</text>
                
                <!-- Driver -->
                <rect x="180" y="110" width="120" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="240" y="135" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Driver</text>
                <text x="240" y="155" font-size="11" fill="#cbd5e1" text-anchor="middle">Lookup Table</text>
                <text x="240" y="170" font-size="10" fill="#f59e0b" text-anchor="middle">(10 MB)</text>
                
                <!-- Executors -->
                <rect x="70" y="210" width="100" height="80" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="120" y="235" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 1</text>
                <text x="120" y="255" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 1: +10MB</text>
                <text x="120" y="270" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 2: +10MB</text>
                <text x="120" y="285" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 3: +10MB</text>
                
                <rect x="190" y="210" width="100" height="80" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="240" y="235" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 2</text>
                <text x="240" y="255" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 4: +10MB</text>
                <text x="240" y="270" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 5: +10MB</text>
                <text x="240" y="285" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 6: +10MB</text>
                
                <rect x="310" y="210" width="100" height="80" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="360" y="235" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 3</text>
                <text x="360" y="255" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 7: +10MB</text>
                <text x="360" y="270" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 8: +10MB</text>
                <text x="360" y="285" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 9: +10MB</text>
                
                <!-- Arrows showing data sent with each task -->
                <path d="M 240 170 L 120 210" stroke="#ef4444" stroke-width="2" stroke-dasharray="3,3" marker-end="url(#arrowred2)"/>
                <path d="M 240 170 L 240 210" stroke="#ef4444" stroke-width="2" stroke-dasharray="3,3" marker-end="url(#arrowred2)"/>
                <path d="M 240 170 L 360 210" stroke="#ef4444" stroke-width="2" stroke-dasharray="3,3" marker-end="url(#arrowred2)"/>
                
                <text x="240" y="300" font-size="11" fill="#ef4444" text-anchor="middle" font-weight="bold">Total: 90 MB transferred (9 tasks  10 MB)</text>
                
                <!-- WITH BROADCAST -->
                <rect x="470" y="60" width="380" height="250" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="660" y="90" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">WITH Broadcast (Efficient)</text>
                
                <!-- Driver -->
                <rect x="600" y="110" width="120" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="660" y="135" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Driver</text>
                <text x="660" y="155" font-size="11" fill="#cbd5e1" text-anchor="middle">Broadcast Variable</text>
                <text x="660" y="170" font-size="10" fill="#10b981" text-anchor="middle">(10 MB)</text>
                
                <!-- Executors with cached broadcast -->
                <rect x="490" y="210" width="100" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="540" y="235" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 1</text>
                <text x="540" y="255" font-size="10" fill="#10b981" text-anchor="middle"> Cached: 10MB</text>
                <text x="540" y="270" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 1,2,3</text>
                <text x="540" y="285" font-size="10" fill="#cbd5e1" text-anchor="middle">share cache</text>
                
                <rect x="610" y="210" width="100" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="660" y="235" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 2</text>
                <text x="660" y="255" font-size="10" fill="#10b981" text-anchor="middle"> Cached: 10MB</text>
                <text x="660" y="270" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 4,5,6</text>
                <text x="660" y="285" font-size="10" fill="#cbd5e1" text-anchor="middle">share cache</text>
                
                <rect x="730" y="210" width="100" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="780" y="235" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Executor 3</text>
                <text x="780" y="255" font-size="10" fill="#10b981" text-anchor="middle"> Cached: 10MB</text>
                <text x="780" y="270" font-size="10" fill="#cbd5e1" text-anchor="middle">Task 7,8,9</text>
                <text x="780" y="285" font-size="10" fill="#cbd5e1" text-anchor="middle">share cache</text>
                
                <!-- Arrows showing one-time broadcast -->
                <path d="M 660 170 L 540 210" stroke="#10b981" stroke-width="3" marker-end="url(#arrowgreen4)"/>
                <path d="M 660 170 L 660 210" stroke="#10b981" stroke-width="3" marker-end="url(#arrowgreen4)"/>
                <path d="M 660 170 L 780 210" stroke="#10b981" stroke-width="3" marker-end="url(#arrowgreen4)"/>
                
                <text x="660" y="300" font-size="11" fill="#10b981" text-anchor="middle" font-weight="bold">Total: 30 MB transferred (3 executors  10 MB)</text>
                
                <!-- Comparison -->
                <rect x="150" y="340" width="600" height="80" fill="rgba(245, 158, 11, 0.2)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="450" y="370" font-size="16" fill="#f59e0b" text-anchor="middle" font-weight="bold">Broadcast Saves: 90 MB - 30 MB = 60 MB (67% reduction!)</text>
                <text x="450" y="395" font-size="13" fill="#cbd5e1" text-anchor="middle">With 100 tasks: 1000 MB vs 30 MB (97% reduction!)</text>
                <text x="450" y="415" font-size="13" fill="#cbd5e1" text-anchor="middle">More tasks = More savings</text>
                
                <!-- Use Case -->
                <rect x="50" y="450" width="800" height="120" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="450" y="480" font-size="16" fill="#3b82f6" text-anchor="middle" font-weight="bold">Common Use Cases</text>
                <text x="450" y="505" font-size="12" fill="#cbd5e1" text-anchor="middle"> Lookup tables (country codes, product catalogs)</text>
                <text x="450" y="525" font-size="12" fill="#cbd5e1" text-anchor="middle"> Machine learning models (for prediction)</text>
                <text x="450" y="545" font-size="12" fill="#cbd5e1" text-anchor="middle"> Broadcast joins (small table joined with large table)</text>
                
                <defs>
                    <marker id="arrowred2" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#ef4444"/>
                    </marker>
                    <marker id="arrowgreen4" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                        <polygon points="0 0, 10 3, 0 6" fill="#10b981"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Broadcast: Send once per executor vs send with every task</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Using Broadcast Variables
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-broadcast-tower"></i> Broadcast Variable Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example 1: Broadcast lookup table
# Scenario: Enrich transaction data with country names

# Small lookup table (can fit in memory)
country_lookup = {
    "US": "United States",
    "UK": "United Kingdom",
    "CA": "Canada",
    "IN": "India",
    # ... more countries
}

# Without broadcast (BAD - sent with every task)
def enrich_without_broadcast(country_code):
    return country_lookup.get(country_code, "Unknown")

transactions = spark.range(10000000).selectExpr("id % 4 as country_id")
# Each task gets full copy of country_lookup  wasteful!

# With broadcast (GOOD - sent once per executor)
broadcast_lookup = spark.sparkContext.broadcast(country_lookup)

def enrich_with_broadcast(country_code):
    # Access broadcast variable with .value
    return broadcast_lookup.value.get(country_code, "Unknown")

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

enrich_udf = udf(enrich_with_broadcast, StringType())
result = transactions.withColumn("country_name", enrich_udf("country_id"))
result.show()

# Example 2: Broadcast join (most common use case)
from pyspark.sql.functions import broadcast

# Large fact table (1 GB)
transactions = spark.read.parquet("transactions.parquet")

# Small dimension table (10 MB)
products = spark.read.parquet("products.parquet")

# Without broadcast: Both tables shuffled
# regular_join = transactions.join(products, "product_id")

# With broadcast: Only large table shuffled (if needed)
broadcast_join = transactions.join(broadcast(products), "product_id")
# products broadcasted to all executors, no shuffle for small table!

broadcast_join.explain()
# Look for: "BroadcastHashJoin" in plan

# Example 3: Broadcast ML model
import pickle

# Train model (simplified)
model_data = {"weights": [0.5, 0.3, 0.2], "bias": 0.1}

# Broadcast model
broadcast_model = spark.sparkContext.broadcast(model_data)

def predict(features):
    # Access model from broadcast variable
    model = broadcast_model.value
    weights = model["weights"]
    bias = model["bias"]
    # Simple prediction
    return sum(w * f for w, f in zip(weights, features)) + bias

# Use in transformation
# data.rdd.map(lambda row: predict(row.features))

# Example 4: Check broadcast variable
print(f"Broadcast ID: {broadcast_lookup.id}")
print(f"Is broadcast: {broadcast_lookup.value}")  # Access the value

# Unpersist broadcast variable when done
broadcast_lookup.unpersist()
# Frees memory on executors</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-chart-line"></i> Performance Comparison</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>import time

# Setup: 1M records, 10K lookup entries
transactions = spark.range(1000000).selectExpr("id % 100 as product_id")
products_data = [(i, f"Product_{i}") for i in range(10000)]
products = spark.createDataFrame(products_data, ["product_id", "product_name"])

# Test 1: Regular join (both tables shuffle)
start = time.time()
regular_join = transactions.join(products, "product_id")
regular_join.count()
regular_time = time.time() - start
print(f"Regular join: {regular_time:.2f}s")

# Test 2: Broadcast join
start = time.time()
broadcast_join = transactions.join(broadcast(products), "product_id")
broadcast_join.count()
broadcast_time = time.time() - start
print(f"Broadcast join: {broadcast_time:.2f}s")

# Results:
# Regular join: 15.23s
# Broadcast join: 3.45s (4x faster!)

print(f"Speedup: {regular_time/broadcast_time:.2f}x")</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When should you use it?</strong></span>
                </div>
                <div class="cross-answer">
                    Use broadcast variables when:<br><br>
                    <strong>1. Small lookup tables (&lt; 10 MB recommended, &lt; 100 MB max):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Country codes, product catalogs, configuration maps
country_map = spark.sparkContext.broadcast(country_dict)</code></pre>
                    <strong>2. Broadcast joins (one small table + one large table):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Small dimension table (&lt; 10 MB), large fact table (GBs)
large_df.join(broadcast(small_df), "key")</code></pre>
                    <strong>3. Sharing read-only data across tasks:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># ML models, configuration, static reference data
model = spark.sparkContext.broadcast(trained_model)</code></pre>
                    <strong>4. Many tasks need same data:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 1000 tasks  10 MB = 10 GB without broadcast
# 10 executors  10 MB = 100 MB with broadcast (100x reduction!)</code></pre>
                    <strong>5. Filter operations with external lists:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>valid_ids = spark.sparkContext.broadcast(set([1, 2, 3, ...]))
df.filter(lambda row: row.id in valid_ids.value)</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What problem does it solve?</strong></span>
                </div>
                <div class="cross-answer">
                    Broadcast variables solve the <strong>data shipping problem</strong>:<br><br>
                    <strong>Problem without broadcast:</strong><br>
                     Large data sent with every task<br>
                     Network congestion<br>
                     Memory waste (multiple copies per executor)<br>
                     Slow task startup<br><br>
                    <strong>Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 10 MB lookup table
# 1000 tasks across 20 executors
# Without broadcast: 1000  10 MB = 10 GB transferred!
# With broadcast: 20  10 MB = 200 MB transferred (50x reduction)</code></pre>
                    <strong>Solution with broadcast:</strong><br>
                     Data sent once per executor (not per task)<br>
                     Cached in executor memory<br>
                     All tasks on executor share one copy<br>
                     Dramatically reduces network traffic<br>
                     Faster task execution<br><br>
                    <strong>Specific problems solved:</strong><br>
                    1. <strong>Network bottleneck:</strong> Reduces network traffic by 10-100x<br>
                    2. <strong>Memory efficiency:</strong> One copy per executor vs one per task<br>
                    3. <strong>Join optimization:</strong> Avoids shuffle for small table in joins<br>
                    4. <strong>Task overhead:</strong> Faster task startup (no data serialization per task)<br><br>
                    <strong>Real-world impact:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Before broadcast: 45 minutes
large_df.join(small_df, "key").count()

# After broadcast: 5 minutes (9x faster!)
large_df.join(broadcast(small_df), "key").count()</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 88: Broadcast Under the Hood -->
<div class="question-content" id="q88">
    <div class="question-header">
        <h1 class="question-title">How does Spark handle broadcast variables under the hood?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Broadcast Internal Mechanism
        </div>
        <div class="definition-box">
            <p>Spark uses a distributed caching mechanism to efficiently distribute broadcast variables. The process involves serialization, chunking, distribution via BitTorrent-like protocol, and caching in executor memory.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-cogs"></i>
            Broadcast Process
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-list-ol"></i> Step-by-Step Process
            </div>
            <div style="margin-top: 1rem;">
                <strong>1. Creation (Driver):</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>broadcast_var = spark.sparkContext.broadcast(data)

# What happens:
# - Data serialized to bytes
# - Stored in driver's BlockManager
# - Assigned unique broadcast ID
# - Divided into chunks (blocks) for transfer</code></pre>
                <strong>2. Distribution (On First Access):</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># When executor needs broadcast variable:
# - Executor requests from driver
# - Driver sends chunks via HTTP/torrent protocol
# - Multiple executors can fetch simultaneously
# - Peer-to-peer sharing between executors (torrent-like)</code></pre>
                <strong>3. Caching (Executor):</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># On executor:
# - Chunks received and reassembled
# - Stored in executor's BlockManager
# - Cached in memory (or disk if too large)
# - All tasks on executor share same cached copy</code></pre>
                <strong>4. Access (Tasks):</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># In task:
value = broadcast_var.value  # Fetches from local cache
# - No network call (already cached)
# - Deserialized on first access
# - Fast read from executor memory</code></pre>
                <strong>5. Cleanup:</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>broadcast_var.unpersist()

# What happens:
# - Removes from all executors' BlockManagers
# - Frees memory
# - Can't be accessed after unpersist</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-network-wired"></i> Distribution Protocol
            </div>
            <div style="margin-top: 1rem;">
                <p style="color: var(--text-secondary);">
                    Spark uses <strong>TorrentBroadcast</strong> by default (for data > 1 MB):<br><br>
                    1. Driver splits data into blocks (~4 MB each)<br>
                    2. Driver acts as initial seed<br>
                    3. First executors fetch from driver<br>
                    4. Later executors can fetch from driver OR other executors<br>
                    5. Reduces load on driver as more executors have data<br>
                    6. Scales well for large clusters
                </p>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Visual representation:
# Time 0: Driver has all blocks [A,B,C,D]
# Time 1: Executor 1 fetches from Driver  [A,B,C,D]
# Time 2: Executor 2 fetches from Driver  [A,B,C,D]
# Time 3: Executor 3 fetches from Driver OR Exec1/2  [A,B,C,D]
# Time 4: Executor 4 fetches from any available source  [A,B,C,D]</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Where are broadcast variables stored on executors?</strong></span>
                </div>
                <div class="cross-answer">
                    Broadcast variables are stored in the <strong>executor's BlockManager</strong>:<br><br>
                    <strong>Storage Location:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Primary: Executor memory (heap)
# - Stored as deserialized Java objects
# - In BlockManager's memory store
# - Shared across all tasks on executor

# Fallback: Executor disk (if memory full)
# - Serialized to disk
# - Slower access but prevents OOM
# - Automatically managed by BlockManager</code></pre>
                    <strong>Storage Hierarchy:</strong><br>
                    1. <strong>Driver:</strong> Original data in driver's BlockManager<br>
                    2. <strong>Executor Memory:</strong> Cached deserialized copy (preferred)<br>
                    3. <strong>Executor Disk:</strong> Spilled if memory insufficient<br>
                    4. <strong>Remote Executors:</strong> Can fetch from peer executors<br><br>
                    <strong>Memory Configuration:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Broadcast variables use executor memory
# Part of: spark.executor.memory

# If broadcast too large, may spill to disk
# Configure executor memory accordingly:
spark.conf.set("spark.executor.memory", "4g")

# Max broadcast size (auto-broadcast for joins)
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")</code></pre>
                    <strong>BlockManager Details:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Check broadcast variable storage in Spark UI:
# 1. Go to: http://localhost:4040
# 2. Click: "Storage" tab
# 3. Look for: "broadcast_X" (X = broadcast ID)
# 4. See: Size, location (memory/disk), replicas

# Example output:
# Broadcast ID: broadcast_0
# Size: 10.5 MB
# Storage Level: Memory Deserialized
# Cached Partitions: 1
# Fraction Cached: 100%</code></pre>
                    <strong>Lifecycle:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># 1. First task on executor needs broadcast
#     Fetch from driver/peer
#     Store in BlockManager
#     Keep in memory

# 2. Subsequent tasks on same executor
#     Read from BlockManager cache
#     No network call needed

# 3. After unpersist()
#     Removed from BlockManager
#     Memory freed</code></pre>
                </div>
            </div>
        </cross-questions>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
</div>

<!-- Question 89: Accumulators -->
<div class="question-content" id="q89">
    <div class="question-header">
        <h1 class="question-title">What is accumulator?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Accumulator</strong> is a shared variable that executors can only add to (not read), and only the driver can read the final value. Used for counters and sums across distributed operations.</p>
            <p style="margin-top: 1rem;">Think of it as a write-only variable for executors, read-only for driver.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Accumulator</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Counter</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Aggregation</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Monitoring</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Accumulator Visualization
        </div>
        <div class="image-container">
            <svg width="900" height="500" viewBox="0 0 900 500">
                <rect width="900" height="500" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Accumulator: Write-Only for Executors</text>
                
                <!-- Driver -->
                <rect x="350" y="70" width="200" height="80" fill="#334155" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="450" y="100" font-size="16" fill="#3b82f6" text-anchor="middle" font-weight="bold">Driver</text>
                <text x="450" y="125" font-size="13" fill="#10b981" text-anchor="middle"> Can CREATE accumulator</text>
                <text x="450" y="145" font-size="13" fill="#10b981" text-anchor="middle"> Can READ final value</text>
                
                <!-- Accumulator value -->
                <rect x="375" y="180" width="150" height="50" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="450" y="210" font-size="16" fill="#f1f5f9" text-anchor="middle" font-weight="bold">acc.value = 150</text>
                
                <!-- Executors -->
                <rect x="70" y="280" width="230" height="180" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="185" y="310" font-size="15" fill="#ef4444" text-anchor="middle" font-weight="bold">Executor 1</text>
                
                <rect x="90" y="330" width="190" height="60" fill="#334155" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="185" y="355" font-size="12" fill="#10b981" text-anchor="middle"> Can ADD to accumulator</text>
                <text x="185" y="375" font-size="12" fill="#cbd5e1" text-anchor="middle">acc.add(10)</text>
                
                <rect x="90" y="400" width="190" height="50" fill="#334155" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="185" y="425" font-size="12" fill="#ef4444" text-anchor="middle"> CANNOT READ value</text>
                <text x="185" y="440" font-size="11" fill="#cbd5e1" text-anchor="middle">acc.value  Error!</text>
                
                <rect x="330" y="280" width="230" height="180" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="445" y="310" font-size="15" fill="#ef4444" text-anchor="middle" font-weight="bold">Executor 2</text>
                
                <rect x="350" y="330" width="190" height="60" fill="#334155" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="445" y="355" font-size="12" fill="#10b981" text-anchor="middle"> Can ADD to accumulator</text>
                <text x="445" y="375" font-size="12" fill="#cbd5e1" text-anchor="middle">acc.add(20)</text>
                
                <rect x="350" y="400" width="190" height="50" fill="#334155" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="445" y="425" font-size="12" fill="#ef4444" text-anchor="middle"> CANNOT READ value</text>
                <text x="445" y="440" font-size="11" fill="#cbd5e1" text-anchor="middle">acc.value  Error!</text>
                
                <rect x="590" y="280" width="230" height="180" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="705" y="310" font-size="15" fill="#ef4444" text-anchor="middle" font-weight="bold">Executor 3</text>
                
                <rect x="610" y="330" width="190" height="60" fill="#334155" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="705" y="355" font-size="12" fill="#10b981" text-anchor="middle"> Can ADD to accumulator</text>
                <text x="705" y="375" font-size="12" fill="#cbd5e1" text-anchor="middle">acc.add(30)</text>
                
                <rect x="610" y="400" width="190" height="50" fill="#334155" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="705" y="425" font-size="12" fill="#ef4444" text-anchor="middle"> CANNOT READ value</text>
                <text x="705" y="440" font-size="11" fill="#cbd5e1" text-anchor="middle">acc.value  Error!</text>
                
                <!-- Arrows showing add operations -->
                <path d="M 185 280 L 420 230" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen5)"/>
                <text x="280" y="250" font-size="11" fill="#10b981" text-anchor="middle" font-weight="bold">+10</text>
                
                <path d="M 445 280 L 455 230" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen5)"/>
                <text x="455" y="250" font-size="11" fill="#10b981" text-anchor="middle" font-weight="bold">+20</text>
                
                <path d="M 705 280 L 480 230" stroke="#10b981" stroke-width="2" marker-end="url(#arrowgreen5)"/>
                <text x="620" y="250" font-size="11" fill="#10b981" text-anchor="middle" font-weight="bold">+30</text>
                
                <defs>
                    <marker id="arrowgreen5" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#10b981"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Accumulators: Executors can only add, Driver can only read</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Using Accumulators
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-plus-circle"></i> Accumulator Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Example 1: Basic counter
# Create accumulator on driver
error_count = spark.sparkContext.accumulator(0)

def process_record(record):
    try:
        # Process record
        result = int(record) * 2
        return result
    except Exception as e:
        # Count errors
        error_count.add(1)
        return None

# Use in RDD transformation
rdd = spark.sparkContext.parallelize(["1", "2", "abc", "4", "xyz"])
results = rdd.map(process_record)

# Trigger action (accumulators only updated after action)
results.collect()

# Read accumulator value (only on driver)
print(f"Total errors: {error_count.value}")  # 2 (abc and xyz)

# Example 2: Multiple accumulators
valid_records = spark.sparkContext.accumulator(0)
invalid_records = spark.sparkContext.accumulator(0)
total_amount = spark.sparkContext.accumulator(0.0)

def process_transaction(record):
    try:
        amount = float(record["amount"])
        if amount > 0:
            valid_records.add(1)
            total_amount.add(amount)
            return record
        else:
            invalid_records.add(1)
            return None
    except:
        invalid_records.add(1)
        return None

# Process data
transactions = spark.sparkContext.parallelize([
    {"amount": "100.50"},
    {"amount": "-50"},
    {"amount": "abc"},
    {"amount": "200.00"}
])
processed = transactions.map(process_transaction)
processed.count()  # Trigger action

# Read results
print(f"Valid: {valid_records.value}")      # 2
print(f"Invalid: {invalid_records.value}")  # 2
print(f"Total: ${total_amount.value}")      # 300.50

# Example 3: With DataFrame (using foreach)
blank_lines = spark.sparkContext.accumulator(0)
lines_with_errors = spark.sparkContext.accumulator(0)

df = spark.read.text("log_file.txt")

def count_errors(row):
    line = row.value
    if not line.strip():
        blank_lines.add(1)
    if "ERROR" in line:
        lines_with_errors.add(1)

# Use foreach (action that doesn't return data)
df.foreach(count_errors)

print(f"Blank lines: {blank_lines.value}")
print(f"Error lines: {lines_with_errors.value}")

# Example 4: Custom accumulator
from pyspark import AccumulatorParam

class SetAccumulatorParam(AccumulatorParam):
    def zero(self, initialValue):
        return set()
    
    def addInPlace(self, v1, v2):
        v1.update(v2)
        return v1

# Create custom accumulator
unique_categories = spark.sparkContext.accumulator(set(), SetAccumulatorParam())

def track_categories(record):
    unique_categories.add({record["category"]})
    return record

data = spark.sparkContext.parallelize([
    {"category": "A"},
    {"category": "B"},
    {"category": "A"},
    {"category": "C"}
])
data.map(track_categories).count()

print(f"Unique categories: {unique_categories.value}")  # {A, B, C}

# Important: Executors cannot read accumulator value
# This will ERROR on executor:
def bad_function(record):
    if error_count.value > 10:  # ERROR! Executors can't read
        return None
    return record</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you use accumulators to return values to the driver?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>No, you cannot use accumulators to return values to the driver in the traditional sense.</strong><br><br>
                    <strong>Why not:</strong><br>
                     Accumulators are for side-effect aggregation, not return values<br>
                     Use actions like <code>collect()</code>, <code>reduce()</code>, or <code>aggregate()</code> to return values<br>
                     Accumulators are best for monitoring/counting, not primary results<br><br>
                    <strong>What you CAN do:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Use accumulator for side information (counts, metrics)
error_count = spark.sparkContext.accumulator(0)

def process(x):
    try:
        result = x * 2
        return result
    except:
        error_count.add(1)  # Count errors as side effect
        return None

rdd = spark.sparkContext.parallelize([1, 2, 3])
results = rdd.map(process).collect()  # Main results via collect()
errors = error_count.value             # Side metric via accumulator

print(f"Results: {results}")  # [2, 4, 6]
print(f"Errors: {errors}")    # 0</code></pre>
                    <strong>What you CANNOT do:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># DON'T use accumulators as primary return mechanism
sum_acc = spark.sparkContext.accumulator(0)

def bad_pattern(x):
    sum_acc.add(x)
    return None  # Throwing away actual data!

rdd.map(bad_pattern).count()
result = sum_acc.value  # Bad! Use reduce() instead

# CORRECT approach:
result = rdd.reduce(lambda x, y: x + y)  # Proper aggregation</code></pre>
                    <strong>Best Practice:</strong><br>
                     Primary results: Use transformations + actions<br>
                     Monitoring/debugging: Use accumulators<br>
                     Counts/metrics: Use accumulators<br>
                     Actual data: Use collect(), reduce(), etc.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 90: What are Accumulators -->
<div class="question-content" id="q90">
    <div class="question-header">
        <h1 class="question-title">What are accumulators?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition (Similar to Q89)
        </div>
        <div class="definition-box">
            <p><strong>Accumulators</strong> are shared variables used for aggregating information across executors. They provide a simple way to count events or sum values in a distributed manner.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-star"></i>
            Key Characteristics
        </div>

        <div class="highlight-box">
            <strong>1. Write-Only for Executors:</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Tasks can only add to accumulators, not read them.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>acc.add(1)  #  OK in executor
value = acc.value  #  ERROR in executor</code></pre>
        </div>

        <div class="highlight-box">
            <strong>2. Read-Only for Driver:</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Only the driver program can read the accumulator's value.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># On driver:
print(acc.value)  #  OK</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Updated After Actions:</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Accumulators are only reliably updated after an action is executed.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>rdd.map(lambda x: acc.add(1))  # Transformation - not executed
print(acc.value)  # Still 0

rdd.map(lambda x: acc.add(1)).count()  # Action - now executed
print(acc.value)  # Updated!</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can accumulators be read by executors?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>No, executors cannot read accumulator values.</strong><br><br>
                    <strong>Why this restriction?</strong><br>
                     Prevents race conditions<br>
                     Ensures consistency<br>
                     Simplifies distributed aggregation<br>
                     Avoids complex synchronization<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># On executor (in transformation):
acc.add(1)      #  Allowed
val = acc.value #  NOT allowed (throws error or returns stale value)

# Only on driver:
print(acc.value)  #  Allowed</code></pre>
                    <strong>What executors CAN do:</strong><br>
                     Add values: <code>acc.add(value)</code><br>
                     Increment: <code>acc.add(1)</code><br>
                     Update: <code>acc.add(amount)</code><br><br>
                    <strong>What executors CANNOT do:</strong><br>
                     Read: <code>acc.value</code><br>
                     Conditional logic based on value<br>
                     Use value in calculations
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What are they used for?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Common Use Cases:</strong><br><br>
                    <strong>1. Counting events:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>error_count = spark.sparkContext.accumulator(0)
success_count = spark.sparkContext.accumulator(0)

def process(record):
    if is_valid(record):
        success_count.add(1)
    else:
        error_count.add(1)
    return record</code></pre>
                    <strong>2. Debugging and monitoring:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>null_values = spark.sparkContext.accumulator(0)
processed_rows = spark.sparkContext.accumulator(0)

# Track data quality issues</code></pre>
                    <strong>3. Summing values:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>total_revenue = spark.sparkContext.accumulator(0.0)

def calculate(sale):
    total_revenue.add(sale.amount)
    return sale</code></pre>
                    <strong>4. Statistics collection:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>min_value = spark.sparkContext.accumulator(float('inf'))
max_value = spark.sparkContext.accumulator(float('-inf'))

# Track min/max across distributed processing</code></pre>
                    <strong>5. Progress tracking:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>records_processed = spark.sparkContext.accumulator(0)

# Monitor job progress in real-time</code></pre>
                    <strong>NOT suitable for:</strong><br>
                     Primary return values (use collect/reduce instead)<br>
                     Complex data structures that executors need to read<br>
                     Aggregations that need intermediate results
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 91: Accumulator Details -->
<div class="question-content" id="q91">
    <div class="question-header">
        <h1 class="question-title">What is an accumulator?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Comprehensive Definition
        </div>
        <div class="definition-box">
            <p><strong>Accumulator</strong> is a shared variable for aggregating information from executors to the driver. It provides write-only access for workers and read access for the driver, enabling distributed counters and sums.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Complete Example
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-plus-square"></i> Comprehensive Accumulator Usage</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Create accumulator
error_accumulator = spark.sparkContext.accumulator(0)

def process_with_accumulator(value):
    try:
        result = int(value) * 2
        return result
    except ValueError:
        # Increment accumulator on error
        error_accumulator.add(1)
        return None

# Apply transformation
rdd = spark.sparkContext.parallelize(["1", "2", "bad", "4", "error"])
processed = rdd.map(process_with_accumulator)

# Must call action to update accumulator
results = processed.collect()

# Read value on driver
print(f"Errors encountered: {error_accumulator.value}")  # 2
print(f"Results: {[r for r in results if r is not None]}")  # [2, 4, 8]</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you read the value of an accumulator inside transformations? Why or why not?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>No, you cannot read accumulator values inside transformations (on executors).</strong><br><br>
                    <strong>Technical Reason:</strong><br>
                     Accumulators are designed as write-only from executor perspective<br>
                     Reading would require synchronization across all executors<br>
                     Would create race conditions and consistency issues<br>
                     Would defeat the purpose of distributed, independent task execution<br><br>
                    <strong>Example of what doesn't work:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>counter = spark.sparkContext.accumulator(0)

def bad_function(x):
    counter.add(1)
    # This will NOT work as expected on executor
    if counter.value > 10:  #  Don't do this!
        return "stop"
    return x

rdd.map(bad_function)  # Unreliable behavior</code></pre>
                    <strong>Why this restriction exists:</strong><br><br>
                    <strong>1. Distributed Nature:</strong> Tasks run independently on different executors. If they could read the accumulator, they'd get stale/inconsistent values since updates haven't been aggregated yet.<br><br>
                    <strong>2. Update Timing:</strong> Accumulators are only updated after an action completes. During transformation, the value isn't finalized.<br><br>
                    <strong>3. Race Conditions:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># If reading were allowed:
# Task 1 on Executor A: counter.add(1), reads 1
# Task 2 on Executor B: counter.add(1), reads 1 (wrong! should be 2)
# Task 3 on Executor A: counter.add(1), reads 2 (wrong! should be 3)
# Result: Chaos and inconsistency!</code></pre>
                    <strong>4. Performance:</strong> Allowing reads would require constant synchronization between executors and driver, killing performance.<br><br>
                    <strong>Correct Pattern:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Do aggregation/counting with accumulators
def process(x):
    counter.add(1)  #  Write only
    return x * 2

result = rdd.map(process).collect()

# Read accumulator AFTER action on driver
print(f"Processed {counter.value} records")  #  Correct</code></pre>
                    <strong>Alternative when you need conditional logic:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Use local variables or pass data explicitly
def process_with_limit(x, limit=10):
    if x > limit:  # Use parameter, not accumulator
        return None
    return x * 2

# Or use filter/reduce for conditional logic
rdd.filter(lambda x: x <= 10).map(lambda x: x * 2)</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How are they different from broadcast variables?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Key Differences:</strong><br><br>
                    <div style="margin-top: 0.5rem;">
                        <table style="width: 100%; border-collapse: collapse; margin-top: 0.5rem;">
                            <thead style="background: rgba(59, 130, 246, 0.1);">
                                <tr>
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">Aspect</th>
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">Accumulator</th>
                                    <th style="padding: 0.75rem; text-align: left; border: 1px solid var(--border-color);">Broadcast Variable</th>
                                </tr>
                            </thead>
                            <tbody style="color: var(--text-secondary);">
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Direction</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Executor  Driver</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Driver  Executor</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Executor Access</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Write-only (add)</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Read-only (value)</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Driver Access</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Read-only (value)</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Read-only (value)</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Purpose</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Aggregate/count from executors</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Share data to executors</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Size</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Small (counters, sums)</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Can be large (lookup tables)</td>
                                </tr>
                                <tr>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);"><strong>Use Case</strong></td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Counting errors, monitoring</td>
                                    <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Lookup tables, ML models</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <br>
                    <strong>Visual Analogy:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Accumulator: Like a donation box
# - Everyone can PUT money in (executor writes)
# - Only organizer can COUNT total (driver reads)

# Broadcast: Like a bulletin board
# - Organizer POSTS information (driver writes)
# - Everyone can READ it (executor reads)</code></pre>
                    <strong>Code Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Accumulator: Executor  Driver
error_count = spark.sparkContext.accumulator(0)

def process(x):
    error_count.add(1)      #  Executor writes
    # error_count.value     #  Executor can't read
    return x

rdd.map(process).collect()
print(error_count.value)    #  Driver reads

# Broadcast: Driver  Executor
lookup = {"a": 1, "b": 2}
broadcast_var = spark.sparkContext.broadcast(lookup)

def enrich(x):
    return broadcast_var.value.get(x)  #  Executor reads
    # broadcast_var.value["c"] = 3     #  Executor can't write

rdd.map(enrich).collect()</code></pre>
                    <strong>When to use which:</strong><br>
                     <strong>Accumulator:</strong> Need to collect counts/sums from distributed tasks<br>
                     <strong>Broadcast:</strong> Need to share reference data to all tasks<br>
                     <strong>Both:</strong> Share lookup table (broadcast) + count usage (accumulator)
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 92: Role of Accumulator Variables -->
<div class="question-content" id="q92">
    <div class="question-header">
        <h1 class="question-title">What is the role of accumulator variables?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Role and Purpose
        </div>
        <div class="definition-box">
            <p>The role of accumulator variables is to provide a mechanism for aggregating information from worker nodes back to the driver in a distributed computation, primarily for monitoring, debugging, and collecting statistics.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-list"></i>
            Main Roles
        </div>

        <div class="highlight-box">
            <strong>1. Monitoring and Debugging:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Track errors during processing
errors = spark.sparkContext.accumulator(0)
warnings = spark.sparkContext.accumulator(0)

def process(record):
    if has_error(record):
        errors.add(1)
    elif has_warning(record):
        warnings.add(1)
    return transform(record)

# Monitor job health
print(f"Errors: {errors.value}, Warnings: {warnings.value}")</code></pre>
        </div>

        <div class="highlight-box">
            <strong>2. Counting Events:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Count different types of records
valid_records = spark.sparkContext.accumulator(0)
invalid_records = spark.sparkContext.accumulator(0)
null_records = spark.sparkContext.accumulator(0)

# Track record quality across distributed processing</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Collecting Statistics:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Aggregate metrics
total_amount = spark.sparkContext.accumulator(0.0)
record_count = spark.sparkContext.accumulator(0)

# Calculate distributed sum without using reduce</code></pre>
        </div>

        <div class="highlight-box">
            <strong>4. Progress Tracking:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Track job progress
processed = spark.sparkContext.accumulator(0)

def process_with_tracking(record):
    processed.add(1)
    if processed.value % 10000 == 0:
        print(f"Processed {processed.value} records")
    return record</code></pre>
        </div>

        <div class="warning-box">
            <strong> What Accumulators Are NOT For:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Primary computation results (use reduce/aggregate)</li>
                <li>Synchronization between tasks</li>
                <li>Conditional logic based on accumulated values</li>
                <li>Passing data between executors</li>
            </ul>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 93: Catalyst Optimizer -->
<div class="question-content" id="q93">
    <div class="question-header">
        <h1 class="question-title">Explain the Catalyst Optimizer</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 15 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            What is Catalyst?
        </div>
        <div class="definition-box">
            <p><strong>Catalyst Optimizer</strong> is Spark SQL's query optimization framework that transforms logical plans into optimized physical execution plans. It uses rule-based and cost-based optimization to make queries run faster.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Query Optimization</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Logical Plan</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Physical Plan</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Catalyst Optimization Phases
        </div>
        <div class="image-container">
            <svg width="900" height="650" viewBox="0 0 900 650">
                <rect width="900" height="650" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Catalyst Optimizer Pipeline</text>
                
                <!-- Phase 1: Analysis -->
                <rect x="100" y="70" width="700" height="80" fill="rgba(59, 130, 246, 0.2)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="130" y="100" font-size="16" fill="#3b82f6" text-anchor="start" font-weight="bold">1. Analysis</text>
                <text x="130" y="120" font-size="12" fill="#cbd5e1" text-anchor="start"> Parse SQL/DataFrame operations</text>
                <text x="130" y="138" font-size="12" fill="#cbd5e1" text-anchor="start"> Resolve table/column names from catalog</text>
                
                <path d="M 450 150 L 450 180" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown1)"/>
                
                <!-- Phase 2: Logical Optimization -->
                <rect x="100" y="180" width="700" height="110" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="130" y="210" font-size="16" fill="#10b981" text-anchor="start" font-weight="bold">2. Logical Optimization (Rule-Based)</text>
                <text x="130" y="230" font-size="12" fill="#cbd5e1" text-anchor="start"> Predicate pushdown (filter early)</text>
                <text x="130" y="248" font-size="12" fill="#cbd5e1" text-anchor="start"> Column pruning (select only needed columns)</text>
                <text x="130" y="266" font-size="12" fill="#cbd5e1" text-anchor="start"> Constant folding (evaluate constants)</text>
                <text x="130" y="284" font-size="12" fill="#cbd5e1" text-anchor="start"> Boolean expression simplification</text>
                
                <path d="M 450 290 L 450 320" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown1)"/>
                
                <!-- Phase 3: Physical Planning -->
                <rect x="100" y="320" width="700" height="100" fill="rgba(245, 158, 11, 0.2)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="130" y="350" font-size="16" fill="#f59e0b" text-anchor="start" font-weight="bold">3. Physical Planning (Cost-Based)</text>
                <text x="130" y="370" font-size="12" fill="#cbd5e1" text-anchor="start"> Generate multiple physical plans</text>
                <text x="130" y="388" font-size="12" fill="#cbd5e1" text-anchor="start"> Choose join strategy (broadcast vs shuffle vs sort-merge)</text>
                <text x="130" y="406" font-size="12" fill="#cbd5e1" text-anchor="start"> Estimate costs using statistics</text>
                
                <path d="M 450 420 L 450 450" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown1)"/>
                
                <!-- Phase 4: Code Generation -->
                <rect x="100" y="450" width="700" height="80" fill="rgba(168, 85, 247, 0.2)" stroke="#a855f7" stroke-width="2" rx="12"/>
                <text x="130" y="480" font-size="16" fill="#a855f7" text-anchor="start" font-weight="bold">4. Code Generation (Tungsten)</text>
                <text x="130" y="500" font-size="12" fill="#cbd5e1" text-anchor="start"> Generate optimized Java bytecode</text>
                <text x="130" y="518" font-size="12" fill="#cbd5e1" text-anchor="start"> Whole-stage code generation (fuses operators)</text>
                
                <path d="M 450 530 L 450 560" stroke="#10b981" stroke-width="3" marker-end="url(#arrowdown1)"/>
                
                <!-- Result -->
                <rect x="100" y="560" width="700" height="60" fill="rgba(239, 68, 68, 0.2)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="450" y="595" font-size="16" fill="#ef4444" text-anchor="middle" font-weight="bold">Optimized Executable Code</text>
                
                <defs>
                    <marker id="arrowdown1" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#10b981"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Four phases of Catalyst optimization</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Catalyst in Action
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-magic"></i> Optimization Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

# Original query (user writes)
df = spark.read.parquet("large_table.parquet")
result = df.select("id", "name", "age", "salary", "department") \
           .filter(col("age") > 25) \
           .filter(col("department") == "Engineering") \
           .select("name", "salary")

# What Catalyst does:

# 1. ANALYSIS
# - Resolves table: large_table.parquet exists
# - Resolves columns: id, name, age, salary, department exist
# - Type checking: age is integer, department is string

# 2. LOGICAL OPTIMIZATION
# Before optimization:
# Project(name, salary)
#   Filter(department = "Engineering")
#     Filter(age > 25)
#       Project(id, name, age, salary, department)
#         Scan(large_table.parquet)

# After optimization:
# Project(name, salary)                     Keep only needed columns
#   Filter(age > 25 AND dept = "Eng")      Combine filters
#     Project(name, age, salary, dept)      Column pruning (no id)
#       Scan(large_table.parquet)           Predicate pushdown to scan

# Optimizations applied:
#  Column Pruning: Only reads name, age, salary, department (not id)
#  Predicate Pushdown: Filter pushed to Parquet reader (reads less data)
#  Filter Combination: Two filters combined into one
#  Constant Folding: "Engineering" string optimized

# 3. PHYSICAL PLANNING
# Chooses:
# - Read strategy: Parquet with predicate pushdown
# - Filter strategy: Whole-stage code generation
# - Projection strategy: In-memory projection

# 4. CODE GENERATION
# Generates efficient bytecode that:
# - Reads only needed columns from Parquet
# - Applies both filters in single pass
# - Projects final columns efficiently

# View the optimized plan
result.explain(True)

# Output shows:
# == Analyzed Logical Plan ==
# == Optimized Logical Plan ==
# == Physical Plan ==</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-eye"></i> Viewing Optimization</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># See what Catalyst does
df = spark.range(1000000)
filtered = df.filter(col("id") > 500).filter(col("id") < 1000)

# Extended explain shows all phases
filtered.explain(extended=True)

# Output:
# == Parsed Logical Plan ==
# Filter (id < 1000)
# +- Filter (id > 500)
#    +- Range (0, 1000000, step=1, splits=Some(16))

# == Analyzed Logical Plan ==
# (same as above, but with types resolved)

# == Optimized Logical Plan ==
# Filter ((id > 500) AND (id < 1000))   Combined!
# +- Range (0, 1000000, step=1, splits=Some(16))

# == Physical Plan ==
# *(1) Filter ((id > 500) AND (id < 1000))
# +- *(1) Range (0, 1000000, step=1, splits=16)

# Mode explain
filtered.explain(mode="formatted")  # Pretty printed
filtered.explain(mode="cost")       # With cost estimates
filtered.explain(mode="codegen")    # Generated code</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which type of optimizations does it perform? (logical vs physical)</strong></span>
                </div>
                <div class="cross-answer">
                    Catalyst performs both <strong>logical</strong> and <strong>physical</strong> optimizations:<br><br>
                    <strong>LOGICAL OPTIMIZATIONS (Rule-Based):</strong><br>
                    These transform the query plan without considering execution details:<br><br>
                    <strong>1. Predicate Pushdown:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Push filters down to data source
df.select("*").filter(col("age") > 25)
#  Read only age > 25 records from Parquet</code></pre>
                    <strong>2. Column Pruning:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Read only needed columns
df.select("a", "b", "c").select("a", "b")
#  Only reads columns a, b from source</code></pre>
                    <strong>3. Constant Folding:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Evaluate constants at compile time
df.filter(col("x") > 1 + 2 + 3)
#  filter(x > 6)</code></pre>
                    <strong>4. Boolean Expression Simplification:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Simplify boolean logic
df.filter((col("x") > 5) | (col("x") > 5))
#  filter(x > 5)</code></pre>
                    <strong>5. Projection Elimination:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Remove unnecessary projections
df.select("a", "b").select("a", "b")
#  Single select</code></pre>
                    <br>
                    <strong>PHYSICAL OPTIMIZATIONS (Cost-Based):</strong><br>
                    These choose the best execution strategy based on data statistics:<br><br>
                    <strong>1. Join Strategy Selection:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Choose: Broadcast, Sort-Merge, or Shuffle Hash Join
# Based on table sizes from statistics
small_df.join(large_df, "key")
#  Broadcast Join (small table &lt; 10MB)</code></pre>
                    <strong>2. Sort vs Hash Aggregation:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Choose aggregation algorithm based on:
# - Number of groups
# - Available memory
# - Data distribution</code></pre>
                    <strong>3. Partition Count:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># With Adaptive Query Execution (AQE)
# Dynamically adjust partition count based on data size</code></pre>
                    <strong>4. Join Reordering:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Reorder multi-way joins for efficiency
# A join B join C
#  Choose optimal order based on sizes</code></pre>
                    <br>
                    <strong>Summary:</strong><br>
                     <strong>Logical:</strong> WHAT to do (query transformation)<br>
                     <strong>Physical:</strong> HOW to do it (execution strategy)
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How does it differ from the Tungsten execution engine?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Catalyst</strong> and <strong>Tungsten</strong> are complementary but different:<br><br>
                    <strong>CATALYST (Query Optimizer):</strong><br>
                     Plans the query<br>
                     Decides WHAT operations to do<br>
                     Logical and physical optimization<br>
                     Creates execution plan<br>
                     Works at compile time<br><br>
                    <strong>TUNGSTEN (Execution Engine):</strong><br>
                     Executes the query<br>
                     Decides HOW to execute efficiently<br>
                     Memory management and code generation<br>
                     Runs the execution plan<br>
                     Works at runtime<br><br>
                    <strong>Relationship:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>SQL Query
    
CATALYST: Optimize query plan
    
Optimized Physical Plan
    
TUNGSTEN: Execute efficiently
    
Results</code></pre>
                    <strong>Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Query
df.filter(col("age") > 25).select("name")

# CATALYST decides:
# 1. Push filter to data source (logical)
# 2. Prune columns (logical)
# 3. Use specific filter algorithm (physical)

# TUNGSTEN executes:
# 1. Generate optimized bytecode
# 2. Use off-heap memory
# 3. Cache-aware algorithms
# 4. Vectorized operations</code></pre>
                    See Question 95 for more on Tungsten.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 95: Tungsten Execution Engine -->
<div class="question-content" id="q95">
    <div class="question-header">
        <h1 class="question-title">What is the Tungsten execution engine?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            What is Tungsten?
        </div>
        <div class="definition-box">
            <p><strong>Tungsten</strong> is Spark's execution engine focused on improving CPU and memory efficiency. It provides whole-stage code generation, memory management, cache-aware computation, and binary processing to make Spark queries run faster.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Execution Engine</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Code Generation</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Memory Management</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-star"></i>
            Key Features of Tungsten
        </div>

        <div class="highlight-box">
            <strong>1. Whole-Stage Code Generation:</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Generates optimized Java bytecode for entire query stages, eliminating virtual function calls and combining multiple operators into single generated function.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Instead of: Iterator  Filter  Map  Filter  ...
# Generates: Single fused function that does everything
# Result: 2-10x faster execution</code></pre>
        </div>

        <div class="highlight-box">
            <strong>2. Off-Heap Memory Management:</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Manages memory outside JVM heap to avoid garbage collection overhead.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Benefits:
# - Reduce GC pauses
# - More predictable performance
# - Better memory utilization
# - Direct memory access</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Cache-Aware Computation:</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Algorithms designed to maximize CPU cache utilization.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Optimizations:
# - Sequential memory access patterns
# - Cache-friendly data structures
# - Reduced memory bandwidth requirements</code></pre>
        </div>

        <div class="highlight-box">
            <strong>4. Binary Processing:</strong>
            <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                Operates directly on binary data without deserialization.
            </p>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Skip object creation overhead
# Work with raw bytes directly
# Faster serialization/deserialization</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Generation Example
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> Before and After Tungsten</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Query
df.filter(col("age") > 25).filter(col("salary") > 50000).select("name")

# WITHOUT Tungsten (traditional execution):
# - Iterator calls for each row
# - Virtual function calls for each operator
# - Object creation for each intermediate result

for row in scan("table"):
    # Filter 1 (virtual call)
    if row.get("age") > 25:
        # Filter 2 (virtual call)
        if row.get("salary") > 50000:
            # Project (virtual call)
            result = row.get("name")
            yield result

# WITH Tungsten (whole-stage code generation):
# - Single generated function
# - No virtual calls
# - No intermediate objects

// Generated code (simplified Java):
public class GeneratedIterator extends BufferedRowIterator {
    public void processNext() {
        while (scan.hasNext()) {
            InternalRow row = scan.next();
            int age = row.getInt(1);
            int salary = row.getInt(2);
            // Fused filters (no virtual calls!)
            if (age > 25 && salary > 50000) {
                String name = row.getString(0);
                currentRow.setString(0, name);
                return;
            }
        }
    }
}

# Performance: 5-10x faster!

# View generated code
df.filter(col("age") > 25).explain(mode="codegen")

# Look for "WholeStageCodegen" in explain output:
# == Physical Plan ==
# *(1) Project [name#0]
# +- *(1) Filter ((age#1 > 25) && (salary#2 > 50000))
#    +- *(1) Scan
# 
# The *(1) indicates whole-stage code generation</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What problem was it solving compared to earlier Spark versions?</strong></span>
                </div>
                <div class="cross-answer">
                    Tungsten solved critical <strong>CPU and memory efficiency problems</strong> in earlier Spark versions:<br><br>
                    <strong>Problems in Earlier Spark (pre-Tungsten):</strong><br><br>
                    <strong>1. CPU Inefficiency:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Problem: Iterator-based processing with virtual calls
# Each operator: filter, map, etc. = separate function call
# Millions of rows  multiple operators = billions of virtual calls
# Result: 50-80% time spent in virtual function dispatch!</code></pre>
                    <strong>2. Memory Overhead:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Problem: Java object overhead
# Each row stored as Java object
# String "hello"  48 bytes in memory (not 5!)
# Result: 3-5x memory overhead

# Example:
# 1 GB data  3-5 GB in JVM heap
# Frequent garbage collection pauses</code></pre>
                    <strong>3. Serialization Cost:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Problem: Constant serialization/deserialization
# Object  Bytes  Object for each operation
# Result: 20-30% time spent in ser/deser</code></pre>
                    <strong>4. GC Pressure:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Problem: Many short-lived objects
# Intermediate results created and discarded
# Result: Frequent GC pauses (seconds to minutes)</code></pre>
                    <br>
                    <strong>How Tungsten Solved These:</strong><br><br>
                    <strong>1. CPU Efficiency (Code Generation):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Solution: Whole-stage code generation
# Fuses operators into single function
# Result: 2-10x faster, 90% less virtual calls</code></pre>
                    <strong>2. Memory Efficiency (Binary Format):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Solution: UnsafeRow (binary format)
# "hello"  5 bytes (not 48!)
# Result: 3-5x less memory usage</code></pre>
                    <strong>3. Reduced Serialization (Binary Processing):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Solution: Work on binary data directly
# Skip object creation/destruction
# Result: 5x faster serialization</code></pre>
                    <strong>4. GC Reduction (Off-Heap Memory):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Solution: Manage memory outside JVM heap
# Explicit memory management
# Result: Near-zero GC pauses</code></pre>
                    <br>
                    <strong>Performance Impact:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Same query, same data:
# Spark 1.5 (before Tungsten): 100 seconds
# Spark 1.6+ (with Tungsten): 20 seconds (5x faster!)

# Memory usage:
# Before: 50 GB for 10 GB data
# After: 15 GB for 10 GB data (70% reduction)</code></pre>
                    <strong>Summary:</strong> Tungsten made Spark's execution as fast as hand-optimized C++ code by eliminating JVM overhead.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 96: SQL Optimizations Internally -->
<div class="question-content" id="q96">
    <div class="question-header">
        <h1 class="question-title">How does Spark handle SQL optimizations internally?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Internal Optimization Process
        </div>
        <div class="definition-box">
            <p>Spark handles SQL optimizations through <strong>Catalyst Optimizer</strong> and <strong>Tungsten Engine</strong>, applying rule-based and cost-based optimizations to transform queries into efficient execution plans.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-list"></i>
            Key Internal Optimizations
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-filter"></i> 1. Predicate Pushdown
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>-- SQL Query
SELECT name FROM users WHERE age > 25;

-- Internal optimization:
-- Instead of: Read all  Filter age > 25
-- Optimized: Push filter to data source (Parquet/ORC)
-- Result: Only reads records where age > 25</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-columns"></i> 2. Column Pruning
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>-- SQL Query
SELECT name, age FROM users;

-- Internal optimization:
-- Only reads name and age columns from storage
-- Skips: address, phone, email, etc.
-- Result: 80% less data read if table has 10 columns</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-compress-arrows-alt"></i> 3. Join Reordering
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>-- SQL Query
SELECT * FROM small_table 
JOIN medium_table ON small.id = medium.id
JOIN large_table ON medium.id = large.id;

-- Internal optimization:
-- Analyzes table sizes from statistics
-- Reorders joins: (small JOIN medium) first, then JOIN large
-- Result: Minimizes intermediate data</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-broadcast-tower"></i> 4. Broadcast Join Detection
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>-- SQL Query
SELECT * FROM large_table 
JOIN small_lookup ON large.key = small.key;

-- Internal optimization:
-- Detects small_lookup < 10MB (autoBroadcastJoinThreshold)
-- Broadcasts small_lookup to all executors
-- Result: No shuffle for small table, 10x faster join</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-calculator"></i> 5. Constant Folding
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>-- SQL Query
SELECT price * (1 + 0.1) FROM products;

-- Internal optimization:
-- Evaluates (1 + 0.1) at compile time
-- Result: SELECT price * 1.1 FROM products;</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-chart-pie"></i> 6. Partition Pruning
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>-- SQL Query (table partitioned by year)
SELECT * FROM sales WHERE year = 2024;

-- Internal optimization:
-- Only scans partitions: /sales/year=2024/
-- Skips: /sales/year=2023/, /sales/year=2022/, etc.
-- Result: 90% less data scanned</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-code"></i> 7. Whole-Stage Code Generation
            </div>
            <div style="margin-top: 1rem;">
                <pre><code>-- SQL Query
SELECT name FROM users WHERE age > 25 AND city = 'NYC';

-- Internal optimization:
-- Generates single Java function for entire stage
-- No virtual calls, no intermediate objects
-- Result: 5-10x faster execution</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Example: Complete Optimization Flow
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-route"></i> SQL Optimization Pipeline</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Original SQL
sql = """
    SELECT u.name, u.age, o.total
    FROM users u
    JOIN orders o ON u.id = o.user_id
    WHERE u.age > 25 
      AND u.country = 'USA'
      AND o.date >= '2024-01-01'
    ORDER BY o.total DESC
    LIMIT 100
"""

df = spark.sql(sql)

# View optimizations
df.explain(extended=True)

# What happens internally:

# 1. PARSING
# Parse SQL  Abstract Syntax Tree (AST)

# 2. ANALYSIS
# Resolve tables: users, orders exist
# Resolve columns: name, age, id, user_id, total, country, date
# Type checking: age is INT, date is DATE

# 3. LOGICAL OPTIMIZATION (Catalyst)
# 
# Before:
# Limit 100
#  Sort [total DESC]
#   Project [name, age, total]
#    Join [u.id = o.user_id]
#     Filter [age > 25 AND country = 'USA']
#      Scan [users]
#     Filter [date >= '2024-01-01']
#      Scan [orders]
#
# After optimization:
# Limit 100
#  TakeOrderedAndProject [total DESC]
#   Join [u.id = o.user_id]
#    Project [id, name, age]                     Column pruning
#     Filter [age > 25 AND country = 'USA']      Predicate pushdown
#      Scan [users]                              Only reads id, name, age
#    Project [user_id, total]                    Column pruning
#     Filter [date >= '2024-01-01']              Predicate pushdown
#      Scan [orders]                             Only reads user_id, total, date
#
# Optimizations applied:
#  Predicate pushdown: Filters pushed to scan
#  Column pruning: Only needed columns read
#  Filter combination: Multiple filters merged
#  Sort optimization: Limit + Sort  TakeOrderedAndProject

# 4. PHYSICAL PLANNING (Cost-Based)
# 
# Analyzes statistics:
# - users: 1,000,000 rows, 500 MB
# - orders: 10,000,000 rows, 5 GB
# - After filters: users ~200,000, orders ~500,000
#
# Chooses:
# - Join type: Sort-Merge Join (both large after filter)
# - Sort algorithm: External sort
# - Partitioning: 200 partitions

# 5. CODE GENERATION (Tungsten)
# 
# Generates optimized bytecode:
# - Whole-stage codegen for filter + project
# - Fused operations in single function
# - No virtual calls, direct memory access

# Output shows all phases:
# == Analyzed Logical Plan ==
# == Optimized Logical Plan ==
# == Physical Plan ==
# *(3) TakeOrderedAndProject(limit=100, orderBy=[total#3 DESC])
# +- *(3) Project [name#0, age#1, total#3]
#    +- *(3) SortMergeJoin [id#2], [user_id#4]
#       :- *(1) Project [name#0, age#1, id#2]
#       :  +- *(1) Filter ((age#1 > 25) && (country#5 = USA))
#       :     +- *(1) FileScan parquet [name,age,id,country] 
#       :        PushedFilters: [IsNotNull(age), GreaterThan(age,25), ...]
#       +- *(2) Project [user_id#4, total#3]
#          +- *(2) Filter (date#6 >= 2024-01-01)
#             +- *(2) FileScan parquet [user_id,total,date]
#                PushedFilters: [IsNotNull(date), GreaterThanOrEqual(date, ...)]

# Notice:
# - *(1), *(2), *(3) = Whole-stage code generation
# - PushedFilters = Predicate pushdown to Parquet
# - Limited columns in FileScan = Column pruning</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 97: Predicate Pushdown -->
<div class="question-content" id="q97">
    <div class="question-header">
        <h1 class="question-title">What is predicate pushdown?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Definition
        </div>
        <div class="definition-box">
            <p><strong>Predicate pushdown</strong> is an optimization technique where filter conditions (predicates) are pushed down to the data source level, allowing the source to filter data during reading rather than after loading all data into memory.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Optimization</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Filter</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Source</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Predicate Pushdown Visualization
        </div>
        <div class="image-container">
            <svg width="900" height="500" viewBox="0 0 900 500">
                <rect width="900" height="500" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">Predicate Pushdown: Without vs With</text>
                
                <!-- WITHOUT Predicate Pushdown -->
                <rect x="50" y="60" width="380" height="400" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="240" y="90" font-size="16" fill="#ef4444" text-anchor="middle" font-weight="bold">WITHOUT Predicate Pushdown</text>
                
                <!-- Storage -->
                <rect x="130" y="120" width="220" height="60" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="240" y="145" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Storage (Parquet)</text>
                <text x="240" y="165" font-size="11" fill="#cbd5e1" text-anchor="middle">1 Million Rows</text>
                
                <path d="M 240 180 L 240 210" stroke="#ef4444" stroke-width="3" marker-end="url(#arrowred3)"/>
                <text x="270" y="200" font-size="11" fill="#ef4444" text-anchor="start" font-weight="bold">Read ALL</text>
                
                <!-- Read All Data -->
                <rect x="130" y="210" width="220" height="60" fill="rgba(239, 68, 68, 0.3)" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="240" y="235" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Spark Memory</text>
                <text x="240" y="255" font-size="11" fill="#cbd5e1" text-anchor="middle">1 Million Rows Loaded</text>
                
                <path d="M 240 270 L 240 300" stroke="#ef4444" stroke-width="3" marker-end="url(#arrowred3)"/>
                
                <!-- Filter -->
                <rect x="130" y="300" width="220" height="60" fill="rgba(239, 68, 68, 0.3)" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="240" y="325" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Filter (age > 25)</text>
                <text x="240" y="345" font-size="11" fill="#cbd5e1" text-anchor="middle">500,000 Rows Remain</text>
                
                <rect x="70" y="380" width="340" height="60" fill="rgba(239, 68, 68, 0.2)" stroke="#ef4444" stroke-width="1" rx="6"/>
                <text x="240" y="405" font-size="12" fill="#ef4444" text-anchor="middle" font-weight="bold"> Inefficient</text>
                <text x="240" y="425" font-size="11" fill="#cbd5e1" text-anchor="middle">Read 1M rows, discarded 500K</text>
                
                <!-- WITH Predicate Pushdown -->
                <rect x="470" y="60" width="380" height="400" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="660" y="90" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">WITH Predicate Pushdown</text>
                
                <!-- Storage with Filter -->
                <rect x="550" y="120" width="220" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="660" y="145" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Storage (Parquet)</text>
                <text x="660" y="165" font-size="11" fill="#10b981" text-anchor="middle" font-weight="bold">Filter Applied Here!</text>
                <text x="660" y="185" font-size="10" fill="#cbd5e1" text-anchor="middle">age > 25</text>
                
                <path d="M 660 200 L 660 230" stroke="#10b981" stroke-width="3" marker-end="url(#arrowgreen6)"/>
                <text x="690" y="220" font-size="11" fill="#10b981" text-anchor="start" font-weight="bold">Read 500K</text>
                
                <!-- Read Filtered Data -->
                <rect x="550" y="230" width="220" height="60" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="660" y="255" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Spark Memory</text>
                <text x="660" y="275" font-size="11" fill="#cbd5e1" text-anchor="middle">500,000 Rows Loaded</text>
                
                <path d="M 660 290 L 660 320" stroke="#10b981" stroke-width="3" marker-end="url(#arrowgreen6)"/>
                <text x="660" y="315" font-size="11" fill="#10b981" text-anchor="middle">Already filtered!</text>
                
                <!-- Ready to Process -->
                <rect x="550" y="320" width="220" height="60" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="660" y="345" font-size="13" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Ready to Process</text>
                <text x="660" y="365" font-size="11" fill="#cbd5e1" text-anchor="middle">500,000 Rows</text>
                
                <rect x="490" y="380" width="340" height="60" fill="rgba(16, 185, 129, 0.2)" stroke="#10b981" stroke-width="1" rx="6"/>
                <text x="660" y="405" font-size="12" fill="#10b981" text-anchor="middle" font-weight="bold"> Efficient</text>
                <text x="660" y="425" font-size="11" fill="#cbd5e1" text-anchor="middle">Only read 500K rows (50% less I/O)</text>
                
                <defs>
                    <marker id="arrowred3" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#ef4444"/>
                    </marker>
                    <marker id="arrowgreen6" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
                        <polygon points="0 0, 10 5, 0 10" fill="#10b981"/>
                    </marker>
                </defs>
            </svg>
            <p class="image-caption">Predicate pushdown: Filter at source vs filter after reading</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Predicate Pushdown Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-filter"></i> How It Works</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

# Read Parquet file
df = spark.read.parquet("users.parquet")

# Apply filter
filtered = df.filter(col("age") > 25)

# Catalyst automatically pushes predicate down!
filtered.explain()

# Output shows:
# == Physical Plan ==
# *(1) Project [...]
# +- *(1) Filter (age#1 > 25)
#    +- *(1) FileScan parquet [...]
#       PushedFilters: [IsNotNull(age), GreaterThan(age,25)]
#                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#                      Filter pushed to Parquet reader!

# Without pushdown: Read 1M rows  Filter  500K rows
# With pushdown: Read 500K rows directly (2x faster!)

# Example 2: Multiple filters
result = df.filter(col("age") > 25) \
           .filter(col("country") == "USA") \
           .filter(col("active") == True)

result.explain()
# PushedFilters: [GreaterThan(age,25), EqualTo(country,USA), EqualTo(active,true)]
# All filters pushed down!

# Example 3: Works with Parquet, ORC, JDBC
# Parquet: Uses row group statistics
parquet_df = spark.read.parquet("data.parquet")
parquet_df.filter(col("year") == 2024)  # Skips row groups

# JDBC: Pushes WHERE clause to database
jdbc_df = spark.read.jdbc(url, "table", properties)
jdbc_df.filter(col("age") > 25)  
# Executes: SELECT * FROM table WHERE age > 25 (on database!)

# ORC: Uses predicate pushdown with bloom filters
orc_df = spark.read.orc("data.orc")
orc_df.filter(col("id").isin([1, 2, 3]))  # Uses bloom filters

# Example 4: Check if pushdown happened
df = spark.read.parquet("large_file.parquet")
filtered = df.filter(col("year") == 2024)

# Look for "PushedFilters" in explain
filtered.explain()

# If you see:
# PushedFilters: [EqualTo(year,2024)]
#  Predicate pushed down! 

# If you see:
# PushedFilters: []
#  No pushdown (complex filter?) 

# Example 5: Predicates that CAN'T be pushed down
# Complex UDFs
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import udf

complex_udf = udf(lambda x: x * 2, IntegerType())
df.filter(complex_udf(col("age")) > 50)  # Can't push down UDF!

# Non-deterministic functions
from pyspark.sql.functions import rand
df.filter(rand() > 0.5)  # Can't push down random!

# Complex expressions (sometimes)
df.filter((col("a") + col("b")) > 100)  # May not push down</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-chart-line"></i> Performance Impact</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>import time

# Large Parquet file: 10 GB, 100M rows
df = spark.read.parquet("large_data.parquet")

# Filter: Only 1% of data matches
filtered = df.filter(col("year") == 2024)

# Performance test
start = time.time()
count = filtered.count()
elapsed = time.time() - start

print(f"Count: {count}, Time: {elapsed:.2f}s")

# Results:
# WITH predicate pushdown: 5 seconds
# - Reads: 100 MB (only year=2024 row groups)
# - I/O: Minimal

# WITHOUT predicate pushdown: 120 seconds
# - Reads: 10 GB (entire file)
# - I/O: Maximum
# - Then filters in memory

# Speedup: 24x faster!

# Why so much faster?
# 1. Less I/O: 100 MB vs 10 GB (100x less)
# 2. Less network: If data on HDFS
# 3. Less CPU: Filter at source (efficient)
# 4. Less memory: Only load needed data</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Best Practices
        </div>

        <div class="highlight-box">
            <strong>1. Use simple predicates:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Good: Simple comparison (will push down)
df.filter(col("age") > 25)

# Bad: Complex UDF (won't push down)
df.filter(complex_udf(col("age")) > 25)</code></pre>
        </div>

        <div class="highlight-box">
            <strong>2. Filter early:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Good: Filter first
df.filter(col("year") == 2024).select("name", "age")

# Less optimal: Select first (still works, but less clear)
df.select("name", "age", "year").filter(col("year") == 2024)</code></pre>
        </div>

        <div class="highlight-box">
            <strong>3. Use partitioned data:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Write with partitioning
df.write.partitionBy("year", "month").parquet("data")

# Read and filter (extremely fast - partition pruning)
spark.read.parquet("data").filter(col("year") == 2024)
# Skips all other year partitions!</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 98: Dynamic Partition Pruning -->
<div class="question-content" id="q98">
    <div class="question-header">
        <h1 class="question-title">How do you enable dynamic partition pruning?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            What is Dynamic Partition Pruning?
        </div>
        <div class="definition-box">
            <p><strong>Dynamic Partition Pruning (DPP)</strong> is an optimization where Spark determines which partitions to read based on filter results from another table in a join, skipping irrelevant partitions at runtime.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Optimization</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Partitions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Joins</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Spark 3.0+</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-cogs"></i>
            How to Enable
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-toggle-on"></i> Enable Dynamic Partition Pruning</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Dynamic Partition Pruning (DPP) is available in Spark 3.0+

# Enable DPP (enabled by default in Spark 3.0+)
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")

# Related configurations
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.useStats", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio", "0.5")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly", "true")

# Check if enabled
print(spark.conf.get("spark.sql.optimizer.dynamicPartitionPruning.enabled"))
# Output: true

# Example scenario where DPP helps:
# Large fact table (partitioned by date) joined with small dimension table

# Create partitioned fact table
sales = spark.read.parquet("sales_partitioned")  # Partitioned by date
# Partitions: date=2024-01-01, date=2024-01-02, ..., date=2024-12-31

# Small dimension table (filtered)
stores = spark.read.parquet("stores").filter(col("region") == "West")
# Only 10 stores in West region

# Join query
result = sales.join(stores, "store_id") \
              .filter(col("date").between("2024-01-01", "2024-12-31"))

# Without DPP:
# - Reads ALL date partitions from sales
# - Then joins with stores
# - Filters out non-matching store_ids
# - Very slow!

# With DPP:
# - First evaluates: which store_ids are in West region? (10 stores)
# - Then reads ONLY sales partitions for those 10 stores
# - Skips 90% of partitions!
# - Much faster!

result.explain()
# Look for: "dynamicpruning" or "DynamicPruningExpression" in plan</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> Complete Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Setup: Create partitioned data
from pyspark.sql.functions import col, lit
from datetime import datetime, timedelta

# Generate sales data (large table)
dates = [(datetime(2024, 1, 1) + timedelta(days=i)).strftime("%Y-%m-%d") 
         for i in range(365)]
stores = list(range(1, 101))  # 100 stores

sales_data = []
for date in dates:
    for store in stores:
        sales_data.append((store, date, 1000))

sales_df = spark.createDataFrame(sales_data, ["store_id", "date", "amount"])

# Write partitioned by date (365 partitions)
sales_df.write.mode("overwrite") \
        .partitionBy("date") \
        .parquet("/tmp/sales_partitioned")

# Read back
sales = spark.read.parquet("/tmp/sales_partitioned")

# Small dimension table (stores)
stores_data = [(i, f"Store{i}", "West" if i <= 10 else "East") 
               for i in range(1, 101)]
stores_df = spark.createDataFrame(stores_data, ["store_id", "name", "region"])

# Enable DPP
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")

# Query with DPP
west_stores = stores_df.filter(col("region") == "West")  # Only 10 stores
result = sales.join(west_stores, "store_id")

# Count results
print(f"Total rows: {result.count()}")  # 10 stores  365 days = 3,650

# Check execution plan
result.explain()

# You'll see:
# PartitionFilters: [dynamicpruning#123]
# This means DPP is working!

# Performance comparison
import time

# Without DPP
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "false")
start = time.time()
result.count()
time_without = time.time() - start

# With DPP
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
start = time.time()
result.count()
time_with = time.time() - start

print(f"Without DPP: {time_without:.2f}s")
print(f"With DPP: {time_with:.2f}s")
print(f"Speedup: {time_without/time_with:.2f}x")

# Typical results:
# Without DPP: 15.0s (reads all 365 partitions)
# With DPP: 2.5s (reads only needed partitions)
# Speedup: 6x</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            When DPP Helps
        </div>

        <div class="highlight-box">
            <strong>DPP is most effective when:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Large partitioned fact table joined with small dimension table</li>
                <li>Join key is partition column or related to partition column</li>
                <li>Filter on dimension table significantly reduces rows</li>
                <li>Many partitions can be skipped</li>
            </ul>
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-check-circle"></i> Good DPP Scenario</span>
            </div>
            <pre><code># sales table: 1 TB, partitioned by date (365 partitions)
# stores table: 1 MB, 100 stores, 10 in region "West"

# This query benefits hugely from DPP:
result = sales.join(
    stores.filter(col("region") == "West"),
    "store_id"
)

# DPP skips ~90% of date partitions that don't have West store sales</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 99: Joins in PySpark -->
<div class="question-content" id="q99">
    <div class="question-header">
        <h1 class="question-title">How do joins work in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            How Joins Work
        </div>
        <div class="definition-box">
            <p>Joins in PySpark combine rows from two DataFrames based on a common key. Spark uses different join strategies (broadcast, sort-merge, shuffle hash) depending on data size and available statistics.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Joins</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Inner Join</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Left Join</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Broadcast</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Join Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-link"></i> Basic Join Syntax</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, broadcast

# Create sample DataFrames
employees = spark.createDataFrame([
    (1, "Alice", 101),
    (2, "Bob", 102),
    (3, "Charlie", 101),
    (4, "David", 103)
], ["emp_id", "name", "dept_id"])

departments = spark.createDataFrame([
    (101, "Engineering"),
    (102, "Sales"),
    (103, "Marketing")
], ["dept_id", "dept_name"])

# Basic join syntax
result = employees.join(departments, "dept_id")
result.show()

# Output:
# +-------+------+-------+------------+
# |dept_id|emp_id|   name|   dept_name|
# +-------+------+-------+------------+
# |    101|     1|  Alice| Engineering|
# |    101|     3|Charlie| Engineering|
# |    102|     2|    Bob|       Sales|
# |    103|     4|  David|   Marketing|
# +-------+------+-------+------------+

# Different join conditions
# 1. Single column (column name as string)
employees.join(departments, "dept_id")

# 2. Multiple columns
employees.join(departments, ["dept_id", "other_col"])

# 3. Different column names
employees.join(departments, employees.dept_id == departments.dept_id)

# 4. Complex condition
employees.join(departments, 
               (employees.dept_id == departments.dept_id) & 
               (employees.emp_id > 1))</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>List different join types</strong></span>
                </div>
                <div class="cross-answer">
                    PySpark supports these join types:<br><br>
                    <strong>1. Inner Join (default):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.join(df2, "key", "inner")
# Returns: Only matching rows from both tables</code></pre>
                    <strong>2. Left (Left Outer) Join:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.join(df2, "key", "left")
df1.join(df2, "key", "left_outer")  # Same
# Returns: All from left + matching from right (nulls if no match)</code></pre>
                    <strong>3. Right (Right Outer) Join:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.join(df2, "key", "right")
df1.join(df2, "key", "right_outer")  # Same
# Returns: All from right + matching from left (nulls if no match)</code></pre>
                    <strong>4. Full (Full Outer) Join:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.join(df2, "key", "full")
df1.join(df2, "key", "full_outer")  # Same
# Returns: All rows from both (nulls where no match)</code></pre>
                    <strong>5. Left Semi Join:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.join(df2, "key", "left_semi")
# Returns: Rows from left that have match in right (no columns from right)</code></pre>
                    <strong>6. Left Anti Join:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.join(df2, "key", "left_anti")
# Returns: Rows from left that DON'T have match in right</code></pre>
                    <strong>7. Cross Join (Cartesian Product):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1.crossJoin(df2)
# Returns: Every row from left with every row from right
# Warning: Very expensive! n  m rows</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>How to perform broadcast joins?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Broadcast joins</strong> avoid shuffle by broadcasting small table to all executors:<br><br>
                    <strong>Method 1: Explicit broadcast hint:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import broadcast

# Explicitly broadcast small table
large_df.join(broadcast(small_df), "key")

# Spark broadcasts small_df to all executors
# No shuffle needed for small_df!</code></pre>
                    <strong>Method 2: Automatic broadcast (if small enough):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Configure auto-broadcast threshold (default 10MB)
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760")  # 10MB

# If small_df < 10MB, Spark auto-broadcasts
large_df.join(small_df, "key")</code></pre>
                    <strong>When to use broadcast joins:</strong><br>
                     Small table < 10 MB (can fit in executor memory)<br>
                     Large table (GBs to TBs)<br>
                     Want to avoid shuffle of large table<br><br>
                    <strong>Performance impact:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Without broadcast: 45 seconds (both tables shuffled)
large_df.join(small_df, "key")

# With broadcast: 8 seconds (only large shuffled if needed)
large_df.join(broadcast(small_df), "key")
# 5-6x faster!</code></pre>
                    <strong>Check if broadcast is used:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>result = large_df.join(broadcast(small_df), "key")
result.explain()

# Look for: "BroadcastHashJoin" in plan
# == Physical Plan ==
# *(2) BroadcastHashJoin [key#1], [key#2], Inner, BuildRight
# ...</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 100: Join Two DataFrames -->
<div class="question-content" id="q100">
    <div class="question-header">
        <h1 class="question-title">How do you join two DataFrames in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Join Syntax
        </div>
        <div class="definition-box">
            <p>To join two DataFrames in PySpark, use the <code>.join()</code> method with the join condition and join type. The most common syntax is: <code>df1.join(df2, join_condition, join_type)</code></p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Join Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> Complete Join Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

# Sample data
employees = spark.createDataFrame([
    (1, "Alice", 101),
    (2, "Bob", 102),
    (3, "Charlie", 101),
    (4, "David", None)
], ["emp_id", "name", "dept_id"])

departments = spark.createDataFrame([
    (101, "Engineering"),
    (102, "Sales"),
    (103, "Marketing")
], ["dept_id", "dept_name"])

# 1. Inner Join (default) - Only matching rows
inner_join = employees.join(departments, "dept_id", "inner")
inner_join.show()
# +-------+------+-------+------------+
# |dept_id|emp_id|   name|   dept_name|
# +-------+------+-------+------------+
# |    101|     1|  Alice| Engineering|
# |    101|     3|Charlie| Engineering|
# |    102|     2|    Bob|       Sales|
# +-------+------+-------+------------+
# David (dept_id=None) and Marketing (dept_id=103) not included

# 2. Left Join - All from left, matching from right
left_join = employees.join(departments, "dept_id", "left")
left_join.show()
# +-------+------+-------+------------+
# |dept_id|emp_id|   name|   dept_name|
# +-------+------+-------+------------+
# |   null|     4|  David|        null|   David included with null dept
# |    101|     1|  Alice| Engineering|
# |    101|     3|Charlie| Engineering|
# |    102|     2|    Bob|       Sales|
# +-------+------+-------+------------+

# 3. Right Join - All from right, matching from left
right_join = employees.join(departments, "dept_id", "right")
right_join.show()
# +-------+------+-------+------------+
# |dept_id|emp_id|   name|   dept_name|
# +-------+------+-------+------------+
# |    101|     1|  Alice| Engineering|
# |    101|     3|Charlie| Engineering|
# |    102|     2|    Bob|       Sales|
# |    103|  null|   null|   Marketing|   Marketing included with null emp
# +-------+------+-------+------------+

# 4. Full Outer Join - All from both
full_join = employees.join(departments, "dept_id", "full")
full_join.show()
# +-------+------+-------+------------+
# |dept_id|emp_id|   name|   dept_name|
# +-------+------+-------+------------+
# |   null|     4|  David|        null|   David
# |    101|     1|  Alice| Engineering|
# |    101|     3|Charlie| Engineering|
# |    102|     2|    Bob|       Sales|
# |    103|  null|   null|   Marketing|   Marketing
# +-------+------+-------+------------+

# 5. Different column names
employees2 = employees.withColumnRenamed("dept_id", "department_id")
result = employees2.join(
    departments,
    employees2.department_id == departments.dept_id,
    "inner"
)

# 6. Multiple conditions
result = employees.join(
    departments,
    (employees.dept_id == departments.dept_id) & 
    (employees.emp_id > 1),
    "inner"
)

# 7. Select specific columns after join
result = employees.join(departments, "dept_id") \
                  .select("emp_id", "name", "dept_name")
result.show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Different types of joins available?</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 99 for complete list. Summary:<br>
                     inner, left (left_outer), right (right_outer)<br>
                     full (full_outer), left_semi, left_anti<br>
                     cross (crossJoin)
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>When would you use a broadcast join?</strong></span>
                </div>
                <div class="cross-answer">
                    Use broadcast join when:<br>
                     One table is small (< 10 MB)<br>
                     Other table is large (GBs+)<br>
                     Want to avoid shuffle of large table<br>
                     Need maximum performance<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import broadcast

# Broadcast small table
large_df.join(broadcast(small_df), "key")</code></pre>
                    See Questions 85 and 99 for more details.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 101: union() vs join() -->
<div class="question-content" id="q101">
    <div class="question-header">
        <h1 class="question-title">What is the difference between union() and join()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Key Differences
        </div>
        <div class="definition-box">
            <p><strong>union()</strong> appends rows from one DataFrame to another (vertical concatenation). <strong>join()</strong> combines columns from two DataFrames based on a key (horizontal combination).</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> union</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> join</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Vertical</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Horizontal</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Visual Comparison
        </div>
        <div class="image-container">
            <svg width="900" height="600" viewBox="0 0 900 600">
                <rect width="900" height="600" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">union() vs join()</text>
                
                <!-- UNION -->
                <rect x="50" y="60" width="380" height="250" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="240" y="90" font-size="18" fill="#10b981" text-anchor="middle" font-weight="bold">union() - Vertical Stack</text>
                
                <!-- DataFrame 1 -->
                <rect x="140" y="110" width="200" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="240" y="135" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">DataFrame 1</text>
                <text x="240" y="155" font-size="10" fill="#cbd5e1" text-anchor="middle">id | name | age</text>
                
                <text x="240" y="185" font-size="14" fill="#10b981" text-anchor="middle" font-weight="bold"> union()</text>
                
                <!-- DataFrame 2 -->
                <rect x="140" y="195" width="200" height="60" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="240" y="220" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">DataFrame 2</text>
                <text x="240" y="240" font-size="10" fill="#cbd5e1" text-anchor="middle">id | name | age</text>
                
                <text x="240" y="270" font-size="14" fill="#10b981" text-anchor="middle" font-weight="bold"> Result</text>
                
                <!-- Result -->
                <rect x="140" y="280" width="200" height="20" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="4"/>
                <text x="240" y="295" font-size="10" fill="#f1f5f9" text-anchor="middle">id | name | age</text>
                
                <!-- JOIN -->
                <rect x="470" y="60" width="380" height="250" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="2" rx="12"/>
                <text x="660" y="90" font-size="18" fill="#3b82f6" text-anchor="middle" font-weight="bold">join() - Horizontal Merge</text>
                
                <!-- DataFrame A -->
                <rect x="490" y="130" width="120" height="60" fill="#334155" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="550" y="155" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">DataFrame A</text>
                <text x="550" y="175" font-size="10" fill="#cbd5e1" text-anchor="middle">id | name</text>
                
                <text x="625" y="165" font-size="16" fill="#3b82f6" text-anchor="middle" font-weight="bold">+</text>
                
                <!-- DataFrame B -->
                <rect x="640" y="130" width="120" height="60" fill="#334155" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="700" y="155" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">DataFrame B</text>
                <text x="700" y="175" font-size="10" fill="#cbd5e1" text-anchor="middle">id | age</text>
                
                <text x="660" y="215" font-size="14" fill="#3b82f6" text-anchor="middle" font-weight="bold"> join(on="id")</text>
                
                <!-- Result -->
                <rect x="560" y="230" width="200" height="60" fill="rgba(59, 130, 246, 0.3)" stroke="#3b82f6" stroke-width="2" rx="8"/>
                <text x="660" y="255" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Result</text>
                <text x="660" y="275" font-size="10" fill="#cbd5e1" text-anchor="middle">id | name | age</text>
                
                <!-- Comparison Table -->
                <rect x="50" y="340" width="800" height="230" fill="rgba(245, 158, 11, 0.1)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="450" y="370" font-size="16" fill="#f59e0b" text-anchor="middle" font-weight="bold">Key Differences</text>
                
                <text x="100" y="400" font-size="13" fill="#10b981" text-anchor="start" font-weight="bold">union():</text>
                <text x="100" y="420" font-size="11" fill="#cbd5e1" text-anchor="start"> Stacks rows vertically</text>
                <text x="100" y="438" font-size="11" fill="#cbd5e1" text-anchor="start"> Requires same columns</text>
                <text x="100" y="456" font-size="11" fill="#cbd5e1" text-anchor="start"> No key needed</text>
                <text x="100" y="474" font-size="11" fill="#cbd5e1" text-anchor="start"> Adds rows (n + m rows)</text>
                <text x="100" y="492" font-size="11" fill="#cbd5e1" text-anchor="start"> Fast (no shuffle needed)</text>
                <text x="100" y="510" font-size="11" fill="#cbd5e1" text-anchor="start"> Example: Combine daily logs</text>
                <text x="100" y="528" font-size="11" fill="#cbd5e1" text-anchor="start"> SQL: UNION</text>
                
                <text x="500" y="400" font-size="13" fill="#3b82f6" text-anchor="start" font-weight="bold">join():</text>
                <text x="500" y="420" font-size="11" fill="#cbd5e1" text-anchor="start"> Merges columns horizontally</text>
                <text x="500" y="438" font-size="11" fill="#cbd5e1" text-anchor="start"> Can have different columns</text>
                <text x="500" y="456" font-size="11" fill="#cbd5e1" text-anchor="start"> Requires join key</text>
                <text x="500" y="474" font-size="11" fill="#cbd5e1" text-anchor="start"> Matches based on key</text>
                <text x="500" y="492" font-size="11" fill="#cbd5e1" text-anchor="start"> Slower (may shuffle)</text>
                <text x="500" y="510" font-size="11" fill="#cbd5e1" text-anchor="start"> Example: Users + Orders</text>
                <text x="500" y="528" font-size="11" fill="#cbd5e1" text-anchor="start"> SQL: JOIN</text>
            </svg>
            <p class="image-caption">union: Vertical stacking | join: Horizontal merging</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> union() Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># union() - Stacks DataFrames vertically
df1 = spark.createDataFrame([
    (1, "Alice", 25),
    (2, "Bob", 30)
], ["id", "name", "age"])

df2 = spark.createDataFrame([
    (3, "Charlie", 35),
    (4, "David", 40)
], ["id", "name", "age"])

# Union
result = df1.union(df2)
result.show()

# Output:
# +---+-------+---+
# | id|   name|age|
# +---+-------+---+
# |  1|  Alice| 25|   From df1
# |  2|    Bob| 30|   From df1
# |  3|Charlie| 35|   From df2
# |  4|  David| 40|   From df2
# +---+-------+---+
# Total: 4 rows (2 + 2)

# Note: union() doesn't remove duplicates
df3 = spark.createDataFrame([
    (1, "Alice", 25)  # Duplicate
], ["id", "name", "age"])

result_dup = df1.union(df3)
result_dup.show()
# Output shows duplicate!

# Use distinct() to remove duplicates
result_dup.distinct().show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-link"></i> join() Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># join() - Merges DataFrames horizontally
employees = spark.createDataFrame([
    (1, "Alice"),
    (2, "Bob"),
    (3, "Charlie")
], ["id", "name"])

salaries = spark.createDataFrame([
    (1, 50000),
    (2, 60000),
    (3, 55000)
], ["id", "salary"])

# Join
result = employees.join(salaries, "id")
result.show()

# Output:
# +---+-------+------+
# | id|   name|salary|
# +---+-------+------+
# |  1|  Alice| 50000|
# |  2|    Bob| 60000|
# |  3|Charlie| 55000|
# +---+-------+------+
# Total: 3 rows (same as input)
# Columns: 3 (1 + 2, with id deduplicated)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you perform union() on DataFrames with different schemas?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>It depends on the union method used:</strong><br><br>
                    <strong>1. union() (position-based):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1 = spark.createDataFrame([(1, "Alice")], ["id", "name"])
df2 = spark.createDataFrame([(2, 30)], ["id", "age"])

# union() matches by position, not column name
result = df1.union(df2)
result.show()
# +---+-----+
# | id| name|
# +---+-----+
# |  1|Alice|
# |  2|   30|   age value in name column!
# +---+-----+
# Works but likely wrong!</code></pre>
                    <strong>2. unionByName() (name-based, Spark 2.3+):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1 = spark.createDataFrame([(1, "Alice", 25)], ["id", "name", "age"])
df2 = spark.createDataFrame([(2, 30, "Bob")], ["id", "age", "name"])

# unionByName() matches by column name
result = df1.unionByName(df2)
result.show()
# +---+-----+---+
# | id| name|age|
# +---+-----+---+
# |  1|Alice| 25|
# |  2|  Bob| 30|   Correct! Matched by name
# +---+-----+---+</code></pre>
                    <strong>3. Different columns with allowMissingColumns (Spark 3.1+):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df1 = spark.createDataFrame([(1, "Alice")], ["id", "name"])
df2 = spark.createDataFrame([(2, 30)], ["id", "age"])

# Allow missing columns (fills with null)
result = df1.unionByName(df2, allowMissingColumns=True)
result.show()
# +---+-----+----+
# | id| name| age|
# +---+-----+----+
# |  1|Alice|null|
# |  2| null|  30|
# +---+-----+----+</code></pre>
                    <strong>Best Practice:</strong><br>
                     Use <code>unionByName()</code> for clarity<br>
                     Use <code>allowMissingColumns=True</code> if schemas differ<br>
                     Avoid <code>union()</code> with different schemas
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 102: RDD Joins -->
<div class="question-content" id="q102">
    <div class="question-header">
        <h1 class="question-title">How do you perform joins using RDD?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            RDD Join Operations
        </div>
        <div class="definition-box">
            <p>RDD joins work on <strong>pair RDDs</strong> (key-value pairs). Use operations like <code>join()</code>, <code>leftOuterJoin()</code>, <code>rightOuterJoin()</code>, and <code>fullOuterJoin()</code>.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            RDD Join Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-link"></i> Basic RDD Joins</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Create pair RDDs (key-value pairs)
rdd1 = spark.sparkContext.parallelize([
    (1, "Alice"),
    (2, "Bob"),
    (3, "Charlie")
])

rdd2 = spark.sparkContext.parallelize([
    (1, 50000),
    (2, 60000),
    (4, 55000)  # No match for key 4 in rdd1
])

# 1. Inner Join (join)
inner = rdd1.join(rdd2)
print(inner.collect())
# Output: [(1, ('Alice', 50000)), (2, ('Bob', 60000))]
# Only keys present in both RDDs

# 2. Left Outer Join
left = rdd1.leftOuterJoin(rdd2)
print(left.collect())
# Output: 
# [(1, ('Alice', 50000)), 
#  (2, ('Bob', 60000)), 
#  (3, ('Charlie', None))]   Charlie has no match
# All keys from left RDD

# 3. Right Outer Join
right = rdd1.rightOuterJoin(rdd2)
print(right.collect())
# Output:
# [(1, ('Alice', 50000)),
#  (2, ('Bob', 60000)),
#  (4, (None, 55000))]   Key 4 has no match in rdd1

# 4. Full Outer Join
full = rdd1.fullOuterJoin(rdd2)
print(full.collect())
# Output:
# [(1, ('Alice', 50000)),
#  (2, ('Bob', 60000)),
#  (3, ('Charlie', None)),   From rdd1
#  (4, (None, 55000))]       From rdd2

# 5. Cartesian Product (like cross join)
cartesian = rdd1.cartesian(rdd2)
print(cartesian.collect())
# Output: Every combination
# [((1, 'Alice'), (1, 50000)),
#  ((1, 'Alice'), (2, 60000)),
#  ((1, 'Alice'), (4, 55000)),
#  ((2, 'Bob'), (1, 50000)),
#  ... total 9 combinations (3  3)]

# Complex example with processing
employees = spark.sparkContext.parallelize([
    ("emp1", ("Alice", "Engineering")),
    ("emp2", ("Bob", "Sales")),
    ("emp3", ("Charlie", "Engineering"))
])

salaries = spark.sparkContext.parallelize([
    ("emp1", 50000),
    ("emp2", 60000),
    ("emp3", 55000)
])

# Join and transform
result = employees.join(salaries) \
    .map(lambda x: (x[0], x[1][0][0], x[1][0][1], x[1][1])) \
    .collect()

print(result)
# [('emp1', 'Alice', 'Engineering', 50000),
#  ('emp2', 'Bob', 'Sales', 60000),
#  ('emp3', 'Charlie', 'Engineering', 55000)]

# Performance tip: Use cogroup() for multiple joins
rdd3 = spark.sparkContext.parallelize([
    (1, "Manager"),
    (2, "Developer")
])

# Instead of: rdd1.join(rdd2).join(rdd3)
# Better: Use cogroup()
cogrouped = rdd1.cogroup(rdd2, rdd3)
print(cogrouped.collect())
# [(1, (ResultIterable(['Alice']), ResultIterable([50000]), ResultIterable(['Manager']))),
#  ...]</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 103: Join Internals -->
<div class="question-content" id="q103">
    <div class="question-header">
        <h1 class="question-title">How does Spark handle joins internally (broadcast, sort-merge, shuffle hash)?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Join Strategies
        </div>
        <div class="definition-box">
            <p>Spark uses three main join strategies: <strong>Broadcast Hash Join</strong> (for small tables), <strong>Sort-Merge Join</strong> (for large sorted tables), and <strong>Shuffle Hash Join</strong> (for medium-sized tables). Catalyst optimizer chooses the best strategy based on table sizes and statistics.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-sitemap"></i>
            Join Strategy Details
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-broadcast-tower"></i> 1. Broadcast Hash Join
            </div>
            <div style="margin-top: 1rem;">
                <strong>When used:</strong> One table is small (< 10 MB by default)<br>
                <strong>How it works:</strong><br>
                1. Small table broadcast to all executors<br>
                2. Large table stays in place<br>
                3. Hash join performed locally on each executor<br><br>
                <strong>Pros:</strong> No shuffle, fastest join<br>
                <strong>Cons:</strong> Small table must fit in memory<br>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Automatic if small table < 10MB
large_df.join(small_df, "key")

# Explicit broadcast
from pyspark.sql.functions import broadcast
large_df.join(broadcast(small_df), "key")</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-sort"></i> 2. Sort-Merge Join
            </div>
            <div style="margin-top: 1rem;">
                <strong>When used:</strong> Both tables are large and sortable<br>
                <strong>How it works:</strong><br>
                1. Shuffle both tables by join key<br>
                2. Sort each partition by key<br>
                3. Merge sorted partitions<br><br>
                <strong>Pros:</strong> Efficient for large sorted data, scalable<br>
                <strong>Cons:</strong> Requires shuffle + sort<br>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Spark chooses this for large tables
large_df1.join(large_df2, "key")

# Check execution plan
result.explain()
# Look for: "SortMergeJoin"</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-hashtag"></i> 3. Shuffle Hash Join
            </div>
            <div style="margin-top: 1rem;">
                <strong>When used:</strong> One table is medium-sized, unsortable<br>
                <strong>How it works:</strong><br>
                1. Shuffle both tables by join key<br>
                2. Build hash table for smaller side<br>
                3. Probe with larger side<br><br>
                <strong>Pros:</strong> No sorting needed<br>
                <strong>Cons:</strong> Requires shuffle, memory for hash table<br>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Rarely chosen by default
# Enable explicitly:
spark.conf.set("spark.sql.join.preferSortMergeJoin", "false")

medium_df1.join(medium_df2, "key")</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Strategy Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Strategy</th>
                        <th>Small Table</th>
                        <th>Large Table</th>
                        <th>Shuffle?</th>
                        <th>Sort?</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Broadcast Hash</strong></td>
                        <td>&lt; 10 MB</td>
                        <td>Any size</td>
                        <td>No</td>
                        <td>No</td>
                        <td>Small dimension table</td>
                    </tr>
                    <tr>
                        <td><strong>Sort-Merge</strong></td>
                        <td>Large</td>
                        <td>Large</td>
                        <td>Yes</td>
                        <td>Yes</td>
                        <td>Large tables</td>
                    </tr>
                    <tr>
                        <td><strong>Shuffle Hash</strong></td>
                        <td>Medium</td>
                        <td>Large</td>
                        <td>Yes</td>
                        <td>No</td>
                        <td>Medium unsortable data</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            How Spark Chooses Strategy
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-decision"></i> Decision Process</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Spark's join strategy selection process:

# 1. Check if either table is small enough to broadcast
if small_table_size < spark.sql.autoBroadcastJoinThreshold:  # 10MB
    use BroadcastHashJoin
    
# 2. Check if both tables are sortable and large
elif both_tables_large and sortable:
    use SortMergeJoin (default for large tables)
    
# 3. Otherwise, use Shuffle Hash Join (rare)
else:
    use ShuffleHashJoin

# View which strategy was chosen
df1.join(df2, "key").explain()

# Output examples:
# "BroadcastHashJoin [key#1], [key#2]"   Broadcast
# "SortMergeJoin [key#1], [key#2]"       Sort-Merge  
# "ShuffleHashJoin [key#1], [key#2]"     Shuffle Hash

# Influence strategy selection:
# 1. Broadcast threshold
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "20971520")  # 20MB

# 2. Prefer sort-merge join
spark.conf.set("spark.sql.join.preferSortMergeJoin", "true")

# 3. Adaptive Query Execution (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
# Spark can change strategy at runtime based on actual data size

# Performance example:
import time

# Large tables (no broadcast)
large1 = spark.range(10000000)
large2 = spark.range(10000000)

# Test Sort-Merge Join
start = time.time()
large1.join(large2, "id").count()
sortmerge_time = time.time() - start
print(f"Sort-Merge: {sortmerge_time:.2f}s")

# Small + Large (broadcast)
small = spark.range(100)
large = spark.range(10000000)

start = time.time()
large.join(broadcast(small), "id").count()
broadcast_time = time.time() - start
print(f"Broadcast: {broadcast_time:.2f}s")

# Typical results:
# Sort-Merge: 25s (shuffle both tables)
# Broadcast: 5s (5x faster, no shuffle for small table)</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 104: Skew Joins -->
<div class="question-content" id="q104">
    <div class="question-header">
        <h1 class="question-title">What are skew joins? How to handle them?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            What is Skew Join?
        </div>
        <div class="definition-box">
            <p><strong>Skew join</strong> occurs when join keys are unevenly distributed, causing some partitions to have significantly more data than others. This leads to straggler tasks and poor performance.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Skew</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Optimization</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-exclamation-triangle"></i>
            Problem: Skew Join
        </div>

        <div class="warning-box">
            <strong>Example scenario:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Customer orders table
# customer_id | order_amount
# 1          | 100
# 1          | 200
# 1          | 150
# ... (1 million orders for customer 1)
# 2          | 50
# 3          | 75
# ... (100 orders each for customers 2-1000)

# When joining with customers table:
# - Partition with customer_id=1: 1 million rows (takes 60 minutes)
# - Other partitions: 100 rows each (takes 1 second)
# 
# Result: Entire job takes 60 minutes waiting for one task!</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-tools"></i>
            Solutions for Skew Joins
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 1. Salting (Most Common Solution)
            </div>
            <div style="margin-top: 1rem;">
                <strong>Add random "salt" to split skewed keys across partitions:</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import col, concat, lit, rand, floor

# Original skewed join
# orders has many rows for customer_id = 1
orders = spark.read.parquet("orders")
customers = spark.read.parquet("customers")

# Solution: Add salt to split skewed key
salt_factor = 10  # Split into 10 partitions

# 1. Add salt to orders (large table)
orders_salted = orders.withColumn(
    "salt", floor(rand() * salt_factor)
).withColumn(
    "salted_key", concat(col("customer_id"), lit("_"), col("salt"))
)

# 2. Replicate customers (small table) for each salt value
from pyspark.sql.functions import explode, array

customers_exploded = customers.crossJoin(
    spark.range(salt_factor).toDF("salt")
).withColumn(
    "salted_key", concat(col("customer_id"), lit("_"), col("salt"))
)

# 3. Join on salted key
result = orders_salted.join(customers_exploded, "salted_key")

# 4. Remove salt column
final = result.drop("salt", "salted_key")

# Now customer_id=1 split across 10 partitions!
# Each partition processes 100K rows instead of 1M</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 2. Adaptive Query Execution (AQE) - Spark 3.0+
            </div>
            <div style="margin-top: 1rem;">
                <strong>Enable automatic skew join optimization:</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Enable AQE skew join optimization
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# Configure skew detection
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionFactor", "5")
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "256MB")

# Spark automatically detects and handles skew
result = orders.join(customers, "customer_id")

# AQE will:
# 1. Detect skewed partitions during execution
# 2. Split skewed partitions into smaller pieces
# 3. Process them separately</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 3. Broadcast Join (if possible)
            </div>
            <div style="margin-top: 1rem;">
                <strong>If one table is small, broadcast it:</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import broadcast

# If customers table is small
result = orders.join(broadcast(customers), "customer_id")

# No shuffle needed, no skew problem!</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 4. Isolated Join (Split Skewed Keys)
            </div>
            <div style="margin-top: 1rem;">
                <strong>Handle skewed keys separately:</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Identify skewed keys
skewed_keys = [1, 2]  # customer_ids with many orders

# Split data
orders_skewed = orders.filter(col("customer_id").isin(skewed_keys))
orders_normal = orders.filter(~col("customer_id").isin(skewed_keys))

customers_skewed = customers.filter(col("customer_id").isin(skewed_keys))
customers_normal = customers.filter(~col("customer_id").isin(skewed_keys))

# Join skewed keys with broadcast (small customer data)
result_skewed = orders_skewed.join(
    broadcast(customers_skewed), "customer_id"
)

# Join normal keys normally
result_normal = orders_normal.join(customers_normal, "customer_id")

# Union results
final_result = result_skewed.union(result_normal)</code></pre>
            </div>
        </div>

        <div class="scenario-box">
            <div class="scenario-title">
                <i class="fas fa-check-circle"></i> 5. Increase Parallelism
            </div>
            <div style="margin-top: 1rem;">
                <strong>More partitions = smaller skew impact:</strong>
                <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Increase shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", "1000")  # Default: 200

# More partitions reduces skew per partition
result = orders.join(customers, "customer_id")</code></pre>
            </div>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-search"></i>
            Detecting Skew
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-chart-bar"></i> How to Detect Skew</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Method 1: Check key distribution
orders.groupBy("customer_id").count() \
      .orderBy(col("count").desc()) \
      .show(10)

# If top keys have 10-100x more rows than average  SKEW!

# Method 2: Check Spark UI
# Look at: Stages  Task Duration
# If one task takes 10x longer  SKEW!

# Method 3: Programmatic detection
from pyspark.sql.functions import count

key_counts = orders.groupBy("customer_id").agg(count("*").alias("cnt"))
stats = key_counts.selectExpr(
    "avg(cnt) as avg_count",
    "max(cnt) as max_count",
    "min(cnt) as min_count"
).collect()[0]

skew_ratio = stats.max_count / stats.avg_count
print(f"Skew ratio: {skew_ratio:.2f}")

if skew_ratio > 10:
    print(" HIGH SKEW DETECTED!")
    # Apply skew handling strategies</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 105: explain() Method -->
<div class="question-content" id="q105">
    <div class="question-header">
        <h1 class="question-title">What is the use of explain() method?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Purpose of explain()
        </div>
        <div class="definition-box">
            <p>The <strong>explain()</strong> method displays the execution plan of a DataFrame query, showing how Spark will execute the transformation. It helps understand query optimization, identify performance bottlenecks, and verify optimization strategies.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Execution Plan</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Query Optimization</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Debugging</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Using explain()
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-eye"></i> explain() Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

df = spark.read.parquet("data.parquet")
result = df.filter(col("age") > 25).select("name", "age")

# 1. Simple explain (shows physical plan)
result.explain()

# Output:
# == Physical Plan ==
# *(1) Project [name#0, age#1]
# +- *(1) Filter (age#1 > 25)
#    +- *(1) FileScan parquet [name,age]

# 2. Extended explain (shows all plans)
result.explain(extended=True)

# Output shows:
# == Parsed Logical Plan ==
# == Analyzed Logical Plan ==
# == Optimized Logical Plan ==
# == Physical Plan ==

# 3. Different explain modes (Spark 3.0+)
result.explain(mode="simple")     # Physical plan only (default)
result.explain(mode="extended")   # All plans
result.explain(mode="codegen")    # Generated code
result.explain(mode="cost")       # With cost estimates
result.explain(mode="formatted")  # Pretty printed

# Example with complex query
complex = df.filter(col("age") > 25) \
            .groupBy("department") \
            .count() \
            .orderBy("count")

complex.explain(extended=True)

# Output shows optimizations:
# == Optimized Logical Plan ==
# Sort [count#10 ASC NULLS FIRST]
# +- Aggregate [department#2], [department#2, count(1) AS count#10]
#    +- Project [department#2]
#       +- Filter (age#1 > 25)
#          +- FileScan parquet [age,department]
#
# Notice: Column pruning (only age, department read)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What are physical and logical plans?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Logical Plan:</strong> WHAT to compute (abstract operations)<br>
                     Describes transformations conceptually<br>
                     Analyzed and optimized by Catalyst<br>
                     Platform-independent<br><br>
                    <strong>Physical Plan:</strong> HOW to compute (concrete operations)<br>
                     Describes actual execution steps<br>
                     Includes algorithms, data structures<br>
                     Platform-specific<br><br>
                    <strong>Example:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.filter(col("age") > 25).select("name")

# Logical Plan (what):
# Project [name]
#   Filter (age > 25)
#     Scan [table]

# Physical Plan (how):
# *(1) Project [name#0]
# +- *(1) Filter (age#1 > 25)
#    +- *(1) FileScan parquet [name,age]
#       PushedFilters: [IsNotNull(age), GreaterThan(age,25)]
#
# Physical adds:
# - *(1) = Whole-stage code generation
# - PushedFilters = Predicate pushdown
# - FileScan = Specific file format reader</code></pre>
                    <strong>Transformation Process:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>User Query
    
Parsed Logical Plan (raw parse tree)
    
Analyzed Logical Plan (resolved columns/tables)
    
Optimized Logical Plan (Catalyst optimizations)
    
Physical Plan (execution strategy)
    
Executable Code (generated by Tungsten)</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 106: View Execution Plan -->
<div class="question-content" id="q106">
    <div class="question-header">
        <h1 class="question-title">How to view the execution plan of a DataFrame?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Methods to View Execution Plan
        </div>
        <div class="definition-box">
            <p>View execution plans using <code>explain()</code> method or <code>queryExecution</code> attribute. These show how Spark will execute the DataFrame operations.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Viewing Execution Plans
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-search"></i> Methods to View Plans</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col

df = spark.read.parquet("data.parquet")
filtered = df.filter(col("age") > 25)

# Method 1: explain() - Most common
filtered.explain()
# Shows physical plan

# Method 2: explain(extended=True) - All plans
filtered.explain(extended=True)
# Shows: Parsed  Analyzed  Optimized  Physical

# Method 3: explain(mode="...") - Specific format
filtered.explain(mode="simple")      # Default physical plan
filtered.explain(mode="extended")    # All plans
filtered.explain(mode="formatted")   # Pretty printed
filtered.explain(mode="cost")        # With statistics
filtered.explain(mode="codegen")     # Generated code

# Method 4: queryExecution attribute
print(filtered.queryExecution.logical)      # Logical plan
print(filtered.queryExecution.optimizedPlan) # Optimized plan
print(filtered.queryExecution.sparkPlan)    # Physical plan
print(filtered.queryExecution.executedPlan) # Final executed plan

# Method 5: Spark UI (visual)
# Run query and go to: http://localhost:4040
# Click on SQL tab  Click on query  View DAG visualization

# Practical example: Verify optimizations
df = spark.read.parquet("large_table.parquet")
result = df.select("id", "name", "age", "salary", "dept") \
           .filter(col("age") > 25) \
           .filter(col("dept") == "Engineering") \
           .select("name", "salary")

result.explain(extended=True)

# Check for optimizations in output:
# 1. Column pruning: Only reads [name, age, salary, dept]
# 2. Predicate pushdown: PushedFilters in FileScan
# 3. Filter combination: (age > 25 AND dept = Engineering)
# 4. Early projection: Selects needed columns early</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What are logical and physical plans?</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 105 for detailed explanation.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What is Catalyst Optimizer?</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 93 for detailed explanation of Catalyst Optimizer.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 107: explain() vs queryExecution() -->
<div class="question-content" id="q107">
    <div class="question-header">
        <h1 class="question-title">What's the difference between explain() and queryExecution()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Key Differences
        </div>
        <div class="definition-box">
            <p><strong>explain()</strong> is a method that prints formatted execution plans to console. <strong>queryExecution</strong> is an attribute that provides programmatic access to all query execution details as objects.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-table"></i>
            Comparison
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>explain()</th>
                        <th>queryExecution</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Type</strong></td>
                        <td>Method</td>
                        <td>Attribute (object)</td>
                    </tr>
                    <tr>
                        <td><strong>Output</strong></td>
                        <td>Prints to console</td>
                        <td>Returns objects</td>
                    </tr>
                    <tr>
                        <td><strong>Format</strong></td>
                        <td>Human-readable string</td>
                        <td>Programmatic access</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Quick inspection, debugging</td>
                        <td>Programmatic analysis</td>
                    </tr>
                    <tr>
                        <td><strong>Returns</strong></td>
                        <td>None (prints)</td>
                        <td>QueryExecution object</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What information does each provide?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>explain() provides:</strong><br>
                     Formatted text output<br>
                     Physical plan (default)<br>
                     All plans (with extended=True)<br>
                     Easy to read for humans<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.explain()
# == Physical Plan ==
# *(1) Project [name#0]
# +- *(1) Filter (age#1 > 25)
#    +- *(1) FileScan parquet [...]</code></pre>
                    <strong>queryExecution provides:</strong><br>
                     Programmatic access to objects<br>
                     All execution phases<br>
                     Can be processed programmatically<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Access individual components
qe = df.queryExecution

# Logical plan (unoptimized)
print(qe.logical)

# Analyzed logical plan (with types)
print(qe.analyzed)

# Optimized logical plan
print(qe.optimizedPlan)

# Physical plan (strategy selected)
print(qe.sparkPlan)

# Executed plan (with runtime info)
print(qe.executedPlan)

# Get plan as string
plan_string = qe.simpleString

# Check specific optimizations programmatically
if "PushedFilters" in qe.executedPlan.toString():
    print(" Predicate pushdown applied")

# Access statistics
print(qe.optimizedPlan.stats)</code></pre>
                    <strong>When to use which:</strong><br>
                     <strong>explain():</strong> Quick debugging, manual inspection<br>
                     <strong>queryExecution:</strong> Automated analysis, testing, monitoring
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 108: Check Execution Plan -->
<div class="question-content" id="q108">
    <div class="question-header">
        <h1 class="question-title">How do you check the execution plan of a DataFrame or SQL query? (explain())</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Quick Answer
        </div>
        <div class="definition-box">
            <p>Use <code>df.explain()</code> for DataFrames or <code>spark.sql(query).explain()</code> for SQL queries to view the execution plan.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-search"></i> Checking Execution Plans</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># For DataFrame
df = spark.read.parquet("data.parquet")
filtered = df.filter(col("age") > 25)

# Check execution plan
filtered.explain()

# For SQL query
spark.sql("SELECT * FROM users WHERE age > 25").explain()

# Extended information
filtered.explain(extended=True)

# Different modes
filtered.explain(mode="simple")      # Physical only
filtered.explain(mode="extended")    # All plans
filtered.explain(mode="formatted")   # Pretty format
filtered.explain(mode="cost")        # With costs
filtered.explain(mode="codegen")     # Generated code</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 109: Aggregations in PySpark -->
<div class="question-content" id="q109">
    <div class="question-header">
        <h1 class="question-title">How do you perform aggregations in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Aggregation Methods
        </div>
        <div class="definition-box">
            <p>Aggregations in PySpark combine multiple rows into summary values using functions like <code>count()</code>, <code>sum()</code>, <code>avg()</code>, <code>min()</code>, <code>max()</code>. Use <code>groupBy()</code> for grouped aggregations or <code>agg()</code> for direct aggregations.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Aggregations</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> groupBy()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> agg()</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Summary Statistics</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Aggregation Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-calculator"></i> Basic Aggregations</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import col, sum, avg, min, max, count, countDistinct

# Sample data
data = [
    ("Alice", "Engineering", 50000),
    ("Bob", "Engineering", 60000),
    ("Charlie", "Sales", 45000),
    ("David", "Sales", 55000),
    ("Eve", "Engineering", 65000)
]
df = spark.createDataFrame(data, ["name", "department", "salary"])

# 1. Simple aggregations (entire DataFrame)
df.agg(sum("salary")).show()
# +------------+
# |sum(salary)|
# +------------+
# |      275000|
# +------------+

# Multiple aggregations
df.agg(
    sum("salary").alias("total_salary"),
    avg("salary").alias("avg_salary"),
    count("name").alias("employee_count")
).show()

# +------------+----------+--------------+
# |total_salary|avg_salary|employee_count|
# +------------+----------+--------------+
# |      275000|   55000.0|             5|
# +------------+----------+--------------+

# 2. Grouped aggregations (groupBy)
df.groupBy("department").agg(
    sum("salary").alias("total_salary"),
    avg("salary").alias("avg_salary"),
    count("name").alias("count")
).show()

# +-----------+------------+----------+-----+
# | department|total_salary|avg_salary|count|
# +-----------+------------+----------+-----+
# |Engineering|      175000|  58333.33|    3|
# |      Sales|      100000|   50000.0|    2|
# +-----------+------------+----------+-----+

# 3. Simple groupBy syntax
df.groupBy("department").count().show()

# +-----------+-----+
# | department|count|
# +-----------+-----+
# |Engineering|    3|
# |      Sales|    2|
# +-----------+-----+

# 4. Multiple grouping columns
sales_data = [
    ("Alice", "Engineering", "2024", 50000),
    ("Bob", "Engineering", "2024", 60000),
    ("Charlie", "Sales", "2024", 45000),
    ("Alice", "Engineering", "2023", 48000),
    ("Bob", "Engineering", "2023", 55000)
]
sales_df = spark.createDataFrame(sales_data, ["name", "dept", "year", "salary"])

sales_df.groupBy("dept", "year").agg(
    sum("salary").alias("total"),
    avg("salary").alias("average")
).show()

# +-----------+----+------+-------+
# |       dept|year| total|average|
# +-----------+----+------+-------+
# |Engineering|2023|103000|51500.0|
# |Engineering|2024|110000|55000.0|
# |      Sales|2024| 45000|45000.0|
# +-----------+----+------+-------+

# 5. All common aggregate functions
from pyspark.sql.functions import stddev, variance, collect_list, collect_set

df.agg(
    count("*").alias("count"),
    sum("salary").alias("sum"),
    avg("salary").alias("avg"),
    min("salary").alias("min"),
    max("salary").alias("max"),
    stddev("salary").alias("stddev"),
    variance("salary").alias("variance"),
    countDistinct("department").alias("unique_depts")
).show()

# 6. Collect aggregations
df.groupBy("department").agg(
    collect_list("name").alias("employees"),
    collect_set("salary").alias("unique_salaries")
).show(truncate=False)

# +-----------+--------------------+------------------+
# |department |employees           |unique_salaries   |
# +-----------+--------------------+------------------+
# |Engineering|[Alice, Bob, Eve]   |[50000,60000,65000]|
# |Sales      |[Charlie, David]    |[45000, 55000]    |
# +-----------+--------------------+------------------+</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-filter"></i> Aggregations with Filters</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Filter before aggregation
df.filter(col("salary") > 50000) \
  .groupBy("department") \
  .count() \
  .show()

# Filter after aggregation
df.groupBy("department") \
  .agg(avg("salary").alias("avg_salary")) \
  .filter(col("avg_salary") > 52000) \
  .show()

# Conditional aggregation
from pyspark.sql.functions import when

df.groupBy("department").agg(
    sum(when(col("salary") > 55000, 1).otherwise(0)).alias("high_earners"),
    sum(when(col("salary") <= 55000, 1).otherwise(0)).alias("regular_earners")
).show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>What's the difference between agg() and groupBy()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>groupBy():</strong> Groups data by specified columns, returns GroupedData object<br>
                    <strong>agg():</strong> Applies aggregate functions, can be used with or without groupBy()<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># groupBy() alone doesn't compute anything
grouped = df.groupBy("department")  # Returns GroupedData
# Must follow with aggregation

# agg() after groupBy() - grouped aggregation
df.groupBy("department").agg(sum("salary"))

# agg() without groupBy() - entire DataFrame aggregation
df.agg(sum("salary"))</code></pre>
                    <strong>Pattern:</strong> groupBy()  agg()<br>
                    1. groupBy() creates groups<br>
                    2. agg() computes aggregate per group
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you aggregate multiple columns at once?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>Yes!</strong> Multiple ways to aggregate multiple columns:<br><br>
                    <strong>Method 1: Multiple arguments to agg():</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.groupBy("department").agg(
    sum("salary").alias("total_salary"),
    avg("salary").alias("avg_salary"),
    max("salary").alias("max_salary"),
    min("salary").alias("min_salary"),
    count("name").alias("count")
).show()</code></pre>
                    <strong>Method 2: Dictionary (column  function):</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>df.groupBy("department").agg({
    "salary": "sum",
    "name": "count"
}).show()

# Note: Dictionary syntax doesn't allow aliases</code></pre>
                    <strong>Method 3: expr() for complex aggregations:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.functions import expr

df.groupBy("department").agg(
    expr("sum(salary) as total"),
    expr("avg(salary) as average"),
    expr("count(*) as count")
).show()</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 110: Perform Aggregations -->
<div class="question-content" id="q110">
    <div class="question-header">
        <h1 class="question-title">How do you perform aggregations?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 7 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Performing Aggregations (Similar to Q109)
        </div>
        <div class="definition-box">
            <p>See Question 109 for comprehensive aggregation examples. Quick summary: Use <code>groupBy().agg()</code> for grouped aggregations, or <code>agg()</code> alone for full DataFrame aggregations.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between groupBy() and agg()?</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 109 for detailed explanation.
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Can you perform multiple aggregations at once?</strong></span>
                </div>
                <div class="cross-answer">
                    Yes, see Question 109 for examples.
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 111: groupBy and agg Functions -->
<div class="question-content" id="q111">
    <div class="question-header">
        <h1 class="question-title">Explain groupBy and agg functions</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Explanation
        </div>
        <div class="definition-box">
            <p><strong>groupBy()</strong> partitions DataFrame into groups based on column values. <strong>agg()</strong> applies aggregate functions to compute summary statistics for each group or the entire DataFrame.</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Detailed Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-layer-group"></i> groupBy() Explained</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># groupBy() creates groups
df = spark.createDataFrame([
    ("Alice", "Engineering", 50000),
    ("Bob", "Engineering", 60000),
    ("Charlie", "Sales", 45000)
], ["name", "dept", "salary"])

# groupBy returns GroupedData (not DataFrame!)
grouped = df.groupBy("dept")
print(type(grouped))  # <class 'pyspark.sql.group.GroupedData'>

# Must apply aggregation to get DataFrame
result = grouped.count()  # Now it's a DataFrame
result.show()

# +--------+-----+
# |    dept|count|
# +--------+-----+
# |Engineering|    2|
# |   Sales|    1|
# +--------+-----+</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-calculator"></i> agg() Explained</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import sum, avg, max

# agg() applies aggregations
df.groupBy("dept").agg(
    sum("salary"),    # Aggregate function 1
    avg("salary"),    # Aggregate function 2
    max("salary")     # Aggregate function 3
).show()

# With aliases for clarity
df.groupBy("dept").agg(
    sum("salary").alias("total"),
    avg("salary").alias("average"),
    max("salary").alias("maximum")
).show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between groupBy() and groupByKey()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>groupBy():</strong> DataFrame API, applies to DataFrames<br>
                    <strong>groupByKey():</strong> RDD API, applies to pair RDDs<br><br>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># DataFrame: groupBy()
df.groupBy("key").count()

# RDD: groupByKey()
rdd = spark.sparkContext.parallelize([("a", 1), ("b", 2), ("a", 3)])
rdd.groupByKey().mapValues(list).collect()
# [('a', [1, 3]), ('b', [2])]</code></pre>
                    <strong>Performance:</strong> Avoid groupByKey() on RDDs, use reduceByKey() instead (see Question 113).
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 112: groupBy and agg Operations -->
<div class="question-content" id="q112">
    <div class="question-header">
        <h1 class="question-title">How to perform groupBy and agg operations?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 6 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Quick Reference
        </div>
        <div class="definition-box">
            <p>See Questions 109-111 for comprehensive examples. Pattern: <code>df.groupBy(column).agg(aggregate_functions)</code></p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Quick Example
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> Complete Pattern</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import sum, avg, count

# Pattern: groupBy().agg()
df.groupBy("department").agg(
    sum("salary").alias("total_salary"),
    avg("salary").alias("avg_salary"),
    count("*").alias("employee_count")
).show()</code></pre>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 113: reduceByKey() vs groupByKey() -->
<div class="question-content" id="q113">
    <div class="question-header">
        <h1 class="question-title">What's the difference between reduceByKey() and groupByKey()?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Key Difference
        </div>
        <div class="definition-box">
            <p><strong>reduceByKey()</strong> combines values locally on each partition before shuffling, reducing network traffic. <strong>groupByKey()</strong> shuffles all values for each key across the network without local reduction.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> RDD Operations</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Performance</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Shuffle</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-image"></i>
            Visual Comparison
        </div>
        <div class="image-container">
            <svg width="900" height="600" viewBox="0 0 900 600">
                <rect width="900" height="600" fill="#1e293b"/>
                
                <text x="450" y="30" font-size="22" fill="#0ea5e9" text-anchor="middle" font-weight="bold">reduceByKey() vs groupByKey()</text>
                
                <!-- groupByKey() -->
                <rect x="50" y="60" width="380" height="250" fill="rgba(239, 68, 68, 0.1)" stroke="#ef4444" stroke-width="2" rx="12"/>
                <text x="240" y="90" font-size="16" fill="#ef4444" text-anchor="middle" font-weight="bold">groupByKey() - Inefficient</text>
                
                <rect x="80" y="110" width="140" height="80" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="150" y="135" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                <text x="150" y="155" font-size="10" fill="#cbd5e1" text-anchor="middle">("A", 1), ("B", 2)</text>
                <text x="150" y="175" font-size="10" fill="#cbd5e1" text-anchor="middle">("A", 3)</text>
                
                <rect x="250" y="110" width="140" height="80" fill="#334155" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="320" y="135" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                <text x="320" y="155" font-size="10" fill="#cbd5e1" text-anchor="middle">("A", 4), ("B", 5)</text>
                <text x="320" y="175" font-size="10" fill="#cbd5e1" text-anchor="middle">("C", 6)</text>
                
                <text x="240" y="215" font-size="13" fill="#ef4444" text-anchor="middle" font-weight="bold"> Shuffle ALL values (no local reduce)</text>
                
                <rect x="80" y="230" width="300" height="60" fill="rgba(239, 68, 68, 0.3)" stroke="#ef4444" stroke-width="2" rx="8"/>
                <text x="230" y="255" font-size="11" fill="#f1f5f9" text-anchor="middle">Network: ("A", [1,3,4]), ("B", [2,5]), ("C", [6])</text>
                <text x="230" y="275" font-size="10" fill="#ef4444" text-anchor="middle" font-weight="bold">6 values transferred!</text>
                
                <!-- reduceByKey() -->
                <rect x="470" y="60" width="380" height="250" fill="rgba(16, 185, 129, 0.1)" stroke="#10b981" stroke-width="2" rx="12"/>
                <text x="660" y="90" font-size="16" fill="#10b981" text-anchor="middle" font-weight="bold">reduceByKey() - Efficient</text>
                
                <rect x="500" y="110" width="140" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="570" y="135" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 1</text>
                <text x="570" y="155" font-size="10" fill="#cbd5e1" text-anchor="middle">("A", 1), ("B", 2)</text>
                <text x="570" y="175" font-size="10" fill="#cbd5e1" text-anchor="middle">("A", 3)</text>
                
                <text x="570" y="205" font-size="10" fill="#10b981" text-anchor="middle" font-weight="bold"> Local reduce</text>
                <text x="570" y="220" font-size="10" fill="#10b981" text-anchor="middle">A:4, B:2</text>
                
                <rect x="670" y="110" width="140" height="80" fill="#334155" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="740" y="135" font-size="12" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Partition 2</text>
                <text x="740" y="155" font-size="10" fill="#cbd5e1" text-anchor="middle">("A", 4), ("B", 5)</text>
                <text x="740" y="175" font-size="10" fill="#cbd5e1" text-anchor="middle">("C", 6)</text>
                
                <text x="740" y="205" font-size="10" fill="#10b981" text-anchor="middle" font-weight="bold"> Local reduce</text>
                <text x="740" y="220" font-size="10" fill="#10b981" text-anchor="middle">A:4, B:5, C:6</text>
                
                <text x="660" y="255" font-size="13" fill="#10b981" text-anchor="middle" font-weight="bold"> Shuffle REDUCED values</text>
                
                <rect x="530" y="270" width="260" height="30" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="660" y="290" font-size="10" fill="#f1f5f9" text-anchor="middle">Network: ("A", 4), ("A", 4), ("B", 2), ("B", 5), ("C", 6)</text>
                
                <rect x="500" y="230" width="140" height="60" fill="rgba(16, 185, 129, 0.3)" stroke="#10b981" stroke-width="2" rx="8"/>
                <text x="570" y="255" font-size="11" fill="#f1f5f9" text-anchor="middle" font-weight="bold">Only 5 values</text>
                <text x="570" y="275" font-size="10" fill="#10b981" text-anchor="middle">instead of 6!</text>
                
                <!-- Comparison -->
                <rect x="100" y="350" width="700" height="220" fill="rgba(245, 158, 11, 0.1)" stroke="#f59e0b" stroke-width="2" rx="12"/>
                <text x="450" y="380" font-size="16" fill="#f59e0b" text-anchor="middle" font-weight="bold">Performance Impact</text>
                
                <text x="150" y="415" font-size="13" fill="#ef4444" text-anchor="start" font-weight="bold">groupByKey():</text>
                <text x="150" y="435" font-size="11" fill="#cbd5e1" text-anchor="start"> Shuffles ALL values</text>
                <text x="150" y="453" font-size="11" fill="#cbd5e1" text-anchor="start"> High network traffic</text>
                <text x="150" y="471" font-size="11" fill="#cbd5e1" text-anchor="start"> More memory needed</text>
                <text x="150" y="489" font-size="11" fill="#cbd5e1" text-anchor="start"> Slower (2-10x)</text>
                <text x="150" y="507" font-size="11" fill="#cbd5e1" text-anchor="start"> Use only if you need all values</text>
                <text x="150" y="540" font-size="12" fill="#ef4444" text-anchor="start" font-weight="bold">Example: 1 million values  1M shuffled</text>
                
                <text x="500" y="415" font-size="13" fill="#10b981" text-anchor="start" font-weight="bold">reduceByKey():</text>
                <text x="500" y="435" font-size="11" fill="#cbd5e1" text-anchor="start"> Local reduction first</text>
                <text x="500" y="453" font-size="11" fill="#cbd5e1" text-anchor="start"> Less network traffic</text>
                <text x="500" y="471" font-size="11" fill="#cbd5e1" text-anchor="start"> Less memory needed</text>
                <text x="500" y="489" font-size="11" fill="#cbd5e1" text-anchor="start"> Faster (2-10x)</text>
                <text x="500" y="507" font-size="11" fill="#cbd5e1" text-anchor="start"> Preferred for aggregations</text>
                <text x="500" y="540" font-size="12" fill="#10b981" text-anchor="start" font-weight="bold">Example: 1 million values  1K shuffled</text>
            </svg>
            <p class="image-caption">reduceByKey: Local reduce before shuffle | groupByKey: Shuffle all values</p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Code Comparison
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-code"></i> Performance Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>import time

# Create test data
data = [(f"key_{i % 100}", 1) for i in range(1000000)]  # 1M records
rdd = spark.sparkContext.parallelize(data, numSlices=100)

# Method 1: groupByKey() - Slow
start = time.time()
result1 = rdd.groupByKey().mapValues(sum).collect()
groupby_time = time.time() - start
print(f"groupByKey: {groupby_time:.2f}s")

# Method 2: reduceByKey() - Fast
start = time.time()
result2 = rdd.reduceByKey(lambda x, y: x + y).collect()
reduce_time = time.time() - start
print(f"reduceByKey: {reduce_time:.2f}s")

print(f"Speedup: {groupby_time/reduce_time:.2f}x")

# Typical results:
# groupByKey: 15.32s  (shuffles 1M values)
# reduceByKey: 3.45s (shuffles 100 reduced values)
# Speedup: 4.44x

# Why reduceByKey is faster:
# groupByKey shuffles: 1,000,000 values
# reduceByKey shuffles: 100 values (after local reduction)
# Network traffic reduced by 10,000x!</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which one is preferred and why?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>reduceByKey() is preferred</strong> for aggregation operations because:<br><br>
                    <strong>1. Better Performance:</strong> 2-10x faster due to local reduction<br>
                    <strong>2. Less Network:</strong> Transfers fewer, pre-aggregated values<br>
                    <strong>3. Less Memory:</strong> Doesn't store all values per key<br>
                    <strong>4. More Scalable:</strong> Works well with large datasets<br><br>
                    <strong>Use reduceByKey when:</strong><br>
                     Aggregating values (sum, count, max, min)<br>
                     Values can be combined with associative operation<br>
                     Don't need all individual values<br><br>
                    <strong>Use groupByKey when:</strong><br>
                     Need all values for each key<br>
                     Operation is not associative/commutative<br>
                     Building complex data structures<br><br>
                    <strong>Example - When to use each:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Good: reduceByKey for sum
rdd.reduceByKey(lambda x, y: x + y)

# Bad: groupByKey for sum (inefficient!)
rdd.groupByKey().mapValues(sum)

# Good: groupByKey when you need all values
rdd.groupByKey().mapValues(list)  # Need complete list

# Alternative: Use aggregateByKey() for more complex cases
rdd.aggregateByKey(
    0,                          # Initial value
    lambda acc, v: acc + v,     # Combine within partition
    lambda acc1, acc2: acc1 + acc2  # Combine across partitions
)</code></pre>
                    <strong>Best Practice:</strong> Always prefer reduceByKey() over groupByKey() for aggregations!
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
<!-- Question 115: Sort Within Groups -->
<div class="question-content" id="q115">
    <div class="question-header">
        <h1 class="question-title">How can you sort data within each group after grouping?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 9 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Sorting Within Groups
        </div>
        <div class="definition-box">
            <p>To sort data within each group, use <strong>window functions</strong> with <code>partitionBy()</code> to define groups and <code>orderBy()</code> to sort within each partition. Use functions like <code>row_number()</code>, <code>rank()</code>, or <code>dense_rank()</code>.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Window Functions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Sorting</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Ranking</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> partitionBy</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-sort"></i> Sorting Within Groups</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, dense_rank, col

# Sample data - employees with salaries
data = [
    ("Alice", "Engineering", 75000),
    ("Bob", "Engineering", 85000),
    ("Charlie", "Engineering", 65000),
    ("David", "Sales", 60000),
    ("Eve", "Sales", 70000),
    ("Frank", "Sales", 55000)
]
df = spark.createDataFrame(data, ["name", "department", "salary"])

# Define window: partition by department, order by salary descending
window = Window.partitionBy("department").orderBy(col("salary").desc())

# Add row number within each department
result = df.withColumn("rank_in_dept", row_number().over(window))
result.show()

# +-------+-----------+------+------------+
# |   name| department|salary|rank_in_dept|
# +-------+-----------+------+------------+
# |    Bob|Engineering| 85000|           1|   Highest in Engineering
# |  Alice|Engineering| 75000|           2|
# |Charlie|Engineering| 65000|           3|
# |    Eve|      Sales| 70000|           1|   Highest in Sales
# |  David|      Sales| 60000|           2|
# |  Frank|      Sales| 55000|           3|
# +-------+-----------+------+------------+

# Filter top 2 in each department
top2_per_dept = result.filter(col("rank_in_dept") <= 2)
top2_per_dept.show()

# +-----+-----------+------+------------+
# | name| department|salary|rank_in_dept|
# +-----+-----------+------+------------+
# |  Bob|Engineering| 85000|           1|
# |Alice|Engineering| 75000|           2|
# |  Eve|      Sales| 70000|           1|
# |David|      Sales| 60000|           2|
# +-----+-----------+------+------------+

# Using rank() - handles ties
data_with_ties = [
    ("Alice", "Engineering", 75000),
    ("Bob", "Engineering", 75000),  # Same salary as Alice
    ("Charlie", "Engineering", 65000),
]
df_ties = spark.createDataFrame(data_with_ties, ["name", "department", "salary"])

window_rank = Window.partitionBy("department").orderBy(col("salary").desc())

df_ties.withColumn("row_num", row_number().over(window_rank)) \
       .withColumn("rank", rank().over(window_rank)) \
       .withColumn("dense_rank", dense_rank().over(window_rank)) \
       .show()

# +-------+-----------+------+-------+----+----------+
# |   name| department|salary|row_num|rank|dense_rank|
# +-------+-----------+------+-------+----+----------+
# |  Alice|Engineering| 75000|      1|   1|         1|
# |    Bob|Engineering| 75000|      2|   1|         1|   Both rank 1
# |Charlie|Engineering| 65000|      3|   3|         2|   rank skips to 3
# +-------+-----------+------+-------+----+----------+
#
# row_number: Always unique (1, 2, 3)
# rank: Ties get same rank, next rank skips (1, 1, 3)
# dense_rank: Ties get same rank, next rank continues (1, 1, 2)</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Which window function would you use for that?</strong></span>
                </div>
                <div class="cross-answer">
                    Use these window functions for sorting within groups:<br><br>
                    <strong>1. row_number():</strong> Unique sequential number (1, 2, 3...)
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>row_number().over(Window.partitionBy("dept").orderBy("salary"))</code></pre>
                    <strong>2. rank():</strong> Same rank for ties, gaps in sequence (1, 1, 3, 4...)
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>rank().over(Window.partitionBy("dept").orderBy("salary"))</code></pre>
                    <strong>3. dense_rank():</strong> Same rank for ties, no gaps (1, 1, 2, 3...)
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>dense_rank().over(Window.partitionBy("dept").orderBy("salary"))</code></pre>
                    <strong>When to use which:</strong><br>
                     <strong>row_number():</strong> Need unique numbers, don't care about ties<br>
                     <strong>rank():</strong> Want ties to have same rank, gaps acceptable<br>
                     <strong>dense_rank():</strong> Want ties to have same rank, no gaps
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 116: Window Functions -->
<div class="question-content" id="q116">
    <div class="question-header">
        <h1 class="question-title">What is window function in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            What are Window Functions?
        </div>
        <div class="definition-box">
            <p><strong>Window functions</strong> perform calculations across a set of rows (a "window") that are related to the current row, without collapsing rows like groupBy does. They enable ranking, running totals, moving averages, and accessing neighboring rows.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Window Functions</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Ranking</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Analytics</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> SQL</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Key Concepts
        </div>

        <div class="highlight-box">
            <strong>Window Specification Components:</strong>
            <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.window import Window

window = Window \
    .partitionBy("column1")      # Groups (like groupBy)
    .orderBy("column2")           # Sorting within partition
    .rowsBetween(start, end)      # Frame: which rows to include

# Example:
Window.partitionBy("department") \
      .orderBy("salary") \
      .rowsBetween(Window.unboundedPreceding, Window.currentRow)</code></pre>
        </div>

        <div class="highlight-box">
            <strong>Common Window Functions:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li><strong>Ranking:</strong> row_number(), rank(), dense_rank(), ntile()</li>
                <li><strong>Analytics:</strong> lag(), lead(), first(), last()</li>
                <li><strong>Aggregates:</strong> sum(), avg(), min(), max(), count()</li>
            </ul>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Window Function Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-window-maximize"></i> Comprehensive Examples</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.window import Window
from pyspark.sql.functions import (
    row_number, rank, dense_rank, ntile,
    lag, lead, first, last,
    sum, avg, min, max, count,
    col
)

# Sample data - sales
data = [
    ("2024-01-01", "A", 100),
    ("2024-01-02", "A", 150),
    ("2024-01-03", "A", 120),
    ("2024-01-01", "B", 200),
    ("2024-01-02", "B", 180),
    ("2024-01-03", "B", 220)
]
df = spark.createDataFrame(data, ["date", "product", "sales"])

# 1. Ranking Functions
window_rank = Window.partitionBy("product").orderBy(col("sales").desc())

df.withColumn("row_num", row_number().over(window_rank)) \
  .withColumn("rank", rank().over(window_rank)) \
  .withColumn("dense_rank", dense_rank().over(window_rank)) \
  .show()

# +----------+-------+-----+-------+----+----------+
# |      date|product|sales|row_num|rank|dense_rank|
# +----------+-------+-----+-------+----+----------+
# |2024-01-02|      A|  150|      1|   1|         1|
# |2024-01-03|      A|  120|      2|   2|         2|
# |2024-01-01|      A|  100|      3|   3|         3|
# |2024-01-03|      B|  220|      1|   1|         1|
# |2024-01-01|      B|  200|      2|   2|         2|
# |2024-01-02|      B|  180|      3|   3|         3|
# +----------+-------+-----+-------+----+----------+

# 2. Lag/Lead (Access previous/next rows)
window_ordered = Window.partitionBy("product").orderBy("date")

df.withColumn("prev_sales", lag("sales", 1).over(window_ordered)) \
  .withColumn("next_sales", lead("sales", 1).over(window_ordered)) \
  .show()

# +----------+-------+-----+----------+----------+
# |      date|product|sales|prev_sales|next_sales|
# +----------+-------+-----+----------+----------+
# |2024-01-01|      A|  100|      null|       150|
# |2024-01-02|      A|  150|       100|       120|
# |2024-01-03|      A|  120|       150|      null|
# |2024-01-01|      B|  200|      null|       180|
# |2024-01-02|      B|  180|       200|       220|
# |2024-01-03|      B|  220|       180|      null|
# +----------+-------+-----+----------+----------+

# 3. Running Total
window_running = Window.partitionBy("product") \
                       .orderBy("date") \
                       .rowsBetween(Window.unboundedPreceding, Window.currentRow)

df.withColumn("running_total", sum("sales").over(window_running)) \
  .show()

# +----------+-------+-----+-------------+
# |      date|product|sales|running_total|
# +----------+-------+-----+-------------+
# |2024-01-01|      A|  100|          100|
# |2024-01-02|      A|  150|          250|   100+150
# |2024-01-03|      A|  120|          370|   100+150+120
# |2024-01-01|      B|  200|          200|
# |2024-01-02|      B|  180|          380|
# |2024-01-03|      B|  220|          600|
# +----------+-------+-----+-------------+

# 4. Moving Average (3-day)
window_moving = Window.partitionBy("product") \
                      .orderBy("date") \
                      .rowsBetween(-2, 0)  # Current + 2 previous

df.withColumn("moving_avg_3day", avg("sales").over(window_moving)) \
  .show()

# +----------+-------+-----+---------------+
# |      date|product|sales|moving_avg_3day|
# +----------+-------+-----+---------------+
# |2024-01-01|      A|  100|          100.0|   Only 1 row
# |2024-01-02|      A|  150|          125.0|   (100+150)/2
# |2024-01-03|      A|  120|          123.3|   (100+150+120)/3
# |2024-01-01|      B|  200|          200.0|
# |2024-01-02|      B|  180|          190.0|
# |2024-01-03|      B|  220|          200.0|
# +----------+-------+-----+---------------+

# 5. Difference from Group Average
window_partition = Window.partitionBy("product")

df.withColumn("avg_sales", avg("sales").over(window_partition)) \
  .withColumn("diff_from_avg", col("sales") - col("avg_sales")) \
  .show()

# +----------+-------+-----+---------+-------------+
# |      date|product|sales|avg_sales|diff_from_avg|
# +----------+-------+-----+---------+-------------+
# |2024-01-01|      A|  100|    123.3|        -23.3|
# |2024-01-02|      A|  150|    123.3|         26.7|
# |2024-01-03|      A|  120|    123.3|         -3.3|
# |2024-01-01|      B|  200|    200.0|          0.0|
# |2024-01-02|      B|  180|    200.0|        -20.0|
# |2024-01-03|      B|  220|    200.0|         20.0|
# +----------+-------+-----+---------+-------------+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Give an example of ranking within partitions</strong></span>
                </div>
                <div class="cross-answer">
                    See examples in code above. Quick example:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.window import Window
from pyspark.sql.functions import rank, col

# Rank employees by salary within each department
window = Window.partitionBy("department").orderBy(col("salary").desc())
df.withColumn("salary_rank", rank().over(window)).show()</code></pre>
                </div>
            </div>

            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Difference between rank() and dense_rank()?</strong></span>
                </div>
                <div class="cross-answer">
                    <strong>rank():</strong> Leaves gaps after ties<br>
                    <strong>dense_rank():</strong> No gaps, consecutive ranks<br><br>
                    <strong>Example with tied salaries:</strong>
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code># Salaries: 90K, 80K, 80K, 70K

# rank():
# 90K  rank 1
# 80K  rank 2 (tied)
# 80K  rank 2 (tied)
# 70K  rank 4 (skips 3!)

# dense_rank():
# 90K  rank 1
# 80K  rank 2 (tied)
# 80K  rank 2 (tied)
# 70K  rank 3 (no gap)</code></pre>
                    <strong>When to use:</strong><br>
                     <strong>rank():</strong> Competition-style ranking (Olympics: 1st, 1st, 3rd)<br>
                     <strong>dense_rank():</strong> Need consecutive ranks
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 117: Window Operations -->
<div class="question-content" id="q117">
    <div class="question-header">
        <h1 class="question-title">How do you perform window operations in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 8 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Window Operations Steps
        </div>
        <div class="definition-box">
            <p>To perform window operations: (1) Import Window, (2) Define window specification using <code>partitionBy()</code> and <code>orderBy()</code>, (3) Apply window function with <code>.over(window)</code></p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Step-by-Step Process
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-list-ol"></i> Window Operation Steps</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col

# Step 1: Import necessary components
# Already done above

# Step 2: Define window specification
window = Window.partitionBy("department").orderBy(col("salary").desc())

# Step 3: Apply window function
df.withColumn("rank", row_number().over(window)).show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Give an example using row_number() or rank()</strong></span>
                </div>
                <div class="cross-answer">
                    See Question 116 for comprehensive examples. Quick example:
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, col

# Create window
window = Window.partitionBy("dept").orderBy(col("salary").desc())

# Apply row_number
df.withColumn("row_num", row_number().over(window)) \
  .withColumn("rank", rank().over(window)) \
  .show()</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 118: Using Window Functions -->
<div class="question-content" id="q118">
    <div class="question-header">
        <h1 class="question-title">How do you use window functions in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Using Window Functions
        </div>
        <div class="definition-box">
            <p>See Questions 116-117 for comprehensive explanations. Pattern: Define window  Apply function with <code>.over(window)</code></p>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-question-circle"></i>
            Cross Questions
        </div>
        <div class="cross-questions">
            <div class="cross-question-item">
                <div class="cross-question-text">
                    <i class="fas fa-arrow-right"></i>
                    <span><strong>Example: Find the top 3 employees by salary within each department</strong></span>
                </div>
                <div class="cross-answer">
                    <pre style="background: var(--code-bg); padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;"><code>from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col

# Sample data
data = [
    ("Alice", "Engineering", 95000),
    ("Bob", "Engineering", 85000),
    ("Charlie", "Engineering", 75000),
    ("David", "Engineering", 65000),
    ("Eve", "Sales", 80000),
    ("Frank", "Sales", 70000),
    ("Grace", "Sales", 60000),
    ("Henry", "Sales", 50000)
]
df = spark.createDataFrame(data, ["name", "department", "salary"])

# Define window: partition by department, order by salary desc
window = Window.partitionBy("department").orderBy(col("salary").desc())

# Add rank
ranked = df.withColumn("rank", row_number().over(window))

# Filter top 3
top3 = ranked.filter(col("rank") <= 3)
top3.show()

# +-------+-----------+------+----+
# |   name| department|salary|rank|
# +-------+-----------+------+----+
# |  Alice|Engineering| 95000|   1|   Top 3 in Engineering
# |    Bob|Engineering| 85000|   2|
# |Charlie|Engineering| 75000|   3|
# |    Eve|      Sales| 80000|   1|   Top 3 in Sales
# |  Frank|      Sales| 70000|   2|
# |  Grace|      Sales| 60000|   3|
# +-------+-----------+------+----+

# Alternative: Using rank() with ties
window_rank = Window.partitionBy("department").orderBy(col("salary").desc())
df.withColumn("rank", rank().over(window_rank)) \
  .filter(col("rank") <= 3) \
  .show()

# If there are salary ties, rank() will handle them appropriately</code></pre>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 119: Running Total and Moving Average -->
<div class="question-content" id="q119">
    <div class="question-header">
        <h1 class="question-title">How to calculate running total or moving average in PySpark?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 10 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Running Total and Moving Average
        </div>
        <div class="definition-box">
            <p>Use window functions with <strong>frame specification</strong> (<code>rowsBetween()</code> or <code>rangeBetween()</code>) to calculate running totals and moving averages.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Running Total</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Moving Average</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Window Frame</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Time Series</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-chart-line"></i> Running Total</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.window import Window
from pyspark.sql.functions import sum, col

# Sample data - daily sales
data = [
    ("2024-01-01", 100),
    ("2024-01-02", 150),
    ("2024-01-03", 120),
    ("2024-01-04", 180),
    ("2024-01-05", 200)
]
df = spark.createDataFrame(data, ["date", "sales"])

# Running total window
# From beginning to current row
window_running = Window.orderBy("date") \
                       .rowsBetween(Window.unboundedPreceding, Window.currentRow)

# Calculate running total
running_total_df = df.withColumn("running_total", sum("sales").over(window_running))
running_total_df.show()

# +----------+-----+-------------+
# |      date|sales|running_total|
# +----------+-----+-------------+
# |2024-01-01|  100|          100|   100
# |2024-01-02|  150|          250|   100+150
# |2024-01-03|  120|          370|   100+150+120
# |2024-01-04|  180|          550|   100+150+120+180
# |2024-01-05|  200|          750|   100+150+120+180+200
# +----------+-----+-------------+

# Alternative: Using rangeBetween for date ranges
window_range = Window.orderBy(col("date").cast("long")) \
                     .rangeBetween(Window.unboundedPreceding, 0)

df.withColumn("running_total_range", sum("sales").over(window_range)).show()</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-chart-area"></i> Moving Average</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import avg

# 3-day moving average window
# Current row + 2 previous rows
window_3day = Window.orderBy("date").rowsBetween(-2, 0)

moving_avg_df = df.withColumn("moving_avg_3day", avg("sales").over(window_3day))
moving_avg_df.show()

# +----------+-----+---------------+
# |      date|sales|moving_avg_3day|
# +----------+-----+---------------+
# |2024-01-01|  100|          100.0|   Only 1 row: 100/1
# |2024-01-02|  150|          125.0|   2 rows: (100+150)/2
# |2024-01-03|  120|          123.3|   3 rows: (100+150+120)/3
# |2024-01-04|  180|          150.0|   3 rows: (150+120+180)/3
# |2024-01-05|  200|          166.7|   3 rows: (120+180+200)/3
# +----------+-----+---------------+

# 5-day moving average
window_5day = Window.orderBy("date").rowsBetween(-4, 0)
df.withColumn("moving_avg_5day", avg("sales").over(window_5day)).show()

# Centered moving average (previous, current, next)
window_centered = Window.orderBy("date").rowsBetween(-1, 1)
df.withColumn("centered_avg", avg("sales").over(window_centered)).show()

# +----------+-----+------------+
# |      date|sales|centered_avg|
# +----------+-----+------------+
# |2024-01-01|  100|       125.0|   (100+150)/2
# |2024-01-02|  150|       123.3|   (100+150+120)/3
# |2024-01-03|  120|       150.0|   (150+120+180)/3
# |2024-01-04|  180|       166.7|   (120+180+200)/3
# |2024-01-05|  200|       190.0|   (180+200)/2
# +----------+-----+------------+</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-project-diagram"></i> Complete Example with Partitions</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Sales data with multiple products
data_multi = [
    ("2024-01-01", "A", 100),
    ("2024-01-02", "A", 150),
    ("2024-01-03", "A", 120),
    ("2024-01-01", "B", 200),
    ("2024-01-02", "B", 180),
    ("2024-01-03", "B", 220)
]
df_multi = spark.createDataFrame(data_multi, ["date", "product", "sales"])

# Running total per product
window_by_product = Window.partitionBy("product") \
                          .orderBy("date") \
                          .rowsBetween(Window.unboundedPreceding, Window.currentRow)

# Moving average per product
window_ma_product = Window.partitionBy("product") \
                          .orderBy("date") \
                          .rowsBetween(-1, 0)  # 2-day MA

result = df_multi.withColumn("running_total", sum("sales").over(window_by_product)) \
                 .withColumn("moving_avg_2day", avg("sales").over(window_ma_product))

result.show()

# +----------+-------+-----+-------------+---------------+
# |      date|product|sales|running_total|moving_avg_2day|
# +----------+-------+-----+-------------+---------------+
# |2024-01-01|      A|  100|          100|          100.0|
# |2024-01-02|      A|  150|          250|          125.0|   (100+150)/2
# |2024-01-03|      A|  120|          370|          135.0|   (150+120)/2
# |2024-01-01|      B|  200|          200|          200.0|
# |2024-01-02|      B|  180|          380|          190.0|
# |2024-01-03|      B|  220|          600|          200.0|
# +----------+-------+-----+-------------+---------------+

# Calculate percentage change
from pyspark.sql.functions import lag

window_lag = Window.partitionBy("product").orderBy("date")

df_multi.withColumn("prev_sales", lag("sales", 1).over(window_lag)) \
        .withColumn("pct_change", 
                    ((col("sales") - col("prev_sales")) / col("prev_sales") * 100)) \
        .show()

# +----------+-------+-----+----------+----------+
# |      date|product|sales|prev_sales|pct_change|
# +----------+-------+-----+----------+----------+
# |2024-01-01|      A|  100|      null|      null|
# |2024-01-02|      A|  150|       100|      50.0|   50% increase
# |2024-01-03|      A|  120|       150|     -20.0|   20% decrease
# |2024-01-01|      B|  200|      null|      null|
# |2024-01-02|      B|  180|       200|     -10.0|
# |2024-01-03|      B|  220|       180|      22.2|
# +----------+-------+-----+----------+----------+</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Frame Specification Reference
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Frame Type</th>
                        <th>Syntax</th>
                        <th>Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Running Total</strong></td>
                        <td><code>rowsBetween(unboundedPreceding, currentRow)</code></td>
                        <td>Cumulative sum from start to current</td>
                    </tr>
                    <tr>
                        <td><strong>Moving Average (N)</strong></td>
                        <td><code>rowsBetween(-(N-1), 0)</code></td>
                        <td>Average of current + N-1 previous</td>
                    </tr>
                    <tr>
                        <td><strong>Centered Average</strong></td>
                        <td><code>rowsBetween(-1, 1)</code></td>
                        <td>Previous + current + next</td>
                    </tr>
                    <tr>
                        <td><strong>All Rows</strong></td>
                        <td><code>rowsBetween(unboundedPreceding, unboundedFollowing)</code></td>
                        <td>Entire partition</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>

<!-- Question 120: Pivot and Unpivot -->
<div class="question-content" id="q120">
    <div class="question-header">
        <h1 class="question-title">How to pivot and unpivot data?</h1>
        <div class="question-meta">
            <span class="meta-tag"><i class="fas fa-layer-group"></i> Expert</span>
            <span class="meta-tag"><i class="fas fa-clock"></i> 12 min read</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-book"></i>
            Pivot and Unpivot
        </div>
        <div class="definition-box">
            <p><strong>Pivot:</strong> Rotates rows into columns (wide format). <strong>Unpivot:</strong> Rotates columns into rows (long format). Use <code>pivot()</code> for pivoting and <code>stack()</code> or <code>melt()</code> pattern for unpivoting.</p>
        </div>

        <div class="keyword-tags">
            <span class="keyword-tag"><i class="fas fa-tag"></i> Pivot</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Unpivot</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Reshape</span>
            <span class="keyword-tag"><i class="fas fa-tag"></i> Data Transformation</span>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-code"></i>
            Pivot Examples
        </div>

        <div class="example-box">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-exchange-alt"></i> Pivot (Long to Wide)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import sum, avg, count

# Original data (long format)
data = [
    ("Alice", "Q1", 100),
    ("Alice", "Q2", 150),
    ("Alice", "Q3", 120),
    ("Bob", "Q1", 200),
    ("Bob", "Q2", 180),
    ("Bob", "Q3", 220)
]
df_long = spark.createDataFrame(data, ["name", "quarter", "sales"])

print("Original (Long Format):")
df_long.show()
# +-----+-------+-----+
# | name|quarter|sales|
# +-----+-------+-----+
# |Alice|     Q1|  100|
# |Alice|     Q2|  150|
# |Alice|     Q3|  120|
# |  Bob|     Q1|  200|
# |  Bob|     Q2|  180|
# |  Bob|     Q3|  220|
# +-----+-------+-----+

# Pivot: Convert quarters from rows to columns
df_pivoted = df_long.groupBy("name").pivot("quarter").sum("sales")

print("After Pivot (Wide Format):")
df_pivoted.show()
# +-----+---+---+---+
# | name| Q1| Q2| Q3|
# +-----+---+---+---+
# |Alice|100|150|120|
# |  Bob|200|180|220|
# +-----+---+---+---+

# Pivot with specific values (better performance)
df_pivoted2 = df_long.groupBy("name") \
                     .pivot("quarter", ["Q1", "Q2", "Q3"]) \
                     .sum("sales")

# Pivot with multiple aggregations
df_multi_agg = df_long.groupBy("name") \
                      .pivot("quarter") \
                      .agg(sum("sales").alias("total_sales"),
                           avg("sales").alias("avg_sales"))

# Pivot with multiple grouping columns
data_multi = [
    ("Alice", "2024", "Q1", 100),
    ("Alice", "2024", "Q2", 150),
    ("Bob", "2024", "Q1", 200),
    ("Bob", "2024", "Q2", 180)
]
df_multi = spark.createDataFrame(data_multi, ["name", "year", "quarter", "sales"])

df_multi.groupBy("name", "year").pivot("quarter").sum("sales").show()
# +-----+----+---+---+
# | name|year| Q1| Q2|
# +-----+----+---+---+
# |Alice|2024|100|150|
# |  Bob|2024|200|180|
# +-----+----+---+---+</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-undo"></i> Unpivot (Wide to Long)</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code>from pyspark.sql.functions import expr, col

# Wide format data
data_wide = [
    ("Alice", 100, 150, 120),
    ("Bob", 200, 180, 220)
]
df_wide = spark.createDataFrame(data_wide, ["name", "Q1", "Q2", "Q3"])

print("Original (Wide Format):")
df_wide.show()
# +-----+---+---+---+
# | name| Q1| Q2| Q3|
# +-----+---+---+---+
# |Alice|100|150|120|
# |  Bob|200|180|220|
# +-----+---+---+---+

# Method 1: Using stack() function
df_unpivoted = df_wide.selectExpr(
    "name",
    "stack(3, 'Q1', Q1, 'Q2', Q2, 'Q3', Q3) as (quarter, sales)"
)

print("After Unpivot (Long Format):")
df_unpivoted.show()
# +-----+-------+-----+
# | name|quarter|sales|
# +-----+-------+-----+
# |Alice|     Q1|  100|
# |Alice|     Q2|  150|
# |Alice|     Q3|  120|
# |  Bob|     Q1|  200|
# |  Bob|     Q2|  180|
# |  Bob|     Q3|  220|
# +-----+-------+-----+

# Method 2: Using expr() for dynamic unpivot
# Get column names to unpivot
value_cols = ["Q1", "Q2", "Q3"]
stack_expr = f"stack({len(value_cols)}, "
stack_expr += ", ".join([f"'{col}', `{col}`" for col in value_cols])
stack_expr += ") as (quarter, sales)"

df_unpivoted2 = df_wide.selectExpr("name", stack_expr)

# Method 3: Programmatic unpivot (for many columns)
def unpivot(df, id_vars, value_vars, var_name="variable", value_name="value"):
    """
    Unpivot DataFrame from wide to long format
    
    Args:
        df: Input DataFrame
        id_vars: Columns to keep as identifiers
        value_vars: Columns to unpivot
        var_name: Name for variable column
        value_name: Name for value column
    """
    # Create stack expression
    stack_expr = f"stack({len(value_vars)}, "
    stack_expr += ", ".join([f"'{col}', `{col}`" for col in value_vars])
    stack_expr += f") as ({var_name}, {value_name})"
    
    # Select id columns and unpivoted columns
    select_expr = id_vars + [stack_expr]
    
    return df.selectExpr(*select_expr)

# Usage
result = unpivot(df_wide, 
                 id_vars=["name"], 
                 value_vars=["Q1", "Q2", "Q3"],
                 var_name="quarter",
                 value_name="sales")
result.show()

# Method 4: Using melt pattern (Spark 3.4+)
# Note: melt() is available in Spark 3.4+
# df.melt(ids=["name"], values=["Q1", "Q2", "Q3"], 
#         variableColumnName="quarter", valueColumnName="sales")</code></pre>
        </div>

        <div class="example-box" style="margin-top: 1.5rem;">
            <div class="example-header">
                <span class="example-title"><i class="fas fa-project-diagram"></i> Complete Pivot/Unpivot Example</span>
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            <pre><code># Real-world example: Sales data transformation

# Original data
sales_data = [
    ("2024-01", "ProductA", "Store1", 1000),
    ("2024-01", "ProductA", "Store2", 1500),
    ("2024-01", "ProductB", "Store1", 800),
    ("2024-01", "ProductB", "Store2", 1200),
    ("2024-02", "ProductA", "Store1", 1100),
    ("2024-02", "ProductA", "Store2", 1600),
    ("2024-02", "ProductB", "Store1", 850),
    ("2024-02", "ProductB", "Store2", 1300)
]
df_sales = spark.createDataFrame(sales_data, ["month", "product", "store", "sales"])

print("Original Sales Data:")
df_sales.show()

# Pivot: Products as columns, stores as rows per month
pivoted_sales = df_sales.groupBy("month", "store") \
                        .pivot("product") \
                        .sum("sales")

print("Pivoted (Products as Columns):")
pivoted_sales.show()
# +-------+------+--------+--------+
# |  month| store|ProductA|ProductB|
# +-------+------+--------+--------+
# |2024-01|Store1|    1000|     800|
# |2024-01|Store2|    1500|    1200|
# |2024-02|Store1|    1100|     850|
# |2024-02|Store2|    1600|    1300|
# +-------+------+--------+--------+

# Unpivot back to original format
unpivoted_sales = pivoted_sales.selectExpr(
    "month",
    "store",
    "stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (product, sales)"
)

print("Unpivoted (Back to Original Format):")
unpivoted_sales.show()

# Performance tip: Specify pivot values for better performance
distinct_products = ["ProductA", "ProductB"]
optimized_pivot = df_sales.groupBy("month", "store") \
                          .pivot("product", distinct_products) \
                          .sum("sales")

print("Optimized Pivot (with specified values):")
optimized_pivot.show()</code></pre>
        </div>
    </div>

    <div class="content-card">
        <div class="section-title">
            <i class="fas fa-lightbulb"></i>
            Best Practices
        </div>

        <div class="highlight-box">
            <strong>Pivot Performance Tips:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Always specify pivot values when known: <code>.pivot("column", ["val1", "val2"])</code></li>
                <li>Limit number of pivot columns (avoid pivoting on high-cardinality columns)</li>
                <li>Filter data before pivoting to reduce volume</li>
                <li>Use appropriate aggregation function</li>
            </ul>
        </div>

        <div class="highlight-box">
            <strong>Unpivot Best Practices:</strong>
            <ul style="margin-top: 0.5rem; color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem;">
                <li>Use <code>stack()</code> for small number of columns</li>
                <li>Create reusable unpivot function for many columns</li>
                <li>Consider using Spark 3.4+ <code>melt()</code> if available</li>
                <li>Filter out null values after unpivoting if needed</li>
            </ul>
        </div>
    </div>

    <div class="navigation-buttons">
        <button class="nav-btn prev" onclick="navigateQuestion('prev')">
            <i class="fas fa-arrow-left"></i> Previous
        </button>
        <button class="nav-btn next" onclick="navigateQuestion('next')">
            Next <i class="fas fa-arrow-right"></i>
        </button>
    </div>
</div>
    </main>

    <!-- Mobile Toggle Button -->
    <div class="mobile-toggle" onclick="toggleSidebar()">
        <i class="fas fa-bars"></i>
    </div>

    <!-- Scroll to Top Button -->
    <div class="scroll-top" id="scrollTop" onclick="scrollToTop()">
        <i class="fas fa-arrow-up"></i>
    </div>

    <script>
        // State Management
        let currentQuestion = 'q1';
        const totalQuestions = 130; 
        let completedQuestions = new Set();

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            initializeApp();
        });

        function initializeApp() {
            setupSidebar();
            setupCrossQuestions();
            setupSearch();
            setupScrollTop();
            updateProgress();
        }

        // Sidebar Navigation
        function setupSidebar() {
            document.querySelectorAll('.difficulty-header').forEach(header => {
                header.addEventListener('click', function() {
                    this.classList.toggle('active');
                    const questionList = this.nextElementSibling;
                    questionList.classList.toggle('active');
                });
            });

            // Open first section by default
            document.querySelector('.difficulty-header').click();

            document.querySelectorAll('.question-item').forEach(item => {
                item.addEventListener('click', function() {
                    const questionId = this.getAttribute('data-question');
                    navigateToQuestion(questionId);
                });
            });
        }

        function navigateToQuestion(questionId) {
            if (currentQuestion) {
                completedQuestions.add(currentQuestion);
                const prevItem = document.querySelector(`[data-question="${currentQuestion}"]`);
                if (prevItem) prevItem.classList.add('completed');
            }

            document.querySelectorAll('.question-content').forEach(content => {
                content.classList.remove('active');
            });

            const selectedContent = document.getElementById(questionId);
            if (selectedContent) {
                selectedContent.classList.add('active');
                currentQuestion = questionId;

                document.querySelectorAll('.question-item').forEach(item => {
                    item.classList.remove('active');
                });
                const activeItem = document.querySelector(`[data-question="${questionId}"]`);
                if (activeItem) {
                    activeItem.classList.add('active');
                    const parentList = activeItem.closest('.question-list');
                    if (parentList && !parentList.classList.contains('active')) {
                        parentList.previousElementSibling.click();
                    }
                }

                window.scrollTo({ top: 0, behavior: 'smooth' });
                updateProgress();
            }
        }

        function navigateQuestion(direction) {
            const questionNumber = parseInt(currentQuestion.substring(1));
            let newQuestionNumber;

            if (direction === 'next') {
                newQuestionNumber = questionNumber + 1;
            } else {
                newQuestionNumber = questionNumber - 1;
            }

            if (newQuestionNumber >= 1 && newQuestionNumber <= 130) {
                navigateToQuestion(`q${newQuestionNumber}`);
            }
        }

        // Cross Questions Toggle
        function setupCrossQuestions() {
            document.querySelectorAll('.cross-question-item').forEach(item => {
                item.addEventListener('click', function() {
                    this.classList.toggle('active');
                });
            });
        }

        // Search Functionality
        function setupSearch() {
            const searchInput = document.getElementById('searchInput');
            searchInput.addEventListener('input', function(e) {
                const searchTerm = e.target.value.toLowerCase();
                
                document.querySelectorAll('.question-item').forEach(item => {
                    const text = item.textContent.toLowerCase();
                    if (text.includes(searchTerm)) {
                        item.style.display = 'flex';
                    } else {
                        item.style.display = 'none';
                    }
                });
            });
        }

        // Scroll to Top
        function setupScrollTop() {
            window.addEventListener('scroll', function() {
                const scrollTop = document.getElementById('scrollTop');
                if (window.pageYOffset > 300) {
                    scrollTop.classList.add('visible');
                } else {
                    scrollTop.classList.remove('visible');
                }
            });
        }

        function scrollToTop() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Progress Tracking
        function updateProgress() {
            const progress = (completedQuestions.size / totalQuestions) * 100;
            document.getElementById('progressFill').style.width = `${progress}%`;
            document.getElementById('progressText').textContent = `${Math.round(progress)}%`;
        }

        // Mobile Sidebar Toggle
        function toggleSidebar() {
            const sidebar = document.getElementById('sidebar');
            sidebar.classList.toggle('active');
        }

        // Copy Code Functionality
        function copyCode(button) {
            const codeBlock = button.closest('.example-box').querySelector('code');
            const code = codeBlock.textContent;
            
            navigator.clipboard.writeText(code).then(() => {
                const originalHTML = button.innerHTML;
                button.innerHTML = '<i class="fas fa-check"></i> Copied!';
                button.style.background = 'var(--success-color)';
                button.style.color = 'white';
                
                setTimeout(() => {
                    button.innerHTML = originalHTML;
                    button.style.background = '';
                    button.style.color = '';
                }, 2000);
            });
        }

        // Keyboard Navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'ArrowRight') {
                navigateQuestion('next');
            } else if (e.key === 'ArrowLeft') {
                navigateQuestion('prev');
            }
        });
    </script>
</body>
</html>